== Prerequisites

=== Helm

To successfully use Velero to backup and restore the {kube} cluster, you first need to install Helm and Tiller.
Refer to <<helm-tiller-install>>.

Add {suse} helm chart repository URL:

[source,bash]
----
helm repo add suse https://kubernetes-charts.suse.com
----

=== Object Storage And It's Credentials

Velero uses object storage to store backups and associated artifacts.
It can also optionally create snapshots of persistent volume and store them in object storage by `restic`, if there is no supported volume snapshot provider.

Choose one of the object storage providers, which fits your environment, from the list below for backing up and restoring the {kube} cluster.

The object storage server checks access permission, so it is vital to have credentials ready. Provide the credentials file `credentials-velero` to the velero server, so that it has the permission to write or read the backup data from the object storage.

[IMPORTANT]
====
Make sure the object storage is created before you install Velero. Otherwise, the Velero server won't be able to start successfully. This is because the Velero server checks that the object storage exists and needs to have the permission to access it during server boot.
====

==== Public Cloud Providers

[options="header"]
|===
| Provider | Object Storage | Plugin Provider Repo

|Amazon Web Services (AWS)
|AWS S3
|link:https://github.com/vmware-tanzu/velero-plugin-for-aws[Velero plugin for AWS]

|Google Cloud Platform (GCP)
|Google Cloud Storage
|link:https://github.com/vmware-tanzu/velero-plugin-for-gcp[Velero plugin for GCP]

|Microsoft Azure
|Azure Blob Storage
|link:https://github.com/vmware-tanzu/velero-plugin-for-microsoft-azure[Velero plugin for Microsoft Azure]
|===

===== AWS S3

. AWS CLI
+
Install `aws` CLI locally, follow the link:https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-welcome.html[doc] to install.

. AWS S3 bucket
+
Create a AWS S3 bucket to store backup data and restore data from the S3 bucket.
+
[source,bash]
----
aws s3api create-bucket \
    --bucket <BUCKET_NAME> \
    --region <REGION> \
    --create-bucket-configuration LocationConstraint=<REGION>
----

. Create the credential file `credentials-velero` in the local machine
+
----
[default]
aws_access_key_id=<AWS_ACCESS_KEY_ID>
aws_secret_access_key=<AWS_SECRET_ACCESS_KEY>
----
+
For details, please refer to link:https://github.com/vmware-tanzu/velero-plugin-for-aws/tree/v1.0.1[Velero Plugin For AWS].

===== Google Cloud Storage

. GCP CLIs
+
Install `gcloud` and `gsutil` CLIs locally, follow the link:https://cloud.google.com/sdk/docs/[doc] to install.

. Create GCS bucket
+
[source,bash]
----
gsutil mb gs://<BUCKET_NAME>/
----

. Create the service account
+
[source,bash]
----
# View current config settings
gcloud config list

# Store the project value to PROJECT_ID environment variable
PROJECT_ID=$(gcloud config get-value project)

# Create a service account
gcloud iam service-accounts create velero \
    --display-name "Velero service account"

# List all accounts
gcloud iam service-accounts list

# Set the SERVICE_ACCOUNT_EMAIL environment variable
SERVICE_ACCOUNT_EMAIL=$(gcloud iam service-accounts list \
  --filter="displayName:Velero service account" \
  --format 'value(email)')

# Attach policies to give velero the necessary permissions
ROLE_PERMISSIONS=(
    compute.disks.get
    compute.disks.create
    compute.disks.createSnapshot
    compute.snapshots.get
    compute.snapshots.create
    compute.snapshots.useReadOnly
    compute.snapshots.delete
    compute.zones.get
)

# Create iam roles
gcloud iam roles create velero.server \
    --project $PROJECT_ID \
    --title "Velero Server" \
    --permissions "$(IFS=","; echo "${ROLE_PERMISSIONS[*]}")"

# Bind iam policy to project
gcloud projects add-iam-policy-binding $PROJECT_ID \
    --member serviceAccount:$SERVICE_ACCOUNT_EMAIL \
    --role projects/$PROJECT_ID/roles/velero.server

gsutil iam ch serviceAccount:$SERVICE_ACCOUNT_EMAIL:objectAdmin gs://<BUCKET_NAME>
----

. Create the credential file `credentials-velero` in the local machine
+
[source,bash]
----
gcloud iam service-accounts keys create credentials-velero \
    --iam-account $SERVICE_ACCOUNT_EMAIL
----
+
For details, please refer to link:https://github.com/vmware-tanzu/velero-plugin-for-gcp/tree/v1.0.1[Velero Plugin For GCP].

===== Azure Blob Storage

. Azure CLI
+
Install `az` CLI locally, follow the link:https://docs.microsoft.com/en-us/cli/azure/install-azure-cli[doc] to install.

. Create a resource group for the backups storage account
+
Create the resource group named Velero_Backups, change the resource group name and location as needed.
+
[source,bash]
----
AZURE_RESOURCE_GROUP=Velero_Backups
az group create -n $AZURE_RESOURCE_GROUP --location <location>
----

. Create the storage account
+
[source,bash]
----
az storage account create \
    --name $AZURE_STORAGE_ACCOUNT_ID \
    --resource-group $AZURE_RESOURCE_GROUP \
    --sku Standard_GRS \
    --encryption-services blob \
    --https-only true \
    --kind BlobStorage \
    --access-tier Hot
----

. Create a blob container
+
Create a blob container named velero. Change the name as needed.
+
[source,bash]
----
BLOB_CONTAINER=velero
az storage container create -n $BLOB_CONTAINER --public-access off --account-name $AZURE_STORAGE_ACCOUNT_ID
----

. Create the credential file `credentials-velero` in the local machine
+
[source,bash]
----
# Obtain your Azure Account Subscription ID
AZURE_SUBSCRIPTION_ID=`az account list --query '[?isDefault].id' -o tsv`

# Obtain your Azure Account Tenant ID
AZURE_TENANT_ID=`az account list --query '[?isDefault].tenantId' -o tsv`

# Generate client secret
AZURE_CLIENT_SECRET=`az ad sp create-for-rbac --name "velero" --role "Contributor" --query 'password' -o tsv`

# Generate client ID
AZURE_CLIENT_ID=`az ad sp list --display-name "velero" --query '[0].appId' -o tsv`

cat << EOF  > ./credentials-velero
AZURE_SUBSCRIPTION_ID=${AZURE_SUBSCRIPTION_ID}
AZURE_TENANT_ID=${AZURE_TENANT_ID}
AZURE_CLIENT_ID=${AZURE_CLIENT_ID}
AZURE_CLIENT_SECRET=${AZURE_CLIENT_SECRET}
AZURE_RESOURCE_GROUP=${AZURE_RESOURCE_GROUP}
EOF
----
+
For details, please refer to link:https://github.com/vmware-tanzu/velero-plugin-for-microsoft-azure/tree/v1.0.1[Velero Plugin For Azure].

==== On-Premise (S3-Compatible Providers)

===== {ses} 6 Ceph Object Gateway (`radosgw`)

{suse} supports the {ses} 6 Ceph Object Gateway (`radosgw`) as an S3-compatible object storage provider.

. Installation
Refer to the link:https://documentation.suse.com/ses/6/html/ses-all/cha-ceph-additional-software-installation.html[SES 6 Object Gateway Manual Installation] on how to install it.
. Create the credential file `credentials-velero` in the local machine
+
----
[default]
aws_access_key_id=<SES_STORAGE_ACCESS_KEY_ID>
aws_secret_access_key=<SES_STORAGE_SECRET_ACCESS_KEY>
----

===== Minio

Besides {ses}, there is an alternative open source S3-compatible object storage provider link:https://min.io/[minio].

. Prepare an external host and install Minio on the host
+
[source,bash]
----
# Download Minio server
wget https://dl.min.io/server/minio/release/linux-amd64/minio
chmod +x minio

# Expose Minio access_key and secret_key
export MINIO_ACCESS_KEY=<access_key>
export MINIO_SECRET_KEY=<secret_key>

# Start Minio server
mkdir -p bucket
./minio server bucket &

# Download Minio client
wget https://dl.min.io/client/mc/release/linux-amd64/mc
chmod +x mc

# Setup Minio server
./mc config host add Velero http://localhost:9000 $MINIO_ACCESS_KEY $MINIO_SECRET_KEY

# Create bucket on Minio server
./mc mb -p velero/velero
----
. Create the credential file `credentials-velero` in the local machine
+
----
[default]
aws_access_key_id=<MINIO_STORAGE_ACCESS_KEY_ID>
aws_secret_access_key=<MINIO_STORAGE_SECRET_ACCESS_KEY>
----

For the rest of the S3-compatible storage providers supported by Velero, refer to link:https://velero.io/docs/v1.3.1/supported-providers/[Velero Supported Providers].

=== Volume Snapshotter

A volume snapshotter is able to snapshot its persistent volumes if its volume driver supports volume snapshot and corresponding API.

If a volume provider does not support volume snapshot or volume snapshot API, or does not have Velero supported storage plugin, Velero leverages `restic` as an agnostic solution to backup and restore this sort of persistent volumes.

[options="header"]
|===
|Provider | Volume Snapshotter | Plugin Provider Repo
|Amazon Web Services (AWS) | AWS EBS | link:https://github.com/vmware-tanzu/velero-plugin-for-aws[Velero plugin for AWS]
|===

For the other `snapshotter` providers refer to link:https://velero.io/docs/v1.3.1/supported-providers/[Velero Supported Providers].

=== Velero CLI

Install Velero CLI to interact with Velero server.

[source,bash]
----
sudo zypper install velero
----

== Known Issues

. Velero reports errors when restoring Cilium CRDs. However, this does not affect Cilium functionality.
+
[NOTE]
====
You can add a label to Cilium CRDs to skip Velero backup.

[source,bash]
----
kubectl label -n kube-system customresourcedefinitions/ciliumendpoints.cilium.io velero.io/exclude-from-backup=true

kubectl label -n kube-system customresourcedefinitions/ciliumnetworkpolicies.cilium.io velero.io/exclude-from-backup=true
----
====

. When restoring `dex` and `gangway`, Velero reports `NodePort` cannot be restored since `dex` and `gangway` are deployed by an addon already and the same `NodePort` has been registered.
However, this does not break the `dex` and `gangway` service access from outside.
+
[NOTE]
====
You can add a label to services `oidc-dex` and `oidc-gangway` to skip Velero backup.

[source,bash]
----
kubectl label -n kube-system services/oidc-dex velero.io/exclude-from-backup=true

kubectl label -n kube-system services/oidc-gangway velero.io/exclude-from-backup=true
----
====
