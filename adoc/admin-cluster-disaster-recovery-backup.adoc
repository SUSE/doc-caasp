
= Backing Up etcd Cluster Data

This chapter describes the backup of `etcd` cluster data running on master nodes of {productname}.

== Data To Backup
. Create backup directories on external storage.
+
[source,bash]
----
BACKUP_DIR=CaaSP_Backup_`date +%Y%m%d%H%M%S`
mkdir /${BACKUP_DIR}
----
. Copy the following files/folders into the backup directory:
* The `skuba` command-line binary: for the running cluster. Used to replace nodes from cluster.
* The cluster definition folder: Directory created during bootstrap holding the cluster certificates and configuration.
* The `etcd` cluster database: Holds all non-persistent cluster data.
Can be used to recover master nodes. Please refer to the next section for steps to create an `etcd` cluster database backup.
. (Optional) Make backup directory into a compressed file, and remove the original backup directory.
+
[source,bash]
----
tar cfv ${BACKUP_DIR}.tgz /${BACKUP_DIR}
rm -rf /${BACKUP_DIR}
----

== Creating an etcd Cluster Database Backup

=== Procedure

. Mount external storage device to all master nodes.
This is only required if the following step is using local hostpath as volume storage.
. Create backup.
.. Find the size of the database to be backed up
+
[source,bash]
----
ls -sh /var/lib/etcd/member/snap/db
----
+
[IMPORTANT]
====
The backup size depends on the cluster. Ensure each of the backups has sufficient space.
The available size should be more than the database snapshot file.

You should also have a rotation method to clean up the unneeded snapshots over time.

When there is insufficient space available during backup, pods will fail to be in `Running` state and `no space left on device` errors will show in pod logs.

The below example manifest shows a binding to a local `hostPath`.
We strongly recommend using other storage methods instead.
====
.. Modify the script example
+
Replace `<STORAGE_MOUNT_POINT>` with the directory in which to store the backup.
The directory must exist on every node in cluster.
+
Replace `<IN_CLUSTER_ETCD_IMAGE>` with the `etcd` image used in the cluster.
This can be retrieved by accessing any one of the nodes in the cluster and running:
+
----
grep image: /etc/kubernetes/manifests/etcd.yaml | awk '{print $2}'
----
.. Create a backup deployment
+
Run the following script:
+
[source,bash]
----
ETCD_SNAPSHOT="<STORAGE_MOUNT_POINT>/etcd_snapshot"
ETCD_IMAGE="<IN_CLUSTER_ETCD_IMAGE>"
MANIFEST="etcd-backup.yaml"

cat << *EOF* > ${MANIFEST}
apiVersion: batch/v1
kind: Job
metadata:
  name: etcd-backup
  namespace: kube-system
  labels:
    jobgroup: backup
spec:
  template:
    metadata:
      name: etcd-backup
      labels:
        jobgroup: backup
    spec:
      containers:
      - name: etcd-backup
        image: ${ETCD_IMAGE}
        env:
        - name: ETCDCTL_API
          value: "3"
        command: ["/bin/sh"]
        args: ["-c", "etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key snapshot save /backup/etcd-snapshot-\$(date +%Y-%m-%d_%H:%M:%S_%Z).db"]
        volumeMounts:
        - mountPath: /etc/kubernetes/pki/etcd
          name: etcd-certs
          readOnly: true
        - mountPath: /backup
          name: etcd-backup
      restartPolicy: OnFailure
      nodeSelector:
        node-role.kubernetes.io/master: ""
      tolerations:
      - effect: NoSchedule
        operator: Exists
      hostNetwork: true
      volumes:
      - name: etcd-certs
        hostPath:
          path: /etc/kubernetes/pki/etcd
          type: DirectoryOrCreate
      - name: etcd-backup
        hostPath:
          path: ${ETCD_SNAPSHOT}
          type: Directory
*EOF*

kubectl create -f ${MANIFEST}
----
+
If you are using local `hostPath` and not using a shared storage device, the `etcd` backup will be created to any one of the master nodes.
To find the node associated with each `etcd` backup run:
+
[source,bash]
----
kubectl get pods --namespace kube-system --selector=job-name=etcd-backup -o wide
----

== Scheduling etcd Cluster Backup
. Mount external storage device to all master nodes.
This is only required if the following step is using local `hostPath` as volume storage.
. Create Cronjob.
.. Find the size of the database to be backed up
+
[IMPORTANT]
====
The backup size depends on the cluster. Ensure each of the backups has sufficient space.
The available size should be more than the database snapshot file.

You should also have a rotation method to clean up the unneeded snapshots over time.

When there is insufficient space available during backup, pods will fail to be in `Running` state and `no space left on device` errors will show in pod logs.

The below example manifest shows a binding to a local `hostPath`.
We strongly recommend using other storage methods instead.
====
+
[source,bash]
----
ls -sh /var/lib/etcd/member/snap/db
----
.. Modify the script example
+
Replace `<STORAGE_MOUNT_POINT>` with directory to store for backup. The directory must exist on every node in cluster.
+
Replace `<IN_CLUSTER_ETCD_IMAGE>` with etcd image used in cluster.
This can be retrieved by accessing any one of the nodes in the cluster and running:
+
----
grep image: /etc/kubernetes/manifests/etcd.yaml | awk '{print $2}'
----
.. Create a backup schedule deployment
+
Run the following script:
+
[source,bash]
----
ETCD_SNAPSHOT="<STORAGE_MOUNT_POINT>/etcd_snapshot"
ETCD_IMAGE="<IN_CLUSTER_ETCD_IMAGE>"

# SCHEDULE in Cron format. https://crontab.guru/
SCHEDULE="*/3 * * * *"

# *_HISTORY_LIMIT is the number of maximum history keep in the cluster.
SUCCESS_HISTORY_LIMIT="3"
FAILED_HISTORY_LIMIT="3"

MANIFEST="etcd-backup.yaml"

cat << *EOF* > ${MANIFEST}
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: etcd-backup
  namespace: kube-system
spec:
  startingDeadlineSeconds: 100
  schedule: "${SCHEDULE}"
  successfulJobsHistoryLimit: ${SUCCESS_HISTORY_LIMIT}
  failedJobsHistoryLimit: ${FAILED_HISTORY_LIMIT}
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: etcd-backup
            image: ${ETCD_IMAGE}
            env:
            - name: ETCDCTL_API
              value: "3"
            command: ["/bin/sh"]
            args: ["-c", "etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key snapshot save /backup/etcd-snapshot-\$(date +%Y-%m-%d_%H:%M:%S_%Z).db"]
            volumeMounts:
            - mountPath: /etc/kubernetes/pki/etcd
              name: etcd-certs
              readOnly: true
            - mountPath: /backup
              name: etcd-backup
          restartPolicy: OnFailure
          nodeSelector:
            node-role.kubernetes.io/master: ""
          tolerations:
          - effect: NoSchedule
            operator: Exists
          hostNetwork: true
          volumes:
          - name: etcd-certs
            hostPath:
              path: /etc/kubernetes/pki/etcd
              type: DirectoryOrCreate
          - name: etcd-backup
            # hostPath is only one of the types of persistent volume. Suggest to setup external storage instead.
            hostPath:
              path: ${ETCD_SNAPSHOT}
              type: Directory
*EOF*

kubectl create -f ${MANIFEST}
----
