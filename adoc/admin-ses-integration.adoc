[[ses-integration]]
= {ses} Integration

{productname} offers {ses} as a storage solution for its containers.
This chapter describes the steps required for successful integration.

== Prerequisites

Before you start with integrating {ses}, you need to ensure the following:

* The {productname} cluster must have `ceph-common` and `xfsprogs` installed on all nodes.
You can check this by running `rpm -q ceph-common` and `rpm -q xfsprogs`.
* The {productname} cluster can communicate with all of the following {ses} nodes:
master, monitoring nodes, OSD nodes and the metadata server (in case you need a shared file system).
For more details refer to the {ses} documentation:
https://documentation.suse.com/ses/6/.
* The {ses} cluster has a pool with RADOS Block Device (RBD) enabled.

== Procedures According to Type of Integration

The steps will differ in small details depending on whether you are using RBD or
CephFS and dynamic or static persistent volumes.


=== Using RBD in a Pod

RBD, also known as the Ceph Block Device or RADOS Block Device,
is software that facilitates the storage of block-based data in the open source
Ceph distributed storage system.
The procedure below describes steps to take when you need to use a RADOS Block Device in a pod.

. Retrieve the Ceph admin secret.
You can get the key value using the following command:
+
----
ceph auth get-key client.admin
----
or directly from `/etc/ceph/ceph.client.admin.keyring`.
. Apply the configuration that includes the Ceph secret by running `kubectl apply`.
Replace `<CEPH_SECRET>` with your own Ceph secret and run the following:
+

----
kubectl apply -f - << *EOF*
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
type: "kubernetes.io/rbd"
data:
  key: "$(echo <CEPH_SECRET> | base64)"
*EOF*
----
. Create an image in the SES cluster.
To do that, run the following command on the {master_node},
replacing `<SIZE>` with the size of the image, for example `2G`,
and `<YOUR_VOLUME>` with the name of the image.
+

----
rbd create -s <SIZE> <YOUR_VOLUME>
----
+
. Create a pod that uses the image by executing the command below.
 This example is the minimal configuration for using a RADOS Block Device. +
 Fill in the IP addresses and ports of your monitor nodes under `<MONITOR_IP>` and `<MONITOR_PORT>`. The default port number is *6789*. +
 Substitute `<POD_NAME>` and `<CONTAINER_NAME>` for a {kube} container and pod name of your choice. +
 `<IMAGE_NAME>` is the name you decide to give your container image, for example "opensuse/leap". +
 `<RBD_POOL>.` is the RBD pool name,
 please refer to the RBD documentation for instructions on how to create the RBD pool:
 https://docs.ceph.com/docs/mimic/rbd/rados-rbd-cmds/#create-a-block-device-pool
+

----
kubectl apply -f - << *EOF*
apiVersion: v1
kind: Pod
metadata:
  name: <POD_NAME>
spec:
  containers:
  - name: <CONTAINER_NAME>
    image: <IMAGE_NAME>
    volumeMounts:
    - mountPath: /mnt/rbdvol
      name: rbdvol
  volumes:
  - name: rbdvol
    rbd:
      monitors:
      - '<MONITOR1_IP:MONITOR1_PORT>'
      - '<MONITOR2_IP:MONITOR2_PORT>'
      - '<MONITOR3_IP:MONITOR3_PORT>'
      pool: <RBD_POOL>
      image: <YOUR_VOLUME>
      user: admin
      secretRef:
        name: ceph-secret
      fsType: ext4
      readOnly: false
*EOF*
----
. Verify that the pod exists and check its status:
+

----
kubectl get pod
----
. Once the pod is running, check the mounted volume:
+

----
kubectl exec -it CONTAINER_NAME -- df -k ...
Filesystem             1K-block    Used    Available Used%   Mounted on
/dev/rbd1              999320      1284    929224    0%      /mnt/rbdvol
...
----

In case you need to delete the pod, run the following command:
----
kubectl delete pod <POD_NAME>
----

=== Using RBD with Static Persistent Volumes


The following procedure describes how to attach a pod to an RBD static persistent volume:

. Retrieve the Ceph admin secret.
You can get the key value using the following command:
+
----
ceph auth get-key client.admin
----
or directly from `/etc/ceph/ceph.client.admin.keyring`.
. Apply the configuration that includes the Ceph secret by using `kubectl apply`.
Replace `<CEPH_SECRET>` with your Ceph secret.
+

----
kubectl apply -f - << *EOF*
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
type: "kubernetes.io/rbd"
data:
  key: "$(echo <CEPH_SECRET> | base64)"
*EOF*
----
. Create an image in the SES cluster. On the {master_node}, run the following command:
+

----
rbd create -s <SIZE> <YOUR_VOLUME>
----
+
Replace `<SIZE>` with the size of the image, for example `2G` (2 gigabytes),
and `<YOUR_VOLUME>` with the name of the image.
. Create the persistent volume:
+

----
kubectl apply -f - << *EOF*
apiVersion: v1
kind: PersistentVolume
metadata:
  name: <PV_NAME>
spec:
  capacity:
    storage: <SIZE>
  accessModes:
    - ReadWriteOnce
  rbd:
    monitors:
    - '<MONITOR1_IP:MONITOR1_PORT>'
    - '<MONITOR2_IP:MONITOR2_PORT>'
    - '<MONITOR3_IP:MONITOR3_PORT>'
    pool: <RBD_POOL>
    image: <YOUR_VOLUME>
    user: admin
    secretRef:
      name: ceph-secret
    fsType: ext4
    readOnly: false
*EOF*
----
+
Replace `<SIZE>` with the desired size of the volume.
Use the _gibibit_ notation, for example ``2Gi``.
. Create a persistent volume claim:
+

----
kubectl apply -f - << *EOF*
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: <PVC_NAME>
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: SIZE
*EOF*
----
+
Replace `<SIZE>` with the desired size of the volume.
Use the _gibibit_ notation, for example ``2Gi``.
+
.Listing Volumes
NOTE: This persistent volume claim does not explicitly list the volume.
Persistent volume claims work by picking any volume that meets the criteria from a pool.
In this case we specified any volume with a size of 2G or larger.
When the claim is removed, the recycling policy will be followed.
+

. Create a pod that uses the persistent volume claim:
+

----
kubectl apply -f - <<*EOF*
apiVersion: v1
kind: Pod
metadata:
  name: <POD_NAME>
spec:
  containers:
  - name: <CONTAINER_NAME>
    image: <IMAGE_NAME>
    volumeMounts:
    - mountPath: /mnt/rbdvol
      name: rbdvol
  volumes:
  - name: rbdvol
    persistentVolumeClaim:
      claimName: <PV_NAME>
*EOF*
----
. Verify that the pod exists and its status:
+

----
kubectl get pod
----
. Once the pod is running, check the volume:
+

----
kubectl exec -it CONTAINER_NAME -- df -k ...
/dev/rbd3               999320      1284    929224   0% /mnt/rbdvol
...
----


In case you need to delete the pod, run the following command:

----
kubectl delete pod <CONTAINER_NAME>
----

.Deleting A Pod
[NOTE]
====
When you delete the pod, the persistent volume claim is deleted as well.
The RBD is not deleted.
====


[[_RBD-dynamic-persistent-volumes]]
=== Using RBD with Dynamic Persistent Volumes


The following procedure describes how to attach a pod to an RBD dynamic persistent volume.

. Retrieve the Ceph *admin* secret.
You can get the key value using the following command:
+
----
ceph auth get-key client.admin
----
or directly from `/etc/ceph/ceph.client.admin.keyring`.
. Apply the configuration that includes the Ceph secret by using `kubectl apply`.
Replace `<CEPH_SECRET>` with your Ceph secret.
+

----
kubectl apply -f - << *EOF*
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-admin
type: "kubernetes.io/rbd"
data:
  key: "$(echo <CEPH_SECRET> | base64)"
*EOF*
----

. Create Ceph user on the SES cluster.
+

----
ceph auth get-or-create client.user mon "allow r" osd "allow class-read object_prefix rbd_children,
allow rwx pool=<RBD_POOL>" -o ceph.client.user.keyring
----
+
Replace `<RBD_POOL>` with the RBD pool name.

. For a dynamic persistent volume, you will also need a user key.
Retrieve the Ceph *user* secret by running:
+
----
ceph auth get-key client.user
----
or directly from `/etc/ceph/ceph.client.user.keyring`
. Apply the configuration that includes the Ceph secret by running the `kubectl apply` command,
replacing `<CEPH_SECRET>` with your own Ceph secret.
+

----
kubectl apply -f - << *EOF*
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-user
type: "kubernetes.io/rbd"
data:
  key: "$(echo <CEPH_SECRET> | base64)"
*EOF*
----
. Create the storage class:
+

----
kubectl apply -f - << *EOF*
apiVersion: storage.k8s.io/v1beta1
kind: StorageClass
metadata:
  name: <SC_NAME>
  annotations:
    storageclass.beta.kubernetes.io/is-default-class: "true"
provisioner: kubernetes.io/rbd
parameters:
  monitors: <MONITOR1_IP:MONITOR1_PORT>, <MONITOR2_IP:MONITOR2_PORT>, <MONITOR3_IP:MONITOR3_PORT>
  adminId: admin
  adminSecretName: ceph-secret-admin
  adminSecretNamespace: default
  pool: <RBD_POOL>
  userId: user
  userSecretName: ceph-secret-user
*EOF*
----
. Create the persistent volume claim:
+

----
kubectl apply -f - << *EOF*
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: <PVC_NAME>
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: <SIZE>
*EOF*
----
+
Replace `<SIZE>` with the desired size of the volume.
Use the _gibibit_ notation, for example ``2Gi``.

. Create a pod that uses the persistent volume claim.
+

----
kubectl apply -f - << *EOF*
apiVersion: v1
kind: Pod
metadata:
  name: <POD_NAME>
spec:
  containers:
  - name: <CONTAINER_NAME>
    image: <IMAGE_NAME>
    volumeMounts:
    - name: rbdvol
      mountPath: /mnt/rbdvol
      readOnly: false
  volumes:
  - name: rbdvol
    persistentVolumeClaim:
      claimName: <PVC_NAME>
*EOF*
----
. Verify that the pod exists and check its status.
+

----
kubectl get pod
----
. Once the pod is running, check the volume:
+

----
kubectl exec -it <CONTAINER_NAME> -- df -k ...
/dev/rbd3               999320      1284    929224   0% /mnt/rbdvol
...
----


In case you need to delete the pod, run the following command:

----
kubectl delete pod <CONTAINER_NAME>
----

.Deleting A Pod
[NOTE]
====
When you delete the pod, the persistent volume claim is deleted as well.
The RBD is not deleted.
====

=== Using CephFS in a Pod


The procedure below describes steps to take when you need to use a CephFS in a pod.

.Procedure: Using CephFS In A Pod


. Retrieve the Ceph admin secret.
You can get the key value using the following command:
+
----
ceph auth get-key client.admin
----
or directly from `/etc/ceph/ceph.client.admin.keyring`.
. Apply the configuration that includes the Ceph secret by running `kubectl apply`.
Replace `<CEPH_SECRET>` with your own Ceph secret and run the following:
+

----
kubectl apply -f - << *EOF*
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
type: "kubernetes.io/rbd"
data:
  key: "$(echo <CEPH_SECRET> | base64)"
*EOF*
----
. Create a pod that uses the image by executing the following command.
This example shows the minimal configuration for a `CephFS` volume.
Fill in the IP addresses and ports of your monitor nodes. The default port number is ``6789``.
+

----
kubectl apply -f - << *EOF*
apiVersion: v1
kind: Pod
metadata:
  name: <POD_NAME>
spec:
  containers:
  - name: <CONTAINER_NAME>
    image: <IMAGE_NAME>
    volumeMounts:
    - mountPath: /mnt/cephfsvol
      name: ceph-vol
  volumes:
  - name: ceph-vol
    cephfs:
      monitors:
      - '<MONITOR1_IP:MONITOR1_PORT>'
      - '<MONITOR2_IP:MONITOR2_PORT>'
      - '<MONITOR3_IP:MONITOR3_PORT>'
      user: admin
      secretRef:
        name: ceph-secret-admin
      readOnly: false
*EOF*
----
. Verify that the pod exists and check its status:
+

----
kubectl get pod
----
. Once the pod is running, check the mounted volume:
+

----
kubectl exec -it <CONTAINER_NAME> -- df -k ...
172.28.0.6:6789,172.28.0.14:6789,172.28.0.7:6789:/  59572224       0  59572224   0% /mnt/cephfsvol
...
----


In case you need to delete the pod, run the following command:

----
kubectl delete pod <POD_NAME>
----

=== Using CephFS with Static Persistent Volumes


The following procedure describes how to attach a CephFS static persistent volume to a pod:

. Retrieve the Ceph admin secret.
You can get the key value using the following command:
+
----
ceph auth get-key client.admin
----
or directly from `/etc/ceph/ceph.client.admin.keyring`.
. Apply the configuration that includes the Ceph secret by running `kubectl apply`.
Replace `<CEPH_SECRET>` with your own Ceph secret and run the following:
+

----
kubectl apply -f - << *EOF*
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
type: "kubernetes.io/rbd"
data:
  key: "$(echo <CEPH_SECRET> | base64)"
*EOF*
----
. Create the persistent volume:
+

----
kubectl apply -f - << *EOF*
apiVersion: v1
kind: PersistentVolume
metadata:
  name: <PV_NAME>
spec:
  capacity:
    storage: <SIZE>
  accessModes:
    - ReadWriteOnce
  cephfs:
    monitors:
    - '<MONITOR1_IP:MONITOR1_PORT>'
    - '<MONITOR2_IP:MONITOR2_PORT>'
    - '<MONITOR3_IP:MONITOR3_PORT>'
    user: admin
    secretRef:
      name: ceph-secret-admin
    readOnly: false
*EOF*
----
+
Replace `<SIZE>` with the desired size of the volume.
Use the _gibibit_ notation, for example ``2Gi``.
. Create a persistent volume claim:
+

----
kubectl apply -f - << *EOF*
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: <PVC_NAME>
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: <SIZE>
*EOF*
----
+
Replace `<SIZE>` with the desired size of the volume.
Use the _gibibit_ notation, for example ``2Gi``.
+

. Create a pod that uses the persistent volume claim.
+

----
kubectl apply -f - << *EOF*
apiVersion: v1
kind: Pod
metadata:
  name: <POD_NAME>
spec:
  containers:
  - name: <CONTAINER_NAME>
    image: <IMAGE_NAME>
    volumeMounts:
    - mountPath: /mnt/cephfsvol
      name: cephfsvol
  volumes:
  - name: cephfsvol
    persistentVolumeClaim:
      claimName: <PVC_NAME>

*EOF*
----
. Verify that the pod exists and check its status.
+

----
kubectl get pod
----
. Once the pod is running, check the volume by running:
+

----
kubectl exec -it <CONTAINER_NAME> -- df -k ...
172.28.0.25:6789,172.28.0.21:6789,172.28.0.6:6789:/  76107776       0  76107776   0% /mnt/cephfsvol
...
----

In case you need to delete the pod, run the following command:

----
kubectl delete pod <CONTAINER_NAME>
----

.Deleting A Pod
[NOTE]
====
When you delete the pod, the persistent volume claim is deleted as well.
The cephFS is not deleted.
====
