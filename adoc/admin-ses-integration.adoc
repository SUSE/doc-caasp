= {ses} Integration
:doctype: book
:sectnums:
:toc: left
:icons: font
:experimental:
:sourcedir: .
:imagesdir: ./images
= SUSE Enterprise Storage Integration
:doctype: book
:sectnums:
:toc: left
:icons: font
:experimental:
:imagesdir: ./images

{productname} can use another {suse} product as storage for containers - {ses} (henceforth called SES). This chapter gives details on several ways to integrate these two products. 

== Prerequisites


Before you start the integration process, you need to ensure the following: 

* The {productname} cluster must have `ceph-common` and `xfsprogs` installed on all nodes. You can verifiy with `rpm -q ceph-common` and `rpm -q xfsprogs`.
* The {productname} cluster can communicate with the SES nodes{mdash} master, monitoring nodes, OSD nodes and the metadata server, in case you need a shared file system. For more details regarding SES refer to the SES documentation, here: https://www.suse.com/documentation/suse-enterprise-storage/. 
* The SES cluster has a pool with RADOS Block Device (RBD) enabled. 


== Using RBD in a Pod


The procedure below describes steps to take when you need to use a RADOS Block Device in a Pod. 

.Procedure: Using RBD In A Pod
. Retrieve the Ceph admin secret. Get the key value using command ``ceph auth get-key client.admin`` or, from the file [path]``/etc/ceph/ceph.client.admin.keyring`` . 
. Apply the configuration that includes the Ceph secret by using `kubectl apply`. Replace `CEPH_SECRET` with your Ceph secret. 
+

----
kubectl apply -f - << *EOF*
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
type: "kubernetes.io/rbd"
data:
  key: "$(echo CEPH_SECRET | base64)"
*EOF*
----
. Create an image in the SES cluster. On the {master_node} , run the following command: 
+

----
rbd create -s SIZE YOUR_VOLUME
----
+
Replace `SIZE` with the size of the image, for example ``2G``, and `YOUR_VOLUME` is the name of the image. 
. Create a pod that uses the image by executing the following command. In this example it is a minimal configuration for using a `RADOS` Block Device. Fill in the IP addresses and ports of your monitor nodes. The default port number is ``6789``. 
+

----
kubectl apply -f - << *EOF*
apiVersion: v1
kind: Pod
metadata:
  name: POD_NAME
spec:
  containers:
  - name: CONTAINER_NAME
    image: IMAGE_NAME
    volumeMounts:
    - mountPath: /mnt/rbdvol
      name: rbdvol
  volumes:
  - name: rbdvol
    rbd:
      monitors:
      - 'MONITOR1_IP:MONITOR1_PORT'
      - 'MONITOR2_IP:MONITOR2_PORT'
      - 'MONITOR3_IP:MONITOR3_PORT'
      pool: RBD_POOL
      image: YOUR_VOLUME
      user: admin
      secretRef:
        name: ceph-secret
      fsType: ext4
      readOnly: false
*EOF*
----
. Verify that the pod exists and its status: 
+

----
kubectl get pod
----
. Once the pod is running, check the mounted volume: 
+

----
kubectl exec -it CONTAINER_NAME -- df -k ...
/dev/rbd1               999320      1284    929224   0% /mnt/rbdvol
...
----


In case you need to delete the pod, run the following command: 

----
kubectl delete pod POD_NAME
----

== Using RBD for Static Persistent Volumes


The following procedure describes how to attach a pod to a RDB static persistent volume. 

.Procedure: Creating a Pod with RBD in Persistent Volume
. Retrieve the Ceph admin secret. Get the key value using command ``ceph auth get-key client.admin`` or, from the file [path]``/etc/ceph/ceph.client.admin.keyring`` . 
. Apply the configuration that includes the Ceph secret by using `kubectl apply`. Replace `CEPH_SECRET` with your Ceph secret. 
+

----
kubectl apply -f - << *EOF*
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
type: "kubernetes.io/rbd"
data:
  key: "$(echo CEPH_SECRET | base64)"
*EOF*
----
. Create an image in the SES cluster. On the {master_node} , run the following command: 
+

----
rbd create -s SIZE YOUR_VOLUME
----
+
Replace `SIZE` with the size of the image, for example `2G` (2 Gigabyte), and `YOUR_VOLUME` is the name of the image. 
. Create the persistent volume: 
+

----
kubectl apply -f - << *EOF*
apiVersion: v1
kind: PersistentVolume
metadata:
  name: PV_NAME
spec:
  capacity:
    storage: SIZE
  accessModes:
    - ReadWriteOnce
  rbd:
    monitors:
    - 'MONITOR1_IP:MONITOR1_PORT'
    - 'MONITOR2_IP:MONITOR2_PORT'
    - 'MONITOR3_IP:MONITOR3_PORT'
    pool: RDB_POOL
    image: YOUR_VOLUME
    user: admin
    secretRef:
      name: ceph-secret
    fsType: ext4
    readOnly: false
*EOF*
----
+
Replace `SIZE` with the desired size of the volume.
Use the _gibibit_ notation, for example ``2Gi``. 
. Create a persistent volume claim: 
+

----
kubectl apply -f - << *EOF*
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: PVC_NAME
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: SIZE
*EOF*
----
+
Replace `SIZE` with the desired size of the volume.
Use the _gibibit_ notation, for example ``2Gi``. 
+
.Listing Volumes
NOTE: This persistent volume claim does not explicitly list the volume.
Persistent volume claims work by picking any volume that meets the criteria from a pool.
In this case we specified any volume with a size of 2G or larger.
When the claim is removed the recycling policy will be followed. 
+

. Create a pod that uses the persistent volume claim. 
+

----
kubectl apply -f - <<*EOF*
apiVersion: v1
kind: Pod
metadata:
  name: POD_NAME
spec:
  containers:
  - name: CONTAINER_NAME
    image: IMAGE_NAME
    volumeMounts:
    - mountPath: /mnt/rbdvol
      name: rbdvol
  volumes:
  - name: rbdvol
    persistentVolumeClaim:
      claimName: PV_NAME
*EOF*
----
. Verify that the pod exists and its status. 
+

----
kubectl get pod
----
. Once pod is running, check the volume: 
+

----
kubectl exec -it CONTAINER_NAME -- df -k ...
/dev/rbd3               999320      1284    929224   0% /mnt/rbdvol
...
----


In case you need to delete the pod, run the following command: 

----
kubectl delete pod CONTAINER_NAME
----

.Deleting A Pod
[NOTE]
====
When you delete the pod, the persistent volume claim is deleted as well.
The RBD is not deleted. 
====

== Using RBD for Dynamic Persistent Volumes


The following procedure describes how to attach a pod to a RDB dynamic persistent volume. 

.Procedure: Creating a Pod with RBD in Dynamic Persistent Volume
. Retrieve the Ceph admin secret. Get the key value using command ``ceph auth get-key client.admin`` or, from the file [path]``/etc/ceph/ceph.client.admin.keyring`` . 
. Apply the configuration that includes the Ceph secret by using `kubectl apply`. Replace `CEPH_SECRET` with your Ceph secret. 
+

----
kubectl apply -f - << *EOF*
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-admin
type: "kubernetes.io/rbd"
data:
  key: "$(echo CEPH_SECRET | base64)"
*EOF*
----

. Create Ceph user on the SES cluster. 
+

----
``ceph auth get-or-create client.user mon "allow r" osd "allow class-read object_prefix rbd_children, allow rwx pool=RBD_POOL" -o ceph.client.kube.keyring`` 
----
+
Replace `RBD_POOL` with the RBD pool name.

. Retrieve the Ceph admin secret. Get the key value using command ``ceph auth get-key client.admin`` or, from the file [path]``/etc/ceph/ceph.client.admin.keyring`` . 
. Apply the configuration that includes the Ceph secret by using `kubectl apply`. Replace `CEPH_SECRET` with your Ceph secret. 
+

----
kubectl apply -f - << *EOF*
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-user
type: "kubernetes.io/rbd"
data:
  key: "$(echo CEPH_SECRET | base64)"
*EOF*
----
. Create the storage class:
+

----
kubectl apply -f - << *EOF*
apiVersion: storage.k8s.io/v1beta1
kind: StorageClass
metadata:
  name: SC_NAME
  annotations:
    storageclass.beta.kubernetes.io/is-default-class: "true"
provisioner: kubernetes.io/rbd
parameters:
  monitors: MONITOR1_IP:MONITOR1_PORT, MONITOR2_IP:MONITOR2_PORT, MONITOR3_IP:MONITOR3_PORT                  
  adminId: admin
  adminSecretName: ceph-secret-admin
  adminSecretNamespace: default
  pool: RBD_POOL
  userId: user
  userSecretName: ceph-secret-user
*EOF*
----
. Create the persistent volume claim: 
+

----
kubectl apply -f - << *EOF*
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: PVC_NAME
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: SIZE
*EOF*
----
+
Replace `SIZE` with the desired size of the volume.
Use the _gibibit_ notation, for example ``2Gi``. 

. Create a pod that uses the persistent volume claim.
+

----
kubectl apply -f - << *EOF*
apiVersion: v1
kind: Pod
metadata:
  name: POD_NAME
spec:
  containers:
  - name: CONTAINER_NAME
    image: IMAGE_NAME
    volumeMounts:
    - name: rbdvol
      mountPath: /mnt/rbdvol
      readOnly: false
  volumes:
  - name: rbdvol
    persistentVolumeClaim:
      claimName: PVC_NAME
*EOF*
----
. Verify that the pod exists and its status. 
+

----
kubectl get pod
----
. Once pod is running, check the volume: 
+

----
kubectl exec -it CONTAINER_NAME -- df -k ...
/dev/rbd3               999320      1284    929224   0% /mnt/rbdvol
...
----


In case you need to delete the pod, run the following command: 

----
kubectl delete pod CONTAINER_NAME
----

.Deleting A Pod
[NOTE]
====
When you delete the pod, the persistent volume claim is deleted as well.
The RBD is not deleted. 
====

== Using CephFS in a Pod


The procedure below describes steps to take when you need to use a CephFS in a Pod. 

.Procedure: Using CephFS In A Pod
. Retrieve the Ceph admin secret. Get the key value using command ``ceph auth get-key client.admin`` or, from the file [path]``/etc/ceph/ceph.client.admin.keyring`` . 
. Apply the configuration that includes the Ceph secret by using `kubectl apply`. Replace `CEPH_SECRET` with your Ceph secret. 
+

----
kubectl apply -f - << *EOF*
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-admin
type: "kubernetes.io/rbd"
data:
  key: "$(echo CEPH_SECRET | base64)"
*EOF*
----
. Create a pod that uses the image by executing the following command. In this example it is a minimal configuration for using a `CephFS` volume. Fill in the IP addresses and ports of your monitor nodes. The default port number is ``6789``. 
+

----
kubectl apply -f - << *EOF*
apiVersion: v1
kind: Pod
metadata:
  name: POD_NAME
spec:
  containers:
  - name: CONTAINER_NAME
    image: IMAGE_NAME
    volumeMounts:
    - mountPath: /mnt/cephfsvol
      name: ceph-vol
  volumes:
  - name: ceph-vol
    cephfs:
      monitors:
      - 'MONITOR1_IP:MONITOR1_PORT'
      - 'MONITOR2_IP:MONITOR2_PORT'
      - 'MONITOR3_IP:MONITOR3_PORT'
      user: admin
      secretRef:
        name: ceph-secret-admin
      readOnly: false
*EOF*
----
. Verify that the pod exists and its status: 
+

----
kubectl get pod
----
. Once the pod is running, check the mounted volume: 
+

----
kubectl exec -it CONTAINER_NAME -- df -k ...
/dev/rbd0           1003      21       962   3% /mnt/cephfsvol
...
----


In case you need to delete the pod, run the following command: 

----
kubectl delete pod POD_NAME 
----

== Using CephFS for Static Persistent Volumes


The following procedure describes how to attach a pod to a CephFS static persistent volume. 

.Procedure: Creating a Pod with CephFS as Persistent Volume
. Retrieve the Ceph admin secret. Get the key value using command ``ceph auth get-key client.admin`` or, from the file [path]``/etc/ceph/ceph.client.admin.keyring`` . 
. Apply the configuration that includes the Ceph secret by using `kubectl apply`. Replace `CEPH_SECRET` with your Ceph secret. 
+

----
kubectl apply -f - << *EOF*
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
type: "kubernetes.io/rbd"
data:
  key: "$(echo CEPH_SECRET | base64)"
*EOF*
----
. Create the persistent volume: 
+

----
kubectl apply -f - << *EOF*
apiVersion: v1
kind: PersistentVolume
metadata:
  name: PV_NAME
spec:
  capacity: 
    storage: SIZE
  accessModes:
    - ReadWriteOnce
  cephfs:
    monitors:
    - 'MONITOR1_IP:MONITOR1_PORT'
    - 'MONITOR2_IP:MONITOR2_PORT'
    - 'MONITOR3_IP:MONITOR3_PORT'
    user: admin
    secretRef:
      name: ceph-secret-admin
    readOnly: false
*EOF*
----
+
Replace `SIZE` with the desired size of the volume.
Use the _gibibit_ notation, for example ``2Gi``. 
. Create a persistent volume claim: 
+

----
kubectl apply -f - << *EOF*
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: PVC_NAME
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: SIZE
*EOF*
----
+
Replace `SIZE` with the desired size of the volume.
Use the _gibibit_ notation, for example ``2Gi``. 
+

. Create a pod that uses the persistent volume claim.
+

----
kubectl apply -f - <<*EOF*
apiVersion: v1
kind: Pod
metadata:
  name: POD_NAME
spec:
  containers:
  - name: CONTAINER_NAME
    image: IMAGE_NAME
    volumeMounts:
    - mountPath: /mnt/cephfsvol
      name: cephfsvol
  volumes:
  - name: cephfsvol
    persistentVolumeClaim:
      claimName: PVC_NAME

*EOF*
----
. Verify that the pod exists and its status.
+

----
kubectl get pod
----
. Once pod is running, check the volume by running: 
+

----
kubectl exec -it CONTAINER_NAME -- df -k ...
172.28.0.25:6789,172.28.0.21:6789,172.28.0.6:6789:/  76107776       0  76107776   0% /mnt/cephfsvol
...
----

In case you need to delete the pod, run the following command: 

----
kubectl delete pod CONTAINER_NAME
----

.Deleting A Pod
[NOTE]
====
When you delete the pod, the persistent volume claim is deleted as well.
The cephFS is not deleted. 
====
