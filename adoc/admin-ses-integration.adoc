= {ses} Integration


{productname} offers {ses} as a storage solution for its containers,
this chapter describes the steps required for successful integration.

== Prerequisites


Before you start with integrating {ses}, you need to ensure the following:

* The {productname} cluster must have `ceph-common` and `xfsprogs` installed on all nodes.
You can check this by running `rpm -q ceph-common` and `rpm -q xfsprogs`.
* The {productname} cluster can communicate with all of the following {ses} nodes:
master, monitoring nodes, OSD nodes and the metadata server (in case you need a shared file system).
For more details refer to the {ses} documentation:
https://www.suse.com/documentation/suse-enterprise-storage/.
* The {ses} cluster has a pool with RADOS Block Device (RBD) enabled.

== Procedures according to type of integration

The steps will differ in small details depending on whether you are using RBD or
CephFS and dynamic or static persistent volumes.


=== Using RBD in a Pod

RBD, also known as the Ceph Block Device or RADOS Block Device,
is software that facilitates the storage of block-based data in the open source
Ceph distributed storage system.
The procedure below describes steps to take when you need to use a RADOS Block Device in a pod.

. Retrieve the Ceph admin secret.
You can get the key value using the following command:
+
----
ceph auth get-key client.admin
----
or directly from `/etc/ceph/ceph.client.admin.keyring`.
. Apply the configuration that includes the Ceph secret by running `kubectl apply`.
Replace `CEPH_SECRET` with your own Ceph secret and run the following:
+

----
kubectl apply -f - << *EOF*
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
type: "kubernetes.io/rbd"
data:
  key: "$(echo CEPH_SECRET | base64)"
*EOF*
----
. Create an image in the SES cluster.
To do that, run the following command on the {master_node},
replacing `SIZE` with the size of the image, or example `2G`,
and `YOUR_VOLUME` with the name of the image.
+

----
rbd create -s SIZE YOUR_VOLUME
----
+
. Create a pod that uses the image by executing the command bellow.
 This example is the minimal configuration for using a RADOS Block Device.
 Fill in the IP addresses and ports of your monitor nodes under `MONITOR_IP` and `MONITOR_PORT`.
 The default port number is *6789*. Substitute
 `POD_NAME` and `CONTAINER_NAME` for a {kube} container and pod name of your choice.
 `IMAGE_NAME` is the name you decide to give your container image, for example "opensuse/leap".
 `RBD_POOL` is the RBD pool name,
 please refer to the RBD documentation on instructions how to create the RBD pool.
 https://docs.ceph.com/docs/mimic/rbd/rados-rbd-cmds/#create-a-block-device-pool
+

----
kubectl apply -f - << *EOF*
apiVersion: v1
kind: Pod
metadata:
  name: POD_NAME
spec:
  containers:
  - name: CONTAINER_NAME
    image: IMAGE_NAME
    volumeMounts:
    - mountPath: /mnt/rbdvol
      name: rbdvol
  volumes:
  - name: rbdvol
    rbd:
      monitors:
      - 'MONITOR1_IP:MONITOR1_PORT'
      - 'MONITOR2_IP:MONITOR2_PORT'
      - 'MONITOR3_IP:MONITOR3_PORT'
      pool: RBD_POOL
      image: YOUR_VOLUME
      user: admin
      secretRef:
        name: ceph-secret
      fsType: ext4
      readOnly: false
*EOF*
----
. Verify that the pod exists and check its status:
+

----
kubectl get pod
----
. Once the pod is running, check the mounted volume:
+

----
kubectl exec -it CONTAINER_NAME -- df -k ...
Filesystem             1K-block    Used    Available Used%   Mounted on
/dev/rbd1              999320      1284    929224    0%      /mnt/rbdvol
...
----

In case you need to delete the pod, run the following command:
----
kubectl delete pod POD_NAME
----

=== Using RBD with Static Persistent Volumes


The following procedure describes how to attach a pod to an RDB static persistent volume:

. Retrieve the Ceph admin secret.
You can get the key value using the following command:
+
----
ceph auth get-key client.admin
----
or directly from `/etc/ceph/ceph.client.admin.keyring`.
. Apply the configuration that includes the Ceph secret by using `kubectl apply`.
Replace `CEPH_SECRET` with your Ceph secret.
+

----
kubectl apply -f - << *EOF*
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
type: "kubernetes.io/rbd"
data:
  key: "$(echo CEPH_SECRET | base64)"
*EOF*
----
. Create an image in the SES cluster. On the {master_node} , run the following command:
+

----
rbd create -s SIZE YOUR_VOLUME
----
+
Replace `SIZE` with the size of the image, for example `2G` (2 Gigabyte), and `YOUR_VOLUME` is the name of the image.
. Create the persistent volume:
+

----
kubectl apply -f - << *EOF*
apiVersion: v1
kind: PersistentVolume
metadata:
  name: PV_NAME
spec:
  capacity:
    storage: SIZE
  accessModes:
    - ReadWriteOnce
  rbd:
    monitors:
    - 'MONITOR1_IP:MONITOR1_PORT'
    - 'MONITOR2_IP:MONITOR2_PORT'
    - 'MONITOR3_IP:MONITOR3_PORT'
    pool: RDB_POOL
    image: YOUR_VOLUME
    user: admin
    secretRef:
      name: ceph-secret
    fsType: ext4
    readOnly: false
*EOF*
----
+
Replace `SIZE` with the desired size of the volume.
Use the _gibibit_ notation, for example ``2Gi``.
. Create a persistent volume claim:
+

----
kubectl apply -f - << *EOF*
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: PVC_NAME
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: SIZE
*EOF*
----
+
Replace `SIZE` with the desired size of the volume.
Use the _gibibit_ notation, for example ``2Gi``.
+
.Listing Volumes
NOTE: This persistent volume claim does not explicitly list the volume.
Persistent volume claims work by picking any volume that meets the criteria from a pool.
In this case we specified any volume with a size of 2G or larger.
When the claim is removed the recycling policy will be followed.
+

. Create a pod that uses the persistent volume claim:
+

----
kubectl apply -f - <<*EOF*
apiVersion: v1
kind: Pod
metadata:
  name: POD_NAME
spec:
  containers:
  - name: CONTAINER_NAME
    image: IMAGE_NAME
    volumeMounts:
    - mountPath: /mnt/rbdvol
      name: rbdvol
  volumes:
  - name: rbdvol
    persistentVolumeClaim:
      claimName: PV_NAME
*EOF*
----
. Verify that the pod exists and its status:
+

----
kubectl get pod
----
. Once pod is running, check the volume:
+

----
kubectl exec -it CONTAINER_NAME -- df -k ...
/dev/rbd3               999320      1284    929224   0% /mnt/rbdvol
...
----


In case you need to delete the pod, run the following command:

----
kubectl delete pod CONTAINER_NAME
----

.Deleting A Pod
[NOTE]
====
When you delete the pod, the persistent volume claim is deleted as well.
The RBD is not deleted.
====


[[_RBD-dynamic-persistent-volumes]]
=== Using RBD with Dynamic Persistent Volumes


The following procedure describes how to attach a pod to an RDB dynamic persistent volume.

. Retrieve the Ceph *admin* secret.
You can get the key value using the following command:
+
----
ceph auth get-key client.admin
----
or directly from `/etc/ceph/ceph.client.admin.keyring`.
. Apply the configuration that includes the Ceph secret by using `kubectl apply`.
Replace `CEPH_SECRET` with your Ceph secret.
+

----
kubectl apply -f - << *EOF*
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-admin
type: "kubernetes.io/rbd"
data:
  key: "$(echo CEPH_SECRET | base64)"
*EOF*
----

. Create Ceph user on the SES cluster.
+

----
ceph auth get-or-create client.user mon "allow r" osd "allow class-read object_prefix rbd_children,
allow rwx pool=RBD_POOL" -o ceph.client.user.keyring
----
+
Replace `RBD_POOL` with the RBD pool name.

. For a dynamic persisten volume, you will also need a user key.
Retrieve the Ceph *user* secret by running:
+
----
ceph auth get-key client.user
----
or directly from `/etc/ceph/ceph.client.user.keyring`
. Apply the configuration that includes the Ceph secret by with the `kubectl apply` command,
replacing `CEPH_SECRET` with your own Ceph secret.
+

----
kubectl apply -f - << *EOF*
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret-user
type: "kubernetes.io/rbd"
data:
  key: "$(echo CEPH_SECRET | base64)"
*EOF*
----
. Create the storage class:
+

----
kubectl apply -f - << *EOF*
apiVersion: storage.k8s.io/v1beta1
kind: StorageClass
metadata:
  name: SC_NAME
  annotations:
    storageclass.beta.kubernetes.io/is-default-class: "true"
provisioner: kubernetes.io/rbd
parameters:
  monitors: MONITOR1_IP:MONITOR1_PORT, MONITOR2_IP:MONITOR2_PORT, MONITOR3_IP:MONITOR3_PORT
  adminId: admin
  adminSecretName: ceph-secret-admin
  adminSecretNamespace: default
  pool: RBD_POOL
  userId: user
  userSecretName: ceph-secret-user
*EOF*
----
. Create the persistent volume claim:
+

----
kubectl apply -f - << *EOF*
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: PVC_NAME
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: SIZE
*EOF*
----
+
Replace `SIZE` with the desired size of the volume.
Use the _gibibit_ notation, for example ``2Gi``.

. Create a pod that uses the persistent volume claim.
+

----
kubectl apply -f - << *EOF*
apiVersion: v1
kind: Pod
metadata:
  name: POD_NAME
spec:
  containers:
  - name: CONTAINER_NAME
    image: IMAGE_NAME
    volumeMounts:
    - name: rbdvol
      mountPath: /mnt/rbdvol
      readOnly: false
  volumes:
  - name: rbdvol
    persistentVolumeClaim:
      claimName: PVC_NAME
*EOF*
----
. Verify that the pod exists and check its status.
+

----
kubectl get pod
----
. Once pod is running, check the volume:
+

----
kubectl exec -it CONTAINER_NAME -- df -k ...
/dev/rbd3               999320      1284    929224   0% /mnt/rbdvol
...
----


In case you need to delete the pod, run the following command:

----
kubectl delete pod CONTAINER_NAME
----

.Deleting A Pod
[NOTE]
====
When you delete the pod, the persistent volume claim is deleted as well.
The RBD is not deleted.
====

=== Using CephFS in a Pod


The procedure below describes steps to take when you need to use a CephFS in a Pod.

.Procedure: Using CephFS In A Pod


. Retrieve the Ceph admin secret.
You can get the key value using the following command:
+
----
ceph auth get-key client.admin
----
or directly from `/etc/ceph/ceph.client.admin.keyring`.
. Apply the configuration that includes the Ceph secret by running `kubectl apply`.
Replace `CEPH_SECRET` with your own Ceph secret and run the following:
+

----
kubectl apply -f - << *EOF*
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
type: "kubernetes.io/rbd"
data:
  key: "$(echo CEPH_SECRET | base64)"
*EOF*
----
. Create a pod that uses the image by executing the following command.
This example shows the minimal configuration for a `CephFS` volume.
Fill in the IP addresses and ports of your monitor nodes. The default port number is ``6789``.
+

----
kubectl apply -f - << *EOF*
apiVersion: v1
kind: Pod
metadata:
  name: POD_NAME
spec:
  containers:
  - name: CONTAINER_NAME
    image: IMAGE_NAME
    volumeMounts:
    - mountPath: /mnt/cephfsvol
      name: ceph-vol
  volumes:
  - name: ceph-vol
    cephfs:
      monitors:
      - 'MONITOR1_IP:MONITOR1_PORT'
      - 'MONITOR2_IP:MONITOR2_PORT'
      - 'MONITOR3_IP:MONITOR3_PORT'
      user: admin
      secretRef:
        name: ceph-secret-admin
      readOnly: false
*EOF*
----
. Verify that the pod exists and check its status:
+

----
kubectl get pod
----
. Once the pod is running, check the mounted volume:
+

----
kubectl exec -it CONTAINER_NAME -- df -k ...
/dev/rbd0           1003      21       962   3% /mnt/cephfsvol
...
----


In case you need to delete the pod, run the following command:

----
kubectl delete pod POD_NAME
----

=== Using CephFS with Static Persistent Volumes


The following procedure describes how to attach a pod to a CephFS static persistent volume:

. Retrieve the Ceph admin secret.
You can get the key value using the following command:
+
----
ceph auth get-key client.admin
----
or directly from `/etc/ceph/ceph.client.admin.keyring`.
. Apply the configuration that includes the Ceph secret by running `kubectl apply`.
Replace `CEPH_SECRET` with your own Ceph secret and run the following:
+

----
kubectl apply -f - << *EOF*
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
type: "kubernetes.io/rbd"
data:
  key: "$(echo CEPH_SECRET | base64)"
*EOF*
----
. Create the persistent volume:
+

----
kubectl apply -f - << *EOF*
apiVersion: v1
kind: PersistentVolume
metadata:
  name: PV_NAME
spec:
  capacity:
    storage: SIZE
  accessModes:
    - ReadWriteOnce
  cephfs:
    monitors:
    - 'MONITOR1_IP:MONITOR1_PORT'
    - 'MONITOR2_IP:MONITOR2_PORT'
    - 'MONITOR3_IP:MONITOR3_PORT'
    user: admin
    secretRef:
      name: ceph-secret-admin
    readOnly: false
*EOF*
----
+
Replace `SIZE` with the desired size of the volume.
Use the _gibibit_ notation, for example ``2Gi``.
. Create a persistent volume claim:
+

----
kubectl apply -f - << *EOF*
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: PVC_NAME
spec:
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: SIZE
*EOF*
----
+
Replace `SIZE` with the desired size of the volume.
Use the _gibibit_ notation, for example ``2Gi``.
+

. Create a pod that uses the persistent volume claim.
+

----
kubectl apply -f - <<*EOF*
apiVersion: v1
kind: Pod
metadata:
  name: POD_NAME
spec:
  containers:
  - name: CONTAINER_NAME
    image: IMAGE_NAME
    volumeMounts:
    - mountPath: /mnt/cephfsvol
      name: cephfsvol
  volumes:
  - name: cephfsvol
    persistentVolumeClaim:
      claimName: PVC_NAME

*EOF*
----
. Verify that the pod exists and check its status.
+

----
kubectl get pod
----
. Once pod is running, check the volume by running:
+

----
kubectl exec -it CONTAINER_NAME -- df -k ...
172.28.0.25:6789,172.28.0.21:6789,172.28.0.6:6789:/  76107776       0  76107776   0% /mnt/cephfsvol
...
----

In case you need to delete the pod, run the following command:

----
kubectl delete pod CONTAINER_NAME
----

.Deleting A Pod
[NOTE]
====
When you delete the pod, the persistent volume claim is deleted as well.
The cephFS is not deleted.
====
