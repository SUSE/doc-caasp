== Product Description

{productname} is a {cncf} certified Kubernetes distribution on top of {sle}.

{productname} automates the orchestration and management of containerized applications and services with powerful Kubernetes capabilities, including:

* Workload scheduling optimizes hardware utilization while taking the container requirements into account.
* Service proxies provide single IP addresses for services and distribute the load between containers.
* Application scaling up and down accommodates changing loads.
* Non-disruptive rollout/rollback of new applications and updates enables frequent changes without downtime.
* Health monitoring and management supports application self-healing and ensures application availability.

In addition, {productname} simplifies the platform operatorâ€™s experience, with everything you need to get up and running quickly, and to manage the environment effectively in production. It provides:

* A complete container execution environment, container run time (CRI-O), and container image registries (registry.suse.com).
* Application ecosystem support with {sle} container base images, and access to tools and services offered by SUSE Ready for CaaS Platform partners and the Kubernetes community.
* Enhanced datacenter integration features that enable you to plug Kubernetes into new or existing infrastructure, systems, and processes.
* End-to-End security, implemented holistically across the full stack including network policies via Cilium, `PodSecurityPolicies` and {kube} RBAC.
* Advanced platform management that simplifies platform installation, configuration, re-configuration, monitoring, maintenance, updates, and recovery.
* Enterprise hardening including comprehensive interoperability testing, support for thousands of platforms, and world-class platform maintenance and technical support.

You can deploy {productname} onto physical servers or use it on virtual machines. After deployment, it is immediately ready to run and provides a highly-scalable cluster.

{productname} inherits benefits of SUSE Linux Enterprise and uses tools and technologies well-known to system administrators such as cloud-init or AutoYaST.

For more information, including a list of the various components which make up {productname} please refer to the Release Notes on https://www.suse.com/releasenotes/.

image::caasp_cluster_components.png[SUSE CaaSP Components, width=100%,pdfwidth=80%]

=== Product Release Cycle

Product releases are aligned with {sle} releases and {kube}. Major version jumps
are determined by the {sle} release cycle. Minor version jumps by the quarterly {kube} releases.
Individual maintenance updates with receive patchlevel numbering.

Versioning scheme: `x.y.z`

* x - Base OS version
* y - Kubernetes version
* z - Patchlevel (incl. new features !)

[%autowidth.stretch]
|===
|Version |Base OS |{kube} version |Patchlevel

|4.0.0
|{sls} {base_os_version}
^|1.15
^|0

|4.0.1
|{sls} {base_os_version}
^|1.15
^|1

|...
|
|
|

|4.1.0
|{sls} {base_os_version}
^|1.16
^|0

|...
|
|
|

|5.0.0
|{sle} 15 SP2
^|tbd
^|0
|===


== Supported Platforms

[NOTE]
====
{productname} currently only runs on **x86_64** architectures.
====

The following platforms are currently supported:

* SUSE OpenStack Cloud 8
* VMware ESXi {vmware_version}
* KVM
* Bare Metal x86_64
* Amazon Web Services (technological preview)

== The {productname} stack

All long-lived components run in containers, with the exception of:

* Kubelet
* CRI-O

All Kubernetes components critical to the infrastructure are created
inside the `kube-system` namespace. As a user, you can create as many
namespaces as necessary for your operations and enforce different
permissions using RBAC rules.

`kubeadm` is what drives the deployment of new machines, and this
component runs on-demand, uncontainerized during the bootstrap or join
procedures.

=== Base Operating System

* {slea} {base_os_version}
** Kernel: `kernel-default` 4.12.14 or greater
** Filesystem: XFS or BTRFS

==== Software Management

CaaSP is distributed as a dedicated repository. All required packages
to deploy a node to the cluster are installable via a pattern. This pattern
will automatically be installed by `skuba` when bootstrapping or
joining a node.

** Extension to base OS image
** Software distribution channels
** Packages
** Container ecosystem
*** Helm
*** SUSE container registry

include::architecture-updates.adoc[leveloffset=+2]

=== Software Versions

{productname} {productmajor} ships with the following components and their
respective dependencies (not listed).

[options="header",cols="<,^",%autowidth.stretch]
|===
^|Component ^|Version

|image:logo_kubernetes.png[Kubernetes,width=200,pdfwidth=20%,height=70]

link:https://kubernetes.io/[]
|*{kube_version}*

|image:logo_crio.svg[CRI-O,width=200,pdfwidth=20%,height=70]

link:https://cri-o.io/[]
|*{crio_version}*

|image:logo_cilium.png[Cilium,width=200,pdfwidth=20%,height=70]

link:https://cilium.io[]
|*{cilium_version}*

|image:logo_etcd.svg[etcd,width=200,pdfwidth=20%,height=70]

link:https://github.com/etcd-io/etcd[]
|*{etcd_version}*

// |image:logo_dex.png[dex,width=200,pdfwidth=20%,height=70]
//
// link:https://github.com/dexidp/dex[]
// |*{dex_version}*

|image:logo_kured.png[Kured,width=100,pdfwidth=20%,height=70] Kured

link:https://github.com/weaveworks/kured[]
|*{kured_version}*

// |Gangway
//
// link:https://github.com/heptiolabs/gangway[]
// |*{gangway_version}*
|===

=== Networking

In the networking level, several concepts need to be defined:

* Container Network Interface (CNI)
+
The default plugin providing CNI
in {productname} is Cilium. CNI forms an overlay network,
allowing for pods running in different machines in the cluster to
communicate with each other transparently.
* Network policies
+
Network policies allow for security in terms of
routing within the cluster. They define how a groups of pods are
allowed to communicate with each other and with other network
endpoints.
+
** Network policies allow for fine grained restrictions on what
  networks are reachable, both in ingress and egress traffic.
* Kube proxy
+
The kube proxy is a service that runs containerized in
all machines of the cluster and allows for pod to service
communication. When you expose a service, and some other pod
consumes this service, the kube proxy is the responsible for setting
the rules on every machine so the service is reachable from within
the pod using that service.

* Envoy
+
Envoy is a high performance service and edge proxy that provides
L7 policy support. Envoy is an application language neutral service and
hence can work along with services written in different languages, even
though the core is written in C++ language. Envoy supports Go-extensions
and WASM support that makes it more adoptable by other apps. Envoy supports
mulitple protocols such as HTTP/1.1, HTTP/2, gRPC, MangoDB, DynamMoDB etc.
Envoy supports L3/L4 filtering. Envoy provides Advanced load balancing
comparable to NGINX. Envoy supports Health checking, observability and
Service Discovery.

Cilium 1.6 uses Envoy for the L7 Network Policies.

* Cilium KVStore free operation
+
Cilium can now operate entirely without a KVstore in the context of Kubernetes.

* Cilium Socket-based load-balancing
+
Cilium's Socket-based load-balancing combines the advantages of client-side
loadbalancing and network based load-balancing to provide a transparent
loadbalancing service through kubernetes to map the ServiceIP to the
Endpoint IP are done only once during connection establishment.

* Cilium Generic CNI chaining
+
CNI chaining framework is introduced in cilium 1.6 release that allows
to run Cilium along with other CNI's such as  Weave, Calico, Flannel,
AWS VPC CNI or the Lyft CNI plugin. This allows to utilize the eBPF
based security policy enforcements and its features while getting the
basic networking support from the legacy CNI plugin in-use.

* Cilium Native AWS ENI mode
+

Cilium 1.6 has introduced a new IPAM mode, AWS ENI mode that works along with
a new operator-based design. This helps when running services in the AWS and
managing IPAM from the operator defined pools while getting the network policy
enforcement from Cilium..

* Cilium Policy scalability improvements
+
Cilium 1.6 provides an improved and scalable policy system that decouples
handling of policy and identity definitions while moving to an entirely
incremental model.

== Deployment Scenarios

=== Kubernetes components

In Kubernetes there are two different machine types:

* Control plane (called "Masters")
* Workers

Control plane machines are responsible for running the main Kubernetes
components, this includes:

* etcd
** etcd is a distributed key value store. It's where all data from the
   cluster is persisted.
* API server
** The API server is responsible for serving all the API endpoints
   that are used by the different Kubernetes components and clients.
* Main controllers
** The main controllers are responsible for most of the core
   functionality from Kubernetes. When you create a Deployment, a
   controller will be responsible for creating the ReplicaSet, and in
   the same way, Pods will be created out of the ReplicaSet by a
   controller as well.
* Scheduler
** The scheduler is the component that assigns pods to the different
   nodes based on a number of restrictions and is aware of individual
   and collective resource requirements.

Both the control plane and worker nodes run an agent called kubelet
and a container runtime (cri-o). The kubelet is responsible for
talking to the container runtime, managing the Pod lifecycle that were
assigned to this machine. The container runtime, is the component that
will create the containers themselves.

=== High Availability Considerations

The default deployment aims for a "High Availability" (HA) Kubernetes service.
In order to achieve HA it's required to run several control planes.

Not any number greater than 1 is optimal for HA, and this can impact
the fault tolerance. The reason for this is the etcd distributed key
value store:

[options="header",width=40%,cols="^,^"]
|===
|Cluster size |Failure Tolerance
|1 | 0
|2 | 0
|3 | 1
|4 | 1
|5 | 2
|6 | 2
|7 | 3
|8 | 3
|9 | 4
|===

Given that etcd runs only on the control plane nodes, having 2
control plane nodes provides a HA solution that is fault tolerant for the
{kube} components but **not** for the etcd cluster. If one of those two
control planes nodes goes down, the cluster storage will be unavailable, and
the cluster won't be able to accept new changes (already running
workloads won't suffer any changes, but the cluster won't be able to
react to new changes from that point on).

In order to provide a fault tolerant HA environment you must
have an odd number of control planes.

A minimum of `3` master nodes is required in order to tolerate a complete loss
of one control plane node.

Control planes are only part of the whole picture. Most components will
talk to the API Server, and the API Server must be exposed on all master nodes
to communicate to the clients and the rest of the cluster components.

==== Load Balancer

The most reasonable way to achieve fault tolerant exposure of the API servers is
a load balancer. The load balancer will point to all the control plane nodes.
The clients and {kube} components will talk to the load balancer: which will
perform health checks against all the control planes and maintain an active pool
of backends.

If only one load balancer is deployed this creates a single point of failure.
For a complete HA solution, more than one load balancer is required.

[IMPORTANT]
If your environment only contains one load balancer it cannot be considered
highly available or fault tolerant, since the load balancer becomes
the single point of failure.

=== Testing / POC

The smallest possible deployment comes without a load balancer and the minimum
amount of nodes to be considered a cluster. This deployment type is in no way
suitable for production use and has no fault tolerance whatsoever.

* One master machine
* One worker machine

Despite not recommended, it's also possible to create a POC or testing
environment with a single master machine.

=== Default Deployment

The default minimal HA scenario requires 5 nodes:

* 2 Load Balancers
* 3 Masters

plus, the number of workers necessary to host your workloads.

* Requires:
** Persistent IP addresses on all nodes.
** NTP server provided on the host network.
** DNS entry that resolves to the load balancer VIP.
** LDAP server or OIDC provider (Active Directory, GitLab, GitHub, etc.)

* (Optional) "Infrastructure node"
** LDAP server if LDAP integration is desired and your organization
   does not have an LDAP server.
** Local RMT server to synchronize RPMs.
** Local mirror of the SUSE container registry (registry.suse.com)
** Local mirror of the SUSE helm chart repository.

=== Air Gapped Deployment

In air gapped environments, the "Infrastructure node" is mandatory, as
it's needed to have a local RMT server mirroring the CaaSP
repositories, a mirror of the SUSE container registry and a mirror of
the SUSE helm chart repository.


=== Control plane nodes certificates

Certificates are stored under `/etc/kubernetes/pki` on the control plane nodes.

==== CA certificates

[options="header"]
|===
|Path        |Valid for |Common Name |Description
|ca.crt      |1 year    |kubernetes  |Kubernetes global CA
|etcd/ca.crt |1 year    |etcd-ca     |etcd global CA
|===

==== CA certificate keys

[options="header"]
|===
|Path        |Key type |Key length (bits)
|ca.key      |RSA      |2048
|etcd/ca.key |RSA      |2048
|===

==== Certificates

[options="header"]
|===
|Path                         |Valid for |CN                            |Parent CA  |O (Subject)    |Kind           |Extra SANs
|apiserver-kubelet-client.crt |1 year    |kube-apiserver-kubelet-client |kubernetes |system:masters |client         |-
|etcd/healthcheck-client.crt  |1 year    |kube-etcd-healthcheck-client  |etcd-ca    |system:masters |client         |-
|etcd/server.crt              |1 year    |master                        |etcd-ca    |-              |server         |Hostname, IP address, localhost, 127.0.0.1, 0:0:0:0:0:0:0:1
|etcd/peer.crt                |1 year    |master                        |etcd-ca    |-              |server, client |Hostname, IP address, localhost, 127.0.0.1, 0:0:0:0:0:0:0:1
|apiserver.crt                |1 year    |kube-apiserver                |kubernetes |-              |server         |Hostname, IP address, Control Plane Address, kubernetes, kubernetes.default, kubernetes.default.svc, kubernetes.default.svc.cluster.local
|apiserver-etcd-client.crt    |1 year    |kube-apiserver-etcd-client    |etcd-ca    |system:masters |client         |-
|===

==== Certificate keys

[options="header",cols="<,^,^"]
|===
|Path                         |Key type |Key length (bits)
|apiserver.key                |RSA      |2048
|apiserver-kubelet-client.key |RSA      |2048
|apiserver-etcd-client.key    |RSA      |2048
|etcd/server.key              |RSA      |2048
|etcd/healthcheck-client.key  |RSA      |2048
|etcd/peer.key                |RSA      |2048
|===

Please refer to the link:{docurl}single-html/caasp-admin/#_certificates[{productname} Administration Guide] for more information on the rotation of certificates.

=== Worker nodes certificates

The CA certificate for the cluster is stored under /etc/kubernetes/pki
on the worker nodes.

When a new worker machine joins the cluster, the kubelet performs a
TLS bootstrap. It requests a certificate to the cluster using a
bootstrap token, this request is automatically approved by the
cluster, and a certificate is created. The new worker downloads this
certificate and writes it to disk, so the kubelet uses this
client certificate from now on to contact the apiserver.

==== CA certificates

The CA certificate is downloaded from the cluster (present in the
cluster-info secret inside the kube-public namespace). Since this
information is public, there's no restriction to download the CA
certificate.

This certificate is saved under /etc/kubernetes/pki/ca.crt.

[options="header"]
|===
|Path                             |Valid for |Common Name         |Description
|/var/lib/kubelet/pki/kubelet.crt |1 year    |worker-ca@random-id |Kubelet CA
|===

==== CA certificate keys

[options="header",cols="<,^,^"]
|===
|Path        |Key type |Key length (bits)
|kubelet.key |RSA      |2048
|===

==== Certificates

Certificates are stored under `/var/lib/kubelet/pki` on the worker nodes.

[options="header"]
|===
|Path                       |Valid for |CN                 |Parent CA           |O (Subject)  |Kind   |Extra SANs |Notes
|kubelet-client-current.pem |1 year    |system:node:worker |kubernetes          |system:nodes |client |-          |Symlink to kubelet-client-timestamp.pem
|kubelet.crt                |1 year    |worker@random-id   |worker-ca@random-id |-            |server |Hostname   |-
|===

==== Certificate keys

[options="header",cols="<,^,^"]
|===
|Path        |Key type |Key length (bits)
|kubelet.key |RSA      |2048
|===

Please refer to the link:{docurl}single-html/caasp-admin/#_certificates[{productname} Administration Guide] for more information on the rotation of certificates.

=== Cluster Management

Cluster lifecycle is managed using `skuba`. It enables you to
manage nodes in your cluster:

* Bootstrap a new cluster
* Join new machines to the cluster
** Master nodes
** Worker nodes
* Remove nodes from the cluster
** Master nodes
** Worker nodes

==== Creating a cluster definition

Creating a cluster definition is the first step to initialize your
cluster. You can execute this task as follows:

----
~/clusters$ skuba cluster init --control-plane 10.86.3.149 my-cluster
[init] configuration files written to /home/my-user/clusters/my-cluster
----

This operation happens strictly offline and will generate a folder
structure like the following:

----
~/clusters > tree my-cluster/
my-cluster/
â”œâ”€â”€ addons
â”‚Â Â  â”œâ”€â”€ cilium
â”‚   â”‚   â”œâ”€â”€ base
â”‚   â”‚   â”‚   â””â”€â”€ cilium.yaml
â”‚Â Â  â”‚Â Â  â””â”€â”€ patches
â”‚Â Â  â”œâ”€â”€ cri
â”‚Â Â  â”‚Â Â  â””â”€â”€ default_flags
â”‚Â Â  â”œâ”€â”€ dex
â”‚   â”‚   â”œâ”€â”€ base
â”‚   â”‚   â”‚   â””â”€â”€ dex.yaml
â”‚Â Â  â”‚Â Â  â””â”€â”€ patches
â”‚Â Â  â”œâ”€â”€ gangway
â”‚   â”‚   â”œâ”€â”€ base
â”‚   â”‚   â”‚   â””â”€â”€ gangway.yaml
â”‚Â Â  â”‚Â Â  â””â”€â”€ patches
â”‚Â Â  â”œâ”€â”€ kured
â”‚   â”‚   â”œâ”€â”€ base
â”‚   â”‚   â”‚   â””â”€â”€ kured.yaml
â”‚Â Â  â”‚Â Â  â””â”€â”€ patches
â”‚Â Â  â””â”€â”€ psp
â”‚       â”œâ”€â”€ base
â”‚       â”‚   â””â”€â”€ psp.yaml
â”‚Â Â  Â Â   â””â”€â”€ patches
â”œâ”€â”€ kubeadm-init.conf
â””â”€â”€ kubeadm-join.conf.d
    â”œâ”€â”€ master.conf.template
    â””â”€â”€ worker.conf.template

18 directories, 9 files
----

At this point, you can inspect all generated files, and if desired you
can experiment by providing your custom settings using with declarative
management of Kubernetes objects using link:https://kustomize.io/[Kustomize].

To provide custom settings of the form of strategic merge patch or a JSON 6902 patch go to addons patches directory for example `addons/dex/pathes` and create custom settings file for example `addons/dex/pathes/custom.yaml`

Read https://github.com/kubernetes-sigs/kustomize/blob/master/docs/glossary.md#patchstrategicmerge and https://github.com/kubernetes-sigs/kustomize/blob/master/docs/glossary.md#patchjson6902 to get more information.

----
apiVersion: apps/v1
kind: Deployment
metadata:
  name: oidc-dex
  namespace: kube-system
spec:
  replicas: 1
  revisionHistoryLimit: 5
----


==== Bootstrapping the first node of the cluster

From within your cluster definition folder, you can bootstrap your
first master machine:

----
~/clusters/my-cluster$ skuba node bootstrap --user sles --sudo --target <IP_ADDRESS/FQDN> <NODENAME>
----

This operation will read the `kubeadm-init.conf` file inside your
cluster definition, will forcefully set certain settings to the ones
required by CaaSP and will bootstrap the node remotely.

Prior to bootstrap it's possible for you to tweak the configuration
that will be used to create the cluster. You can:

* Tweak the default Pod Security Policies or create extra ones. If you
  place extra Pod Security Policies in the `addons/psp/base` folder, those
  will be created as well when the bootstrap is completed. You can
  also modify the default ones and/or remove them.

* Inspect the `kubeadm-init.conf` and set extra configuration settings
  supported by `kubeadm`. The latest supported version is {kubeadm_api_version}.

After this operation has completed several modifications will have
happened on your cluster definition folder:

* The `kubeadm-init.conf` file will contain the final complete
  contents used to bootstrap the node, so you can inspect what
  exact configuration was used to bootstrap the node.

* An `admin.conf` file will be created on your cluster definition file,
  this is a `kubeconfig` file that has complete access to the cluster
  and uses client certificates for authenticating against the cluster.

==== Adding master nodes to the cluster

Adding new master nodes to the cluster can be achieved by executing
the following `skuba` command:

----
~/clusters/my-cluster$ skuba node join --role master --user sles --sudo --target <IP_ADDRESS/FQDN> <NODENAME>
----

This operation will try to read the `kubeadm-join.conf.d/<IP_ADDRESS/FQDN>.conf`
file if it exists. This allows you to set specific settings for this
new node prior to joining it (a similar procedure to how
`kubeadm-init.conf` file behaves when bootstrapping). If this file
does not exist, `skuba` will read the `kubeadm-join.conf.d/master.conf.template`
instead and will create this file automatically when `skuba node
join` is called.

This operation will increase the `etcd` member count by one, so it's
recommended to always keep an odd number of master nodes because as
described in previous sections an even number of nodes does not
improve fault tolerance.

==== Adding worker nodes to the cluster

Adding new worker nodes to the cluster can be achieved by executing
the following `skuba` command:

----
~/clusters/my-cluster$ skuba node join --role worker --user sles --sudo --target <IP_ADDRESS/FQDN> <NODENAME>
----

This operation will try to read the `kubeadm-join.conf.d/<IP_ADDRESS/FQDN>.conf`
file if it exists. This allows you to set specific settings for this
new node prior to joining it (a similar procedure to how
`kubeadm-init.conf` file behaves when bootstrapping). If this file
does not exist, `skuba` will read the `kubeadm-join.conf.d/worker.conf.template`
instead and will create this file automatically when `skuba node
join` is called.

==== Removing nodes from the cluster

Removing nodes from the cluster requires you to execute `skuba`
from a folder containing an `admin.conf` file, because this operation
is performed exclusively using Kubernetes, and no access to the node
being removed or other nodes for that matter is required.

For removing a node, the following command has to be executed:

----
~/clusters/my-cluster$ skuba node remove <NODENAME>
----

If the node to be removed is a master, specific actions will be
automatically executed, like removing the etcd member from the cluster.
Note that this node cannot be added back to the cluster or any other
skuba-initiated kubernetes cluster without reinstalling first.
