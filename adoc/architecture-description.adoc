== Product Description

{productname} is a {cncf} certified Kubernetes distribution on top of {sle}.

{productname} automates the orchestration and management of containerized applications and services with powerful Kubernetes capabilities, including:

* Workload scheduling optimizes hardware utilization while taking the container requirements into account.
* Service proxies provide single IP addresses for services and distribute the load between containers.
* Application scaling up and down accommodates changing loads.
* Non-disruptive rollout/rollback of new applications and updates enables frequent changes without downtime.
* Health monitoring and management supports application self-healing and ensures application availability.

In addition, {productname} simplifies the platform operator’s experience, with everything you need to get up and running quickly, and to manage the environment effectively in production. It provides:

* A complete container execution environment, container run time (CRI-O), and container image registries (registry.suse.com).
* Application ecosystem support with {sle} container base images, and access to tools and services offered by SUSE Ready for CaaS Platform partners and the Kubernetes community.
* Enhanced datacenter integration features that enable you to plug Kubernetes into new or existing infrastructure, systems, and processes.
* End-to-End security, implemented holistically across the full stack including network policies via Cilium, `PodSecurityPolicies` and {kube} RBAC.
* Advanced platform management that simplifies platform installation, configuration, re-configuration, monitoring, maintenance, updates, and recovery.
* Enterprise hardening including comprehensive interoperability testing, support for thousands of platforms, and world-class platform maintenance and technical support.

You can deploy {productname} onto physical servers or use it on virtual machines. After deployment, it is immediately ready to run and provides a highly-scalable cluster.

{productname} inherits benefits of SUSE Linux Enterprise and uses tools and technologies well-known to system administrators such as cloud-init or AutoYaST.

For more information, including a list of the various components which make up {productname} please refer to the Release Notes on https://www.suse.com/releasenotes/.

image::architecture-caasp-components.jpg[SUSE CaaSP Components, width=100%,pdfwidth=80%]

=== Product Release Cycle

Product releases are aligned with {sle} releases and {kube}. Major version jumps
are determined by the {sle} release cycle. Minor version jumps by the quarterly {kube} releases.
Individual maintenance updates with receive patchlevel numbering.

Versioning scheme: `x.y.z`

* x - Base OS version
* y - Kubernetes version
* z - Patchlevel (incl. new features !)

[%autowidth.stretch]
|===
|Version |Base OS |{kube} version |Patchlevel

|4.0.0
|{sle} 15 SP1
^|1.15
^|0

|4.0.1
|{sle} 15 SP1
^|1.15
^|1

|...
|
|
|

|4.1.0
|{sle} 15 SP1
^|1.16
^|0

|...
|
|
|

|5.0.0
|{sle} 15 SP2
^|tbd
^|0
|===


== Supported Platforms

* SUSE OpenStack Cloud 8
* VMware ESXi 6.0.0 Update 3
* Bare Metal

== Supported Architectures

* x86_64

=== Public Cloud

* Google Compute Engine (GCE)
* Microsoft Azure (Azure)
* Amazon Web Services (AWS)

== The {productname} stack

All long-lived components run in containers, with the exception of:

* kubelet
* cri-o

All Kubernetes components critical to the infrastructure are created
inside the `kube-system` namespace. As a user, you can create as many
namespaces as necessary for your operations and enforce different
permissions using RBAC rules.

`kubeadm` is what drives the deployment of new machines, and this
component runs on-demand, uncontainerized during the bootstrap or join
procedures.

=== Base Operating System

* SLE 15 SP1
** Kernel: exact version TBD
** Filesystem: XFS

==== Software Management

CaaSP is distributed as a dedicated repository. All required packages
to deploy a node to the cluster are installable via a pattern.

** extension to base OS image
** software distribution channels
** packages
** container ecosystem
*** helm
*** SUSE container registry

include::architecture-updates.adoc[leveloffset=+2]

=== Software Versions

{productname} {productmajor} ships with the following components and their
respective dependencies (not listed).

[options="header",cols="<,^",%autowidth.stretch]
|===
^|Component ^|Version

|image:logo_kubernetes.png[Kubernetes,width=200,pdfwidth=20%,height=70]

link:https://kubernetes.io/[]
|*{kube_version}*

|image:logo_crio.svg[CRI-O,width=200,pdfwidth=20%,height=70]

link:https://cri-o.io/[]
|*{crio_version}*

|image:logo_cilium.png[Cilium,width=200,pdfwidth=20%,height=70]

link:https://cilium.io[]
|*{cilium_version}*

|image:logo_etcd.svg[etcd,width=200,pdfwidth=20%,height=70]

link:https://github.com/etcd-io/etcd[]
|*{etcd_version}*

// |image:logo_dex.png[dex,width=200,pdfwidth=20%,height=70]
//
// link:https://github.com/dexidp/dex[]
// |*{dex_version}*

|image:logo_kured.png[Kured,width=100,pdfwidth=20%,height=70] Kured

link:https://github.com/weaveworks/kured[]
|*{kured_version}*

// |Gangway
//
// link:https://github.com/heptiolabs/gangway[]
// |*{gangway_version}*
|===

=== Container Runtime environment

* CRI-O
* orchestration
** etcd only on master nodes never on workers
* special considerations
** From https://github.com/SUSE/avant-garde/issues/15#issuecomment-485785102:
+
----
One thing we could extract from my experience, is that we might need to document this and inform our support engineers that kubelet is responsible for bringing those specific containers up. So in case of debugging in a customer's environment they don't accidentally create even more mess by trying to run api-server as a workload on the workers
----

=== Networking

In the networking level, several concepts need to be defined:

* Container Network Interface (CNI)
+
The default plugin providing CNI
in {productname} is Cilium. CNI forms an overlay network,
allowing for pods running in different machines in the cluster to
communicate with each other transparently.
* Network policies
+
Network policies allow for security in terms of
routing within the cluster. They define how a groups of pods are
allowed to communicate with each other and with other network
endpoints.
+
** Network policies allow for fine grained restrictions on what
  networks are reachable, both in ingress and egress traffic.
* Kube proxy
+
The kube proxy is a service that runs containerized in
all machines of the cluster and allows for pod to service
communication. When you expose a service, and some other pod
consumes this service, the kube proxy is the responsible for setting
the rules on every machine so the service is reachable from within
the pod using that service.

=== Storage

* Storage
* integrations
* defaults

=== Configuration Management and Orchestration

* skuba
* etcd
* cloud-init
* AutoYaST

==== Integrations

* integration points with external services and applications

== Deployment Scenarios

In Kubernetes there are two different machine types:

* Control plane (called "Masters")
* Workers

Control plane machines are responsible for running the main Kubernetes
components, this includes:

* etcd
** etcd is a distributed key value store. It's where all data from the
   cluster is persisted.
* API server
** The API server is responsible for serving all the API endpoints
   that are used by the different Kubernetes components and clients.
* Main controllers
** The main controllers are responsible for most of the core
   functionality from Kubernetes. When you create a Deployment, a
   controller will be responsible for creating the ReplicaSet, and in
   the same way, Pods will be created out of the ReplicaSet by a
   controller as well.
* Scheduler
** The scheduler is the component that assigns pods to the different
   nodes based on a number of restrictions and is aware of individual
   and collective resource requirements.

Both the control plane and worker nodes run an agent called kubelet
and a container runtime (cri-o). The kubelet is responsible for
talking to the container runtime, managing the Pod lifecycle that were
assigned to this machine. The container runtime, is the component that
will create the containers themselves.

=== High Availability Considerations

The default deployment aims for a "High Availability" (HA) Kubernetes service.
In order to achieve HA it's required to run several control planes.

Not any number greater than 1 is optimal for HA, and this can impact
the fault tolerance. The reason for this is the etcd distributed key
value store:

[options="header",width=40%,cols="^,^"]
|===
|Cluster size |Failure Tolerance
|1 | 0
|2 | 0
|3 | 1
|4 | 1
|5 | 2
|6 | 2
|7 | 3
|8 | 3
|9 | 4
|===

Given that etcd runs only on the control plane nodes, despite having 2
control plane nodes is HA is not fault tolerant. If one of those two
control planes goes down, the cluster storage will be unavailable, and
the cluster won't be able to accept new changes (already running
workloads won't suffer any changes, but the cluster won't be able to
react to new changes from that point on).

In order to provide a fault tolerant HA environment you must
have an odd number of control planes.

A minimum of `3` master nodes is required in order to tolerate a complete loss
of one control plane node.

Control planes are only part of the whole picture. Most components will
talk to the API Server, and the API Server must be exposed on all master nodes
to communicate to the clients and the rest of the cluster components.

==== Load Balancer

The most reasonable way to achieve fault tolerant exposure of the API servers is
a load balancer. The load balancer will point to all the control plane nodes.
The clients and {kube} components will talk to the load balancer: which will
perform health checks against all the control planes and maintain an active pool
of backends.

If only one load balancer is deployed this creates a single point of failure.
For a complete HA solution, more than one load balancer is required.

[IMPORTANT]
If your environment only contains one load balancer it can not be considered
highly available or fault tolerant.

=== Testing / POC

The smallest possible deployment comes without a load balancer and the minimum
amount of nodes to be considered a cluster. This deployment type is in no way
suitable for production use and has no fault tolerance whatsoever.

* Single master
* 2 workers

=== Default Deployment

The default scenario requires 8 nodes:

* 2 Load Balancers
* 3 Masters
* 3 Workers

* Requires:
** Persistent IP addresses on all nodes.
** NTP server provided on the host network.
** DNS entry that resolves to the load balancer VIP.
** LDAP server or OIDC provider (Active Directory, GitLab, GitHub etc.)

* (Optional) "Infrastructure node"
** LDAP server if LDAP integration is desired and your organization
   does not have an LDAP server.
** Local RMT server to synchronize RPMs.
** Local mirror of the SUSE container registry (registry.suse.com)
** Local mirror of the SUSE helm chart repository.

=== Air Gapped Deployment

In air gapped environments, the "Infrastructure node" is mandatory, as
it's needed to have a local RMT server mirroring the CaaSP
repositories, a mirror of the SUSE container registry and a mirror of
the SUSE helm chart repository.

== Security

* security concepts

=== RBAC / PodSecurityPolicies

* user/role/group concepts

=== Network Policies



=== Control plane nodes certificates

Certificates are stored under /etc/kubernetes/pki on the control plane nodes.

==== CA certificates

[options="header"]
|===
|Path        |Valid for |Common Name |Description
|ca.crt      |1 year    |kubernetes  |Kubernetes global CA
|etcd/ca.crt |1 year    |etcd-ca     |etcd global CA
|===

==== CA certificate keys

[options="header"]
|===
|Path        |Key type |Key length (bits)
|ca.key      |RSA      |2048
|etcd/ca.key |RSA      |2048
|===

==== Certificates

[options="header"]
|===
|Path                         |Valid for |CN                            |Parent CA  |O (Subject)    |Kind           |Extra SANs
|apiserver-kubelet-client.crt |1 year    |kube-apiserver-kubelet-client |kubernetes |system:masters |client         |-
|etcd/healthcheck-client.crt  |1 year    |kube-etcd-healthcheck-client  |etcd-ca    |system:masters |client         |-
|etcd/server.crt              |1 year    |master                        |etcd-ca    |-              |server         |Hostname, IP address, localhost, 127.0.0.1, 0:0:0:0:0:0:0:1
|etcd/peer.crt                |1 year    |master                        |etcd-ca    |-              |server, client |Hostname, IP address, localhost, 127.0.0.1, 0:0:0:0:0:0:0:1
|apiserver.crt                |1 year    |kube-apiserver                |kubernetes |-              |server         |Hostname, IP address, Control Plane Address, kubernetes, kubernetes.default, kubernetes.default.svc, kubernetes.default.svc.cluster.local
|apiserver-etcd-client.crt    |1 year    |kube-apiserver-etcd-client    |etcd-ca    |system:masters |client         |-
|===

==== Certificate keys

[options="header",cols="<,^,^"]
|===
|Path                         |Key type |Key length (bits)
|apiserver.key                |RSA      |2048
|apiserver-kubelet-client.key |RSA      |2048
|apiserver-etcd-client.key    |RSA      |2048
|etcd/server.key              |RSA      |2048
|etcd/healthcheck-client.key  |RSA      |2048
|etcd/peer.key                |RSA      |2048
|===

=== Worker nodes certificates

The CA certificate for the cluster is stored under /etc/kubernetes/pki
on the worker nodes.

When a new worker machine joins the cluster, the kubelet performs a
TLS bootstrap. It requests a certificate to the cluster using a
bootstrap token, this request is automatically approved by the
cluster, and a certificate is created. The new worker downloads this
certificate and writes it to disk, so the kubelet uses this
client certificate from now on to contact the apiserver.

==== CA certificates

The CA certificate is downloaded from the cluster (present in the
cluster-info secret inside the kube-public namespace). Since this
information is public, there's no restriction to download the CA
certificate.

This certificate is saved under /etc/kubernetes/pki/ca.crt.

[options="header"]
|===
|Path                             |Valid for |Common Name         |Description
|/var/lib/kubelet/pki/kubelet.crt |1 year    |worker-ca@random-id |Kubelet CA
|===

==== CA certificate keys

[options="header",cols="<,^,^"]
|===
|Path        |Key type |Key length (bits)
|kubelet.key |RSA      |2048
|===

==== Certificates

Certificates are stored under `/var/lib/kubelet/pki` on the worker nodes.

[options="header"]
|===
|Path                       |Valid for |CN                 |Parent CA           |O (Subject)  |Kind   |Extra SANs |Notes
|kubelet-client-current.pem |1 year    |system:node:worker |kubernetes          |system:nodes |client |-          |Symlink to kubelet-client-timestamp.pem
|kubelet.crt                |1 year    |worker@random-id   |worker-ca@random-id |-            |server |Hostname   |-
|===

==== Certificate keys

[options="header",cols="<,^,^"]
|===
|Path        |Key type |Key length (bits)
|kubelet.key |RSA      |2048
|===

== Maintenance, Logging and Monitoring

* TBD

=== Cluster Management

Cluster lifecycle is managed using `skuba`. It enables you to
manage nodes in your cluster:

* Bootstrap a new cluster
* Join new machines to the cluster
** Master nodes
** Worker nodes
* Remove nodes from the cluster
** Master nodes
** Worker nodes

==== Creating a cluster definition

Creating a cluster definition is the first step to initialize your
cluster. You can execute this task as follows:

----
~/clusters$ skuba cluster init --control-plane 10.86.3.149 my-cluster
[init] configuration files written to /home/my-user/clusters/my-cluster
----

This operation happens strictly offline and will generate a folder
structure like the following:

----
~/clusters$ tree my-cluster
my-cluster/
├── addons
│   ├── cni
│   │   └── cilium.yaml
│   └── psp
│       ├── podsecuritypolicy-privileged.yaml
│       └── podsecuritypolicy-unprivileged.yaml
├── kubeadm-init.conf
└── kubeadm-join.conf.d
    ├── master.conf.template
    └── worker.conf.template

4 directories, 6 files
----

At this point, you can inspect all generated files, and if desired you
can experiment by changing the default settings.

==== Bootstrapping the first node of the cluster

From within your cluster definition folder, you can bootstrap your
first master machine:

----
~/clusters/my-cluster$ skuba node bootstrap --user sles --sudo --target <IP address/FQDN> <nodename>
----

This operation will read the `kubeadm-init.conf` file inside your
cluster definition, will forcefully set certain settings to the ones
required by CaaSP and will bootstrap the node remotely.

Prior to bootstrap it's possible for you to tweak the configuration
that will be used to create the cluster. You can:

* Replace the CNI by a different one, or completely skip the CNI
  creation. The CNI that will be deployed by default is Cilium. For
  this, you only need to remove the `addons/cni/cilium.yaml` and place
  another file instead. You can also leave the directory empty if you
  prefer to skip the CNI creation.

* Tweak the default Pod Security Policies or create extra ones. If you
  place extra Pod Security Policies in the `addons/psp` folder, those
  will be created as well when the bootstrap is completed. You can
  also modify the default ones and/or remove them.

* Inspect the `kubeadm-init.conf` and set extra configuration settings
  supported by `kubeadm`. The latest supported version is {kubeadm_api_version}.

After this operation has completed several modifications will have
happened on your cluster definition folder:

* The `kubeadm-init.conf` file will contain the final complete
  contents used to bootstrap the node, so you can inspect what
  exact configuration was used to bootstrap the node.

* A `admin.conf` file will be created on your cluster definition file,
  this is a `kubeconfig` file that has complete access to the cluster
  and uses client certificates for authenticating against the cluster.

==== Adding master nodes to the cluster

Adding new master nodes to the cluster can be achieved by executing
the following `skuba` command:

----
~/clusters/my-cluster$ skuba node join --role master --user sles --sudo --target <IP address/FQDN> <nodename>
----

This operation will try to read the `kubeadm-join.conf.d/<IP address/FQDN>.conf`
file if it exists. This allows you to set specific settings for this
new node prior to joining it (a similar procedure to how
`kubeadm-init.conf` file behaves when bootstrapping). If this file
does not exist, `skuba` will read the `kubeadm-join.conf.d/master.conf.template`
instead and will create this file automatically when `skuba node
join` is called.

This operation will increase the `etcd` member count by one, so it's
recommended to always keep an odd number of master nodes because as
described in previous sections an even number of nodes does not
improve fault tolerance.

==== Adding worker nodes to the cluster

Adding new worker nodes to the cluster can be achieved by executing
the following `skuba` command:

----
~/clusters/my-cluster$ skuba node join --role worker --user sles --sudo --target <IP address/FQDN> <nodename>
----

This operation will try to read the `kubeadm-join.conf.d/<IP address/FQDN>.conf`
file if it exists. This allows you to set specific settings for this
new node prior to joining it (a similar procedure to how
`kubeadm-init.conf` file behaves when bootstrapping). If this file
does not exist, `skuba` will read the `kubeadm-join.conf.d/worker.conf.template`
instead and will create this file automatically when `skuba node
join` is called.

==== Removing nodes from the cluster

Removing nodes from the cluster requires you to execute `skuba`
from a folder containing an `admin.conf` file, because this operation
is performed exclusively using Kubernetes, and no access to the node
being removed or other nodes for that matter is required.

For removing a node, the following command has to be executed:

----
~/clusters/my-cluster$ skuba node remove <nodename>
----

If the node to be removed is a master, specific actions will be
automatically executed, like removing the etcd member from the cluster. 
Note that this node cannot be added back to the cluster or any other 
skuba-initiated kubernetes cluster without reinstalling first.

=== Monitoring

* monitoring approach and considerations taken
** prometheus
** grafana
** containerized
** user defined dashboards

=== Logging

* logging approach and considerations taken
* logging formats and access points (API?)

=== Disaster Recovery

* disaster recovery concepts
* backup and restore concepts
* data retention and portability
