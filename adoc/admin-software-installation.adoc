[#software-installation]
= Software Installation

Software can be installed in three basic layers

Base OS layer::
Linux RPM packages, Kernel etc.. Installation via {ay},{tf} or {zypper}

{kube} Stack::
Software that helps/controls execution of workloads in {kube}

Container image::
Here it entirely depends on the actual makeup of the container what can be installed and how.
Please refer to your respecitve container image documentation for further details.
[NOTE]
Installation of software in container images is beyond the scope of this document.

== Base OS

Applications that will be deployed to {kube} will typically contain all the required software to be executed.
In some cases, especially when it comes to the hardware layer abstraction (storage backends, GPU), additional packages
must be installed on the underlying operating system outside of {kube}.

[NOTE]
====
The following examples show installation of required packages for `Ceph`, please adjust the list of
packages and repositories to whichever software you need to install.

While you can install any software package from the {slsa} ecosystem this falls outside of the support scope for {productname}.
====

=== Initial Rollout

During the rollout of nodes you can use either {ay} or {tf} (depending on your chosen deployment type)
to automatically install packages to all nodes.

For example, to install additional packages required by the `Ceph` storage backend you can modify
your `autoyast.xml` or `tfvars.yml` files to include the additional repositories and instructions to
install `xfsprogs` and `ceph-common`.

. `tfvars.yml`
+
[source,yaml]
----
# EXAMPLE:
# repositories = {
#   repository1 = "http://example.my.repo.com/repository1/"
#   repository2 = "http://example.my.repo.com/repository2/"
# }
repositories = {
        ....
}

# Minimum required packages. Do not remove them.
# Feel free to add more packages
packages = [
  "kernel-default",
  "-kernel-default-base",
  "xfsprogs",
  "ceph-common"
]
----
. `autoyast.xml`
+
[source,xml]
----
<!-- install required packages -->
<software>
  <image/>
  <products config:type="list">
    <product>SLES</product>
  </products>
  <instsource/>
  <patterns config:type="list">
    <pattern>base</pattern>
    <pattern>enhanced_base</pattern>
    <pattern>minimal_base</pattern>
    <pattern>basesystem</pattern>
  </patterns>
  <packages config:type="list">
    <package>ceph-common</package>
    <package>xfsprogs</package>
  </packages>
</software>
----

=== Existing Cluster

To install software on existing cluster nodes, you must use `zypper` on each node individually.
Simply log in to a node via SSH and run:

----
sudo zypper in ceph-common xfsprogs
----

== {kube} stack

[#helm-tiller-install]
=== Installing Helm

As of {productname} {productversion},, Helm is part of the {productname} package repository, so to use this,
you only need to run the following command from the location where you normally run `skuba` commands:

[source,bash]
----
sudo zypper install helm
----

=== Installing Tiller

As of {productname} {productversion},, Tiller is not part of the {productname} package repository but it is available as a
helm chart from the chart. To install the Tiller server, choose either way to deploy the Tiller server:

==== Unsecured Tiller Deployment

This will install Tiller without additional certificate security.

[source,bash,subs='attributes']
----
kubectl create serviceaccount --namespace kube-system tiller

kubectl create clusterrolebinding tiller \
    --clusterrole=cluster-admin \
    --serviceaccount=kube-system:tiller

helm init \
    --tiller-image registry.suse.com/caasp/v5/helm-tiller:{helm_tiller_version} \
    --service-account tiller
----

==== Secured Tiller Deployment with TLS certificate

This installs tiller with TLS certificate security.

===== Trusted Certificates

Please reference to <<trusted-server-certificate>> and <<trusted-client-certificate>> on how to sign the trusted tiller and helm certificate.
The server.conf for IP.1 is `127.0.0.1`.

Then, import trusted certificate to {kube} cluster. In this example, trusted certificate are `ca.crt`, `tiller.crt`, `tiller.key`, `helm.crt` and `helm.key`.

===== Self-signed Certificates (optional)

Please reference to <<self-signed-server-certificate>> and <<self-signed-client-certificate>> on how to sign the self-signed tiller and helm certificate.
The server.conf for IP.1 is `127.0.0.1`.

Then, import trusted certificate to {kube} cluster. In this example, trusted certificate are `ca.crt`, `tiller.crt`, `tiller.key`, `helm.crt` and `helm.key`.

. Deploy Tiller server with TLS certificate
+
[source,bash,subs="attributes"]
----
kubectl create serviceaccount --namespace kube-system tiller
kubectl create clusterrolebinding tiller \
    --clusterrole=cluster-admin \
    --serviceaccount=kube-system:tiller

helm init \
    --tiller-tls \
    --tiller-tls-verify \
    --tiller-tls-cert tiller.crt \
    --tiller-tls-key tiller.key \
    --tls-ca-cert ca.crt \
    --tiller-image registry.suse.com/caasp/v5/helm-tiller:{helm_tiller_version} \
    --service-account tiller
----

. Configure Helm client with TLS certificate
+
Setup $HELM_HOME environment and copy the CA certificate, helm client certificate and key to the $HELM_HOME path.
+
[source,bash]
----
export HELM_HOME=<path/to/helm/home>

cp ca.crt $HELM_HOME/ca.pem
cp helm.crt $HELM_HOME/cert.pem
cp helm.key $HELM_HOME/key.pem
----
+
Then, for helm commands, pass flag `--tls`. For example:
[source,bash]
+
----
helm ls --tls [flags]
helm install --tls <CHART> [flags]
helm upgrade --tls <RELEASE_NAME> <CHART> [flags]
helm del --tls <RELEASE_NAME> [flags]
----

////
Note: When Helm is included in v4, Tiller server will be automatically installed after CaaS Platform setup.
So we probably just need to mention that we use it and that it's installed automatically.
////
