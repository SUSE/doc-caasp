[#software-installation]
= Software Installation

Software can be installed in three basic layers

Base OS layer::
Linux RPM packages, Kernel etc.. Installation via {ay},{tf} or {zypper}

{kube} Stack::
Software that helps/controls execution of workloads in {kube}

Container image::
Here it entirely depends on the actual makeup of the container what can be installed and how.
Please refer to your respecitve container image documentation for further details.
[NOTE]
Installation of software in container images is beyond the scope of this document.

== Base OS

Applications that will be deployed to {kube} will typically contain all the required software to be executed.
In some cases, especially when it comes to the hardware layer abstraction (storage backends, GPU), additional packages
must be installed on the underlying operating system outside of {kube}.

[NOTE]
====
The following examples show installation of required packages for `Ceph`, please adjust the list of
packages and repositories to whichever software you need to install.

While you can install any software package from the {slsa} ecosystem this falls outside of the support scope for {productname}.
====

=== Initial Rollout

During the rollout of nodes you can use either {ay} or {tf} (depending on your chosen deployment type)
to automatically install packages to all nodes.

For example, to install additional packages required by the `Ceph` storage backend you can modify
your `autoyast.xml` or `tfvars.yml` files to include the additional repositories and instructions to
install `xfsprogs` and `ceph-common`.

. `tfvars.yml`
+
[source,yaml]
----
# EXAMPLE:
# repositories = {
#   repository1 = "http://example.my.repo.com/repository1/"
#   repository2 = "http://example.my.repo.com/repository2/"
# }
repositories = {
        ....
}

# Minimum required packages. Do not remove them.
# Feel free to add more packages
packages = [
  "kernel-default",
  "-kernel-default-base",
  "xfsprogs",
  "ceph-common"
]
----
. `autoyast.xml`
+
[source,xml]
----
<!-- install required packages -->
<software>
  <image/>
  <products config:type="list">
    <product>SLES</product>
  </products>
  <instsource/>
  <patterns config:type="list">
    <pattern>base</pattern>
    <pattern>enhanced_base</pattern>
    <pattern>minimal_base</pattern>
    <pattern>basesystem</pattern>
  </patterns>
  <packages config:type="list">
    <package>ceph-common</package>
    <package>xfsprogs</package>
  </packages>
</software>
----

=== Existing Cluster

To install software on existing cluster nodes, you must use `zypper` on each node individually.
Simply log in to a node via SSH and run:

----
sudo zypper in ceph-common xfsprogs
----

== {kube} stack

[[helm-tiller-install]]
=== Installing Helm

As of {productname} {productversion}, Helm is part of the {productname} package repository, so to use this,
you only need to run the following command from the location where you normally run `skuba` commands:

[source,bash]
----
sudo zypper install helm
----

Helm 2 is the default for {productname} 5. Helm 3 is offered as an alternate tool and may be installed in parallel to aid migration. Helm 3 may be selected by installing from zypper then selecting the alternative.

[source,bash]
----
sudo zypper install helm3
sudo update-alternatives --set helm /usr/bin/helm3
----

[WARNING]
Helm 2 is planned to end support in November 2020. Helm 3 is offered as an alternative in {productname} 5.0 and will become the default tool in the following release. Please see <<helm-2to3-migration>> for upgrade instructions and upgrade as soon as feasible.

=== Installing Tiller

[NOTE]
Tiller is only a requirement for Helm 2 and has been removed from Helm 3.  If using Helm 3, please skip this section.

As of {productname} {productversion}, Tiller is not part of the {productname} package repository but it is available as a
helm chart from the chart repository. To install the Tiller server, choose either way to deploy the Tiller server:

==== Unsecured Tiller Deployment

This will install Tiller without additional certificate security.

[source,bash,subs='attributes']
----
kubectl create serviceaccount --namespace kube-system tiller

kubectl create clusterrolebinding tiller \
    --clusterrole=cluster-admin \
    --serviceaccount=kube-system:tiller

helm init \
    --tiller-image registry.suse.com/caasp/v5/helm-tiller:{helm_tiller_version} \
    --service-account tiller
----

==== Secured Tiller Deployment with TLS certificate

This installs tiller with TLS certificate security.

===== Trusted Certificates

Please reference to <<trusted-server-certificate>> and <<trusted-client-certificate>> on how to sign the trusted tiller and helm certificate.
The server.conf for IP.1 is `127.0.0.1`.

Then, import trusted certificate to {kube} cluster. In this example, trusted certificate are `ca.crt`, `tiller.crt`, `tiller.key`, `helm.crt` and `helm.key`.

===== Self-signed Certificates (optional)

Please reference to <<self-signed-server-certificate>> and <<self-signed-client-certificate>> on how to sign the self-signed tiller and helm certificate.
The server.conf for IP.1 is `127.0.0.1`.

Then, import trusted certificate to {kube} cluster. In this example, trusted certificate are `ca.crt`, `tiller.crt`, `tiller.key`, `helm.crt` and `helm.key`.

. Deploy Tiller server with TLS certificate
+
[source,bash,subs="attributes"]
----
kubectl create serviceaccount --namespace kube-system tiller
kubectl create clusterrolebinding tiller \
    --clusterrole=cluster-admin \
    --serviceaccount=kube-system:tiller

helm init \
    --tiller-tls \
    --tiller-tls-verify \
    --tiller-tls-cert tiller.crt \
    --tiller-tls-key tiller.key \
    --tls-ca-cert ca.crt \
    --tiller-image registry.suse.com/caasp/v5/helm-tiller:{helm_tiller_version} \
    --service-account tiller
----

. Configure Helm client with TLS certificate
+
Setup $HELM_HOME environment and copy the CA certificate, helm client certificate and key to the $HELM_HOME path.
+
[source,bash]
----
export HELM_HOME=<path/to/helm/home>

cp ca.crt $HELM_HOME/ca.pem
cp helm.crt $HELM_HOME/cert.pem
cp helm.key $HELM_HOME/key.pem
----
+
Then, for helm commands, pass flag `--tls`. For example:
[source,bash]
+
----
helm ls --tls [flags]
helm install --tls <CHART> [flags]
helm upgrade --tls <RELEASE_NAME> <CHART> [flags]
helm del --tls <RELEASE_NAME> [flags]
----

[[helm-2to3-migration]]
=== Helm 2 to 3 Migration
[NOTE]S
====
The process for migrating an installation from Helm 2 to Helm 3 has been documented and tested by the Helm community. Please reference the following links before proceeding.

- https://v3.helm.sh/docs/topics/v2_v3_migration/
- https://helm.sh/blog/migrate-from-helm-v2-to-helm-v3/
- https://github.com/helm/helm-2to3

====

==== Preconditions

* A healthy CaaSP 5.0 installation with applications deployed using Helm 2 and Tiller.
* A system, which `skuba` and `helm` version 2 have run on previously.
** The procedure below requires an available internet connection to install the `2to3` plugin.  If the installation is in an air gapped environment, the system may need to be moved back out of the air gapped environment.
* These instructions are written for a single cluster managed from a single Helm 2 installation. If more than one cluster is being managed by this installation of Helm 2, please reference https://github.com/helm/helm-2to3 for further details and do not do the clean-up step until all clusters are migrated.


==== Migration Procedure

This is a procedure for migrating a CaaSP 5.0 deployment that has used Helm 2 to deploy applications.

. Install `helm3` package in the same location you normally run `skuba` commands (alongside the helm2 package):
+
----
sudo zypper in helm3
----
. Install the `2to3` plugin:
+
----
helm3 plugin install https://github.com/helm/helm-2to3.git
----
. Backup Helm 2 data found in the following:
.. Helm 2 home folder.
.. Release data from the cluster. Refer to link:http://technosophos.com/2017/03/23/how-helm-uses-configmaps-to-store-data.html[How Helm Uses ConfigMaps to Store Data] for details on how Helm 2 stores release data in the cluster. This should apply similarly if Helm 2 is configured for secrets.
. Move configuration from 2 to 3:
+
----
helm3 2to3 move config
----
.. After the move, if you have installed any custom plugins, then check that they work fine with Helm 3. If needed, remove and re-add them as described in https://github.com/helm/helm-2to3s.
.. If you have configured any local helm chart repositories, you will need to remove and re-add them.  For example:
+
----
helm3 repo remove <my-custom-repo>
helm3 repo add <my-custom-repo> <url-to-custom-repo>
helm3 repo update
----
. Migrate Helm releases (deployed charts) in place:
+
----
helm3 2to3 convert RELEASE
----
. Clean up Helm 2 data:
+
WARNING: Tiller will be cleaned up, and Helm 2 will not be usable on this cluster after cleanup.
+
----
helm3 2to3 cleanup
----
. You may now uninstall the `helm2` package and use the `helm` command line from the `helm3` package from now on.
+
----
sudo zypper remove helm2
----

////
Note: When Helm is included in v4, Tiller server will be automatically installed after CaaS Platform setup.
So we probably just need to mention that we use it and that it's installed automatically.
Note: 5.0 will still use Helm 2, but Helm 3 will become the default in 5.1 and Helm 2 and Tiller will be dropped in 5.2.
////
