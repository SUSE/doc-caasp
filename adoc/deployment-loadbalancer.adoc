[[loadbalancer.nginx]]
== Nginx TCP Load Balancer with Passive Checks

For TCP load balancing, we can use the `ngx_stream_module` module (available since version 1.9.0). In this mode, `nginx` will just forward the TCP packets to the master nodes.

The default mechanism is *round-robin* so each request will be distributed to a different server.

[WARNING]
====
The open source version of Nginx referred to in this guide only allows the use of
use passive health checks. `nginx` will mark a node as unresponsive only after
a failed request. The original request is lost and not forwarded to an available
alternative server.

This load balancer configuration is therefore only suitable for testing and proof-of-concept (POC) environments.

For production environments, we recommend the use of link:https://documentation.suse.com/sle-ha/15-SP1/[{sle} {hasi} 15]
====

=== Configuring the Load Balancer

. Register SLES and enable the "Server Applications" module:
+
[source,bash]
----
SUSEConnect -r CAASP_REGISTRATION_CODE
SUSEConnect --product sle-module-server-applications/15.1/x86_64
----
. Install Nginx:
+
[source,bash]
----
zypper in nginx
----
. Write the configuration in `/etc/nginx/nginx.conf`:
+
----
user  nginx;
worker_processes  auto;

load_module /usr/lib64/nginx/modules/ngx_stream_module.so;

error_log  /var/log/nginx/error.log;
error_log  /var/log/nginx/error.log  notice;
error_log  /var/log/nginx/error.log  info;

events {
    worker_connections  1024;
    use epoll;
}

stream {
    log_format proxy '$remote_addr [$time_local] '
                     '$protocol $status $bytes_sent $bytes_received '
                     '$session_time "$upstream_addr"';

    error_log  /var/log/nginx/k8s-masters-lb-error.log;
    access_log /var/log/nginx/k8s-masters-lb-access.log proxy;

    upstream k8s-masters {
        #hash $remote_addr consistent; // <1>
        server master00:6443 weight=1 max_fails=2 fail_timeout=5s; // <2>
        server master01:6443 weight=1 max_fails=2 fail_timeout=5s;
        server master02:6443 weight=1 max_fails=2 fail_timeout=5s;
    }
    server {
        listen 6443;
        proxy_connect_timeout 5s;
        proxy_timeout 5s;
        proxy_pass k8s-masters;
    }

    upstream dex-backends {
        #hash $remote_addr consistent; // <1>
        server master00:32000 weight=1 max_fails=2 fail_timeout=5s; // <2>
        server master01:32000 weight=1 max_fails=2 fail_timeout=5s;
        server master02:32000 weight=1 max_fails=2 fail_timeout=5s;
    }
    server {
        listen 32000;
        proxy_connect_timeout 5s;
        proxy_timeout 5s;
        proxy_pass dex-backends; // <3>
    }

    upstream gangway-backends {
        #hash $remote_addr consistent; // <1>
        server master00:32001 weight=1 max_fails=2 fail_timeout=5s; // <2>
        server master01:32001 weight=1 max_fails=2 fail_timeout=5s;
        server master02:32001 weight=1 max_fails=2 fail_timeout=5s;
    }
    server {
        listen 32001;
        proxy_connect_timeout 5s;
        proxy_timeout 5s;
        proxy_pass gangway-backends; // <3>
    }
}
----
<1> **Note:** To enable session persistence, uncomment the `hash` option
so the same client will always be redirected to the same server except if this
server is unavailable.
<2> Replace the individual `masterXX` with the IP/FQDN of your actual master nodes (one entry each) in the `upstream k8s-masters` section.
<3> Dex port `32000` and Gangway port `32001` must be accessible through the load balancer for RBAC authentication.
. Configure `firewalld` to open up port `6443`. As root, run:
+
[source,bash]
----
firewall-cmd --zone=public --permanent --add-port=6443/tcp
firewall-cmd --zone=public --permanent --add-port=32000/tcp
firewall-cmd --zone=public --permanent --add-port=32001/tcp
firewall-cmd --reload
----
. Start and enable Nginx. As root, run:
+
[source,bash]
----
systemctl enable --now nginx
----

=== Verifying the Load Balancer

[IMPORTANT]
The {productname} cluster must be up and running for this to produce any useful
results. This step can only be performed after <<bootstrap>> is completed
successfully.

To verify that the load balancer works, you can run a simple command to repeatedly
retrieve cluster information from the master nodes. Each request should be forwarded
to a different master node.

From your workstation, run:

[source,bash]
----
while true; do skuba cluster status; sleep 1; done;
----

There should be no interruption in the  *skuba cluster status* running command.

On the load balancer virtual machine, check the logs to validate
that each request is correctly distributed in a round robin way.

[source,bash]
----
# tail -f /var/log/nginx/k8s-masters-lb-access.log
10.0.0.47 [17/May/2019:13:49:06 +0000] TCP 200 2553 1613 1.136 "10.0.0.145:6443"
10.0.0.47 [17/May/2019:13:49:08 +0000] TCP 200 2553 1613 0.981 "10.0.0.148:6443"
10.0.0.47 [17/May/2019:13:49:10 +0000] TCP 200 2553 1613 0.891 "10.0.0.7:6443"
10.0.0.47 [17/May/2019:13:49:12 +0000] TCP 200 2553 1613 0.895 "10.0.0.145:6443"
10.0.0.47 [17/May/2019:13:49:15 +0000] TCP 200 2553 1613 1.157 "10.0.0.148:6443"
10.0.0.47 [17/May/2019:13:49:17 +0000] TCP 200 2553 1613 0.897 "10.0.0.7:6443"
----

[[loadbalancer.haproxy]]
== HAProxy TCP Load Balancer with Active Checks

`HAProxy` is a very powerful load balancer which, unlike `nginx`, can be used in production.
The version used at this date is the `1.8.7`.

[IMPORTANT]
====
The configuration of an HA cluster is out of the scope of this document.
====

.Package Support
[WARNING]
====
`HAProxy` is available as a supported package with a link:https://documentation.suse.com/sle-ha/15-SP1/[{sle} {hasi} 15] subscription.

Alternatively, you can install `HAProxy` from link:https://packagehub.suse.com/packages/haproxy/[{pkghub}] but you will not receive product support for this component.
====

The default mechanism is *round-robin* so each request will be distributed to a different server.

The health-checks are executed every two seconds. If a connection fails, the check will be retried two times with a timeout of five seconds for each request.
If no connection succeeds within this interval (2x5s), the node will be marked as `DOWN` and no traffic will be sent until the checks succeed again.

=== Configuring the Load Balancer

. Register SLES and enable the "Server Applications" module:
+
[source,bash]
----
SUSEConnect -r CAASP_REGISTRATION_CODE
SUSEConnect --product sle-module-server-applications/15.1/x86_64
----
. Enable the source for the `haproxy` package:
* If you are using the {sle} {hasi}
+
----
SUSEConnect --product sle-ha/15.1/x86_64 -r ADDITIONAL_REGCODE
----
* If you want the free (unsupported) package:
+
----
SUSEConnect --product PackageHub/15.1/x86_64
----
. Configure `/dev/log` for HAProxy chroot (optional)
+
This step is only required when `HAProxy` is configured to run in a jail directory (chroot). This is highly recommended since it increases the security of `HAProxy`.
+
Since `HAProxy` is chrooted, it's necessary to make the log socket available inside the jail directory so `HAProxy` can send logs to the socket.
+
[source,bash]
----
mkdir -p /var/lib/haproxy/dev/ && touch /var/lib/haproxy/dev/log
----
+
This systemd service will take care of mounting the socket in the jail directory.
+
[source,bash]
----
cat > /etc/systemd/system/bindmount-dev-log-haproxy-chroot.service <<EOF
[Unit]
Description=Mount /dev/log in HAProxy chroot
After=systemd-journald-dev-log.socket
Before=haproxy.service

[Service]
Type=oneshot
ExecStart=/bin/mount --bind /dev/log /var/lib/haproxy/dev/log

[Install]
WantedBy=multi-user.target
EOF
----
+
Enabling the service will make the changes persistent after a reboot.
[source,bash]
+
----
systemctl enable --now bindmount-dev-log-haproxy-chroot.service
----
. Install HAProxy:
+
[source,bash]
----
zypper in haproxy
----
. Write the configuration in `/etc/haproxy/haproxy.cfg`:
+
[NOTE]
====
Replace the individual `<MASTER_XX_IP_ADDRESS>` with the IP of your actual master nodes (one entry each) in the `server` lines.
Feel free to leave the name argument in the `server` lines (`master00` and etc.) as is - it only serves as a label that will show up in the haproxy logs.
====
+
----
global
  log /dev/log local0 info // <1>
  chroot /var/lib/haproxy // <2>
  user haproxy
  group haproxy
  daemon

defaults
  mode       tcp
  log        global
  option     tcplog
  option     redispatch
  option     tcpka
  retries    2
  http-check     expect status 200 // <5>
  default-server check check-ssl verify none
  timeout connect 5s
  timeout client 5s
  timeout server 5s
  timeout tunnel 86400s // <3>

listen stats // <4>
  bind    *:9000
  mode    http
  stats   hide-version
  stats   uri       /stats

listen apiserver // <6>
  bind   *:6443
  option httpchk GET /healthz
  server master00 <MASTER_00_IP_ADDRESS>:6443
  server master01 <MASTER_01_IP_ADDRESS>:6443
  server master02 <MASTER_02_IP_ADDRESS>:6443

listen dex // <7>
  bind   *:32000
  option httpchk GET /healthz
  server master00 <MASTER_00_IP_ADDRESS>:32000
  server master01 <MASTER_01_IP_ADDRESS>:32000
  server masetr02 <MASTER_02_IP_ADDRESS>:32000

listen gangway // <8>
  bind   *:32001
  option httpchk GET /
  server master00 <MASTER_00_IP_ADDRESS>:32001
  server master01 <MASTER_01_IP_ADDRESS>:32001
  server master02 <MASTER_02_IP_ADDRESS>:32001
----
<1> Forward the logs to systemd journald, the log level can be set to `debug` to increase verbosity.
<2> Define if it will run in a `chroot`.
<3> This timeout is set to `24h` in order to allow long connections when accessing pod logs or port forwarding.
<4> URL to expose `HAProxy` stats on port `9000`, it is accessible at `+http://loadbalancer:9000/stats+`
<5> The performed health checks will expect a `200` return code
<6> Kubernetes apiserver listening on port `6443`, the checks are performed against `+https://MASTER_XX_IP_ADDRESS:6443/healthz+`
<7> Dex listening on port `32000`, it must be accessible through the load balancer for RBAC authentication, the checks are performed against `+https://MASTER_XX_IP_ADDRESS:32000/healthz+`
<8> Gangway listening on port `32001`, it must be accessible through the load balancer for RBAC authentication, the checks are performed against `+https://MASTER_XX_IP_ADDRESS:32001/+`
. Configure `firewalld` to open up port `6443`. As root, run:
+
[source,bash]
----
firewall-cmd --zone=public --permanent --add-port=6443/tcp
firewall-cmd --zone=public --permanent --add-port=32000/tcp
firewall-cmd --zone=public --permanent --add-port=32001/tcp
firewall-cmd --reload
----
. Start and enable `HAProxy`. As root, run:
+
[source,bash]
----
systemctl enable --now haproxy
----

=== Verifying the Load Balancer

[IMPORTANT]
The {productname} cluster must be up and running for this to produce any useful
results. This step can only be performed after <<bootstrap>> is completed
successfully.

To verify that the load balancer works, you can run a simple command to repeatedly
retrieve cluster information from the master nodes. Each request should be forwarded
to a different master node.

From your workstation, run:

[source,bash]
----
while true; do skuba cluster status; sleep 1; done;
----

There should be no interruption in the  *skuba cluster status* running command.

On the load balancer virtual machine, check the logs to validate
that each request is correctly distributed in a round robin way.

[source,bash]
----
# journalctl -flu haproxy
haproxy[2525]: 10.0.0.47:59664 [30/Sep/2019:13:33:20.578] apiserver apiserver/master00 1/0/578 9727 -- 18/18/17/3/0 0/0
haproxy[2525]: 10.0.0.47:59666 [30/Sep/2019:13:33:22.476] apiserver apiserver/master01 1/0/747 9727 -- 18/18/17/7/0 0/0
haproxy[2525]: 10.0.0.47:59668 [30/Sep/2019:13:33:24.522] apiserver apiserver/master02 1/0/575 9727 -- 18/18/17/7/0 0/0
haproxy[2525]: 10.0.0.47:59670 [30/Sep/2019:13:33:26.386] apiserver apiserver/master00 1/0/567 9727 -- 18/18/17/3/0 0/0
haproxy[2525]: 10.0.0.47:59678 [30/Sep/2019:13:33:28.279] apiserver apiserver/master01 1/0/575 9727 -- 18/18/17/7/0 0/0
haproxy[2525]: 10.0.0.47:59682 [30/Sep/2019:13:33:30.174] apiserver apiserver/master02 1/0/571 9727 -- 18/18/17/7/0 0/0
----
