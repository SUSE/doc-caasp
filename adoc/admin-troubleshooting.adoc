//= Troubleshooting

This chapter summarizes frequent problems that can occur while using {productname}
and their solutions.

Additionally, {suse} support collects problems and their solutions online at link:https://www.suse.com/support/kb/?id=SUSE_CaaS_Platform[] .

== The `supportconfig` Tool

As a first step for any troubleshooting/debugging effort, you need to find out
the location of the cause of the problem. For this purpose we ship the `supportconfig` tool
and plugin with {productname}. With a simple command you can collect and compile
a variety of details about your cluster to enable {suse} support to pinpoint
the potential cause of an issue.

In case of problems, a detailed system report can be created with the
`supportconfig` command line tool. It will collect information about the system, such as:

* Current Kernel version
* Hardware information
* Installed packages
* Partition setup
* Cluster and node status

[TIP]
====
A full list of of the data collected by `supportconfig` can be found under
https://github.com/SUSE/supportutils-plugin-suse-caasp/blob/master/README.md.
====

[IMPORTANT]
====
To collect all the relevant logs, run the `supportconfig` command on all the master
and worker nodes individually.
====

[source,bash]
----
sudo supportconfig
sudo tar -xvJf /var/log/nts_*.txz
cd /var/log/nts*
sudo cat kubernetes.txt crio.txt
----

The result is a `TAR` archive of files. Each of the `*.txz` files should be given a name that can be used to identify which cluster node it was created on.

After opening a Service Request (SR), you can upload the `TAR` archives to {suse} Global Technical Support.

The data will help to debug the issue you reported and assist you in solving the problem. For details, see https://documentation.suse.com/sles/15-SP1/single-html/SLES-admin/#cha-adm-support.

== Cluster definition directory

Apart from the logs provided by running the `supportconfig` tool, an additional set of data might be required for debugging purposes. This information is located at the Management node, under your cluster definition directory. This folder contains important and sensitive information about your {productname} cluster and it's the one from where you issue `skuba` commands.

[WARNING]
====
If the problem you are facing is related to your production environment, do **not** upload the `admin.conf` as this would expose access to your cluster to anyone in possession of the collected information! The same precautions apply for the `pki` directory, since this also contains sensitive information (CA cert and key).

In this case add `--exclude='./my-cluster/admin.conf' --exclude='./my-cluster/pki/'` to the command in the following example. Make sure to replace `./my-cluster` with the actual path of your cluster definition folder.

If you need to debug issues with your private certificates, a separate call with {suse} support must be scheduled to help you.
====

Create a `TAR` archive by compressing the cluster definition directory.
[source,bash]
----
# Read the TIP above
# Move the admin.conf and pki directory to another safe location or exclude from packaging
tar -czvf cluster.tar.gz /home/user/my-cluster/
# If the error is related to Terraform, please copy the terraform configuration files as well
tar -czvf cluster.tar.gz /home/user/my-terraform-configuration/
----

After opening a Service Request (SR), you can upload the `TAR` archive to {suse} Global Technical Support.

[#troubleshooting-logs]
== Log collection

Some of these information are required for debugging certain cases. The data collected by
via `supportconfig` in such cases are following:

* etcd.txt (_master nodes_)
+
----
curl -Ls --cacert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/server.key --cert /etc/kubernetes/pki/etcd/server.crt https://localhost:2379/health
curl -Ls --cacert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/server.key --cert /etc/kubernetes/pki/etcd/server.crt https://localhost:2379/v2/members
curl -Ls --cacert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/server.key --cert /etc/kubernetes/pki/etcd/server.crt https://localhost:2379/v2/stats/leader
curl -Ls --cacert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/server.key --cert /etc/kubernetes/pki/etcd/server.crt https://localhost:2379/v2/stats/self
curl -Ls --cacert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/server.key --cert /etc/kubernetes/pki/etcd/server.crt https://localhost:2379/v2/stats/store
curl -Ls --cacert /etc/kubernetes/pki/etcd/ca.crt --key /etc/kubernetes/pki/etcd/server.key --cert /etc/kubernetes/pki/etcd/server.crt https://localhost:2379/metrics

etcdcontainer=$(crictl ps --label io.kubernetes.container.name=etcd --quiet)

crictl exec $etcdcontainer sh -c \"ETCDCTL_ENDPOINTS='https://127.0.0.1:2379' ETCDCTL_CACERT='/etc/kubernetes/pki/etcd/ca.crt' ETCDCTL_CERT='/etc/kubernetes/pki/etcd/server.crt' ETCDCTL_KEY='/etc/kubernetes/pki/etcd/server.key' ETCDCTL_API=3 etcdctl check perf\"

crictl logs -t $etcdcontainer

crictl stats --id $etcdcontainer

etcdpod=$(crictl ps | grep etcd | awk -F ' ' '{ print $9 }')

crictl inspectp $etcdpod
----

[NOTE]
====
For more information about `etcd`, refer to <<troubleshooting-etcd>>.
====

* kubernetes.txt (_all nodes_)
+
----
export KUBECONFIG=/etc/kubernetes/admin.conf

kubectl version

kubectl api-versions

kubectl config view

kubectl -n kube-system get pods

kubectl get events --sort-by=.metadata.creationTimestamp

kubectl get nodes

kubectl get all -A

kubectl get nodes -o yaml
----

* kubernetes-cluster-info.txt (_all nodes_)
+
----
export KUBECONFIG=/etc/kubernetes/admin.conf

# a copy of kubernetes logs /var/log/kubernetes
kubectl cluster-info dump --output-directory="/var/log/kubernetes"
----

* kubelet.txt (_all nodes_)
+
----
systemctl status --full kubelet

journalctl -u kubelet

# a copy of kubernetes manifests /etc/kubernetes/manifests"
cat /var/lib/kubelet/config.yaml
----

* oidc-gangway.txt (_all nodes_)
+
----
container=$(crictl ps --label io.kubernetes.container.name="oidc-gangway" --quiet)

crictl logs -t $container

crictl inspect $container

pod=$(crictl ps | grep "oidc-gangway" | awk -F ' ' '{ print $9 }')

crictl inspectp $pod
----

* oidc-dex.txt (_worker nodes_)
+
----
container=$(crictl ps --label io.kubernetes.container.name="oidc-dex" --quiet)

crictl logs -t $container

crictl inspect $container

pod=$(crictl ps | grep "oidc-dex" | awk -F ' ' '{ print $9 }')

crictl inspectp $pod
----

* cilium-agent.txt (_all nodes_)
+
----
container=$(crictl ps --label io.kubernetes.container.name="cilium-agent" --quiet)

crictl logs -t $container

crictl inspect $container

pod=$(crictl ps | grep "cilium-agent" | awk -F ' ' '{ print $9 }')

crictl inspectp $pod
----

* cilium-operator.txt (_only from the worker node is runs_)
+
----
container=$(crictl ps --label io.kubernetes.container.name="cilium-operator" --quiet)

crictl logs -t $container

crictl inspect $container

pod=$(crictl ps | grep "cilium-operator" | awk -F ' ' '{ print $9 }')

crictl inspectp $pod
----

* kured.txt (_all nodes_)
+
----
container=$(crictl ps --label io.kubernetes.container.name="kured" --quiet)

crictl logs -t $container

crictl inspect $container

pod=$(crictl ps | grep "kured" | awk -F ' ' '{ print $9 }')

crictl inspectp $pod
----

* coredns.txt (_worker nodes)
+
----
container=$(crictl ps --label io.kubernetes.container.name="coredns" --quiet)

crictl logs -t $container

crictl inspect $container

pod=$(crictl ps | grep "coredns" | awk -F ' ' '{ print $9 }')

crictl inspectp $pod
----

* kube-apiserver.txt (_master nodes_)
+
----
container=$(crictl ps --label io.kubernetes.container.name="kube-apiserver" --quiet)

crictl logs -t $container

crictl inspect $container

pod=$(crictl ps | grep "kube-apiserver" | awk -F ' ' '{ print $9 }')

crictl inspectp $pod
----

* kube-proxy.txt (_all nodes_)
+
----
container=$(crictl ps --label io.kubernetes.container.name="kube-proxy" --quiet)

crictl logs -t $container

crictl inspect $container

pod=$(crictl ps | grep "kube-proxy" | awk -F ' ' '{ print $9 }')

crictl inspectp $pod
----

* kube-scheduler.txt (_master nodes_)
+
----
container=$(crictl ps --label io.kubernetes.container.name="kube-scheduler" --quiet)

crictl logs -t $container

crictl inspect $container

pod=$(crictl ps | grep "kube-scheduler" | awk -F ' ' '{ print $9 }')

crictl inspectp $pod
----

* kube-controller-manager.txt (_master nodes_)
+
----
container=$(crictl ps --label io.kubernetes.container.name="kube-controller-manager" --quiet)

crictl logs -t $container

crictl inspect $container

pod=$(crictl ps | grep "kube-controller-manager" | awk -F ' ' '{ print $9 }')

crictl inspectp $pod
----

* kube-system.txt (_all nodes_)
+
----
export KUBECONFIG=/etc/kubernetes/admin.conf

kubectl get all -n kube-system -o yaml
----

* crio.txt (_all_nodes_)
+
----
crictl version

systemctl status --full crio.service

crictl info

crictl images

crictl ps --all

crictl stats --all

journalctl -u crio

# a copy of /etc/crictl.yaml

# a copy of /etc/sysconfig/crio

# a copy of /etc/crio/crio.conf

# a copy of every file under /etc/crio/

# Run the following three commands for every container using this loop:
for i in $(crictl  ps -a 2>/dev/null | grep -v "CONTAINER" | awk '{print $1}');
do
    crictl stats --id $i
    crictl logs $i
    crictl inspect $i
done
----

== Debugging SLES Nodes provision

If {tf} fails to setup the required {slsa} infrastructure for your cluster, please provide the configuration
you applied in a form of a TAR archive.

Create a `TAR` archive by compressing the {tf}.
[source,bash]
----
tar -czvf terraform.tar.gz /path/to/terraform/configuration
----

After opening a Service Request (SR), you can upload the TAR archive to Global Technical Support.

== Debugging Cluster Deployment

If the cluster deployment fails, please re-run the command again with setting verbosity level to 5 `-v=5`.

For example, if bootstraps the first master node of the cluster fails, re-run the command like
[source,bash]
----
skuba node bootstrap --user sles --sudo --target <IP/FQDN> <NODE_NAME> -v=5
----

However, if the `join` procedure fails at the last final steps, re-running it might _not_ help. To verify
this, please list the current member nodes of your cluster and look for the one who failed.

[source,bash]
----
kubectl get nodes
----

If the node that failed to `join` is nevertheless listed in the output as part of your cluster,
then this is a bad indicator. This node cannot be reset back to a clean state anymore and it's not safe to keep
it online in this _unknown_ state. As a result, instead of trying to fix its existing configuration either by hand or re-running
the join/bootstrap command, we would highly recommend you to remove this node completely from your cluster and
then replace it with a new one.

[source,bash]
----
skuba node remove <NODE_NAME> --drain-timeout 5s
----

== Error `x509: certificate signed by unknown authority`

When interacting with {kube}, you might run into the situation where your existing configuration for the authentication has changed (cluster has been rebuild, certificates have been switched.)
In such a case you might see an error message in the output of your CLI or Web browser.

----
x509: certificate signed by unknown authority
----

This message indicates that your current system does not know the Certificate Authority (CA) that signed the SSL certificates used for encrypting the communication to the cluster. You then need to add or update the Root CA certificate in your local trust store.

. Obtain the root CA certificate from on of the {kube} cluster node, at the location `/etc/kubernetes/pki/ca.crt`
. Copy the root CA certificate into your local machine directory `/etc/pki/trust/anchors/`
. Update the cache for know CA certificates
+
[source,bash]
----
sudo update-ca-certificates
----

== Error `Invalid client credentials`

When using Dex & Gangway for authentication, you might see the following error message in the Web browser output:

----
oauth2: cannot fetch token: 401 Unauthorized
Response: {"error":"invalid_client","error_description":"Invalid client credentials."}
----

This message indicates that your {kube} cluster Dex & Gangway client secret is out of sync. Please update the Dex & Gangway ConfigMap to use the same client secret.

[source,bash]
----
kubectl get configmap oidc-dex-config > oidc-dex-config.yaml
kubectl get configmap oidc-gangway-config > oidc-gangway-config.yaml
----

Make sure the oidc's `secret` in `oidc-dex-config.yaml` is the same as the `clientSecret` in `oidc-gangway-config.yaml`. Then, apply the updated ConfigMap.

[source,bash]
----
kubectl replace -f oidc-dex-config.yaml
kubectl replace -f oidc-gangway-config.yaml
----

== Replacing a Lost Node

If your cluster loses a node, for example due to failed hardware, remove the node as explained in <<removing_nodes>>.
Then add a new node as described in <<adding_nodes>>.

== Rebooting an Undrained Node with RBD Volumes Mapped

Rebooting a cluster node always requires a preceding `drain`.
In some cases, draining the nodes first might not be possible and some problem can occur during reboot if some RBD volumes are mapped to the nodes.

In this situation, apply the following steps.

. Make sure kubelet and {crio} are stopped:
+
[source,bash]
----
systemctl stop kubelet crio
----
. Unmount every RBD device `/dev/rbd*` before rebooting. For example:
+
[source,bash]
----
umount -vAf /dev/rbd0
----

If there are several device mounted, this little script can be used to avoid manual unmounting:

[source,bash]
----
#!/usr/bin/env bash

while grep "rbd" /proc/mounts > /dev/null 2>&1; do
  for dev in $(lsblk -p -o NAME | grep "rbd"); do
    if $(mountpoint -x $dev > /dev/null 2>&1); then
      echo ">>> umounting $dev"
      umount -vAf "$dev"
    fi
  done
done
----

include::admin-troubleshooting-etcd.adoc[etcd Debugging,leveloffset=+1]

== Kubernetes debugging tips

* General guidelines and instructions:
{kubedoc}tasks/debug-application-cluster/troubleshooting/

* Troubleshooting applications:
{kubedoc}tasks/debug-application-cluster/debug-application

* Troubleshooting clusters:
{kubedoc}tasks/debug-application-cluster/debug-cluster

* Debugging pods:
{kubedoc}tasks/debug-application-cluster/debug-pod-replication-controller

* Debugging services:
{kubedoc}tasks/debug-application-cluster/debug-service

== Helm `Error: context deadline exceeded`

This means the tiller installation was secured via SSL/TLS as described in <<helm_tiller_install>>.
You must pass the `--tls` flag to helm to enable authentication.


== AWS Deployment fails with `cannot attach profile` error

For {productname} to be properly deployed, you need to have proper IAM role, role policy and instance profile set up in AWS.
Under normal circumstances {tf} will be invoked by a user with suitable permissions during deployment and automatically create these profiles.
If your access permissions on the AWS account forbid {tf} from creating the profiles automatically, they must be created before attempting deployment.

=== Create IAM Role, Role Policy, and Instance Profile through AWS CLI

Users who do not have permission to create IAM role, role policy, and instance profile using {tf}, devops should create them for you, using the instructions below:

* `STACK_NAME`: Cluster Stack Name

. Install AWS CLI:
+
----
sudo zypper --gpg-auto-import-keys install -y aws-cli
----

. Setup AWS credentials:
+
----
aws configure
----

. Prepare role policy:
+
----
cat <<*EOF* >"./<STACK_NAME>-trust-policy.json"
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Action": "sts:AssumeRole",
      "Principal": {
        "Service": "ec2.amazonaws.com"
      },
      "Effect": "Allow",
      "Sid": ""
    }
  ]
}
*EOF*
----

. Prepare master instance policy:
+
----
cat <<*EOF* >"./<STACK_NAME>-master-role-trust-policy.json"
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "autoscaling:DescribeAutoScalingGroups",
        "autoscaling:DescribeLaunchConfigurations",
        "autoscaling:DescribeTags",
        "ec2:DescribeInstances",
        "ec2:DescribeRegions",
        "ec2:DescribeRouteTables",
        "ec2:DescribeSecurityGroups",
        "ec2:DescribeSubnets",
        "ec2:DescribeVolumes",
        "ec2:CreateSecurityGroup",
        "ec2:CreateTags",
        "ec2:CreateVolume",
        "ec2:ModifyInstanceAttribute",
        "ec2:ModifyVolume",
        "ec2:AttachVolume",
        "ec2:AuthorizeSecurityGroupIngress",
        "ec2:CreateRoute",
        "ec2:DeleteRoute",
        "ec2:DeleteSecurityGroup",
        "ec2:DeleteVolume",
        "ec2:DetachVolume",
        "ec2:RevokeSecurityGroupIngress",
        "ec2:DescribeVpcs",
        "elasticloadbalancing:AddTags",
        "elasticloadbalancing:AttachLoadBalancerToSubnets",
        "elasticloadbalancing:ApplySecurityGroupsToLoadBalancer",
        "elasticloadbalancing:CreateLoadBalancer",
        "elasticloadbalancing:CreateLoadBalancerPolicy",
        "elasticloadbalancing:CreateLoadBalancerListeners",
        "elasticloadbalancing:ConfigureHealthCheck",
        "elasticloadbalancing:DeleteLoadBalancer",
        "elasticloadbalancing:DeleteLoadBalancerListeners",
        "elasticloadbalancing:DescribeLoadBalancers",
        "elasticloadbalancing:DescribeLoadBalancerAttributes",
        "elasticloadbalancing:DetachLoadBalancerFromSubnets",
        "elasticloadbalancing:DeregisterInstancesFromLoadBalancer",
        "elasticloadbalancing:ModifyLoadBalancerAttributes",
        "elasticloadbalancing:RegisterInstancesWithLoadBalancer",
        "elasticloadbalancing:SetLoadBalancerPoliciesForBackendServer",
        "elasticloadbalancing:AddTags",
        "elasticloadbalancing:CreateListener",
        "elasticloadbalancing:CreateTargetGroup",
        "elasticloadbalancing:DeleteListener",
        "elasticloadbalancing:DeleteTargetGroup",
        "elasticloadbalancing:DescribeListeners",
        "elasticloadbalancing:DescribeLoadBalancerPolicies",
        "elasticloadbalancing:DescribeTargetGroups",
        "elasticloadbalancing:DescribeTargetHealth",
        "elasticloadbalancing:ModifyListener",
        "elasticloadbalancing:ModifyTargetGroup",
        "elasticloadbalancing:RegisterTargets",
        "elasticloadbalancing:SetLoadBalancerPoliciesOfListener",
        "iam:CreateServiceLinkedRole",
        "kms:DescribeKey"
      ],
      "Resource": [
        "*"
      ]
    }
  ]
}
*EOF*
----

. Prepare worker instance policy:
+
----
cat <<*EOF* >"./<STACK_NAME>-worker-role-trust-policy.json"
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": [
        "ec2:DescribeInstances",
        "ec2:DescribeRegions",
        "ecr:GetAuthorizationToken",
        "ecr:BatchCheckLayerAvailability",
        "ecr:GetDownloadUrlForLayer",
        "ecr:GetRepositoryPolicy",
        "ecr:DescribeRepositories",
        "ecr:ListImages",
        "ecr:BatchGetImage"
      ],
      "Resource": "*"
    }
  ]
}
*EOF*
----

. Create roles:
+
----
aws iam create-role --role-name <STACK_NAME>_cpi_master --assume-role-policy-document file://<FILE_DIRECTORY>/<STACK_NAME>-trust-policy.json
aws iam create-role --role-name <STACK_NAME>_cpi_worker --assume-role-policy-document file://<FILE_DIRECTORY>/<STACK_NAME>-trust-policy.json
----

. Create instance role policies:
+
----
aws iam put-role-policy --role-name <STACK_NAME>_cpi_master --policy-name <STACK_NAME>_cpi_master --policy-document file://<FILE_DIRECTORY>/<STACK_NAME>-master-role-trust-policy.json
aws iam put-role-policy --role-name <STACK_NAME>_cpi_worker --policy-name <STACK_NAME>_cpi_worker --policy-document file://<FILE_DIRECTORY>/<STACK_NAME>-worker-role-trust-policy.json
----

. Create instance profiles:
+
----
aws iam create-instance-profile --instance-profile-name <STACK_NAME>_cpi_master
aws iam create-instance-profile --instance-profile-name <STACK_NAME>_cpi_worker
----

. Add role to instance profiles:
+
----
aws iam add-role-to-instance-profile --role-name <STACK_NAME>_cpi_master --instance-profile-name <STACK_NAME>_cpi_master
aws iam add-role-to-instance-profile --role-name <STACK_NAME>_cpi_worker --instance-profile-name <STACK_NAME>_cpi_worker
----
