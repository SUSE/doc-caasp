//= Troubleshooting

This chapter summarizes frequent problems that can occur while using {productname}
and their solutions.

Additionally, {suse} support collects problems and their solutions online at link:https://www.suse.com/support/kb/?id=SUSE_CaaS_Platform[] .

== The `supportconfig` Tool

As a first step for any troubleshooting/debugging effort, you need to find out
the location of the cause of the problem. For this purpose we ship the `supportconfig` tool
and plugin with {productname}. With a simple command you can collect and compile
a variety of details about your cluster to enable {suse} support to pinpoint
the potential cause of an issue.

In case of problems, a detailed system report can be created with the
`supportconfig` command line tool. It will collect information about the system, such as:

* Current Kernel version
* Hardware information
* Installed packages
* Partition setup
* Cluster and node status

[TIP]
====
A full list of of the data collected by `supportconfig` can be found under
https://github.com/SUSE/supportutils-plugin-suse-caasp/blob/master/README.md.
====

[IMPORTANT]
====
To collect all the relevant logs, run the `supportconfig` command on all the master
and worker nodes individually.
====


[source,bash]
----
sudo supportconfig
sudo tar -xvJf /var/log/nts_*.txz
cd /var/log/nts*
sudo cat kubernetes.txt crio.txt
----

The result is a `TAR` archive of files. Each of the `*.txz` files should be given a name that can be used to identify which
cluster node it was created on.

After opening a Service Request (SR), you can upload the TAR archives to Global Technical Support.

The data will help to debug the issue you reported and assist you in solving the problem.
For details, see https://www.suse.com/documentation/sles-15/book_sle_admin/data/cha_adm_support.html.

== Cluster definition directory

Apart from the logs provided by running the `supportconfig` tool, an additional set of data might be required for
debugging purposes. This information is located at the Management node, under your cluster definition directory.
This folder contains important and sensitive information about your CaaSP cluster and it's the one
from where you issue `skuba` commands.

[TIP]
====
If the problem you are facing is related to your production environment, do **not** upload the `admin.conf` as this would
give us direct access to your cluster! Same applies for `pki` directory, since this also contains sensitive information
(ca cert and key). In case this is needed, we will setup a special call for you.
====

Create a `TAR` archive by compressing the cluster definition directory.
[source,bash]
----
# Read the TIP above
# Move the admin.conf and pki directory to another safe location
tar -czvf cluster.tar.gz /path/to/cluster/definition/directory
----

After opening a Service Request (SR), you can upload the TAR archive to Global Technical Support.

== Debugging SLES Nodes provision

If terraform fails to setup the required SLES infrastructure for your cluster, please provide the configuration
you applied in a form of a TAR archive.

Create a `TAR` archive by compressing the terraform.
[source,bash]
----
tar -czvf terraform.tar.gz /path/to/terraform/configuration
----

After opening a Service Request (SR), you can upload the TAR archive to Global Technical Support.

== Debugging Cluster Deployment

If the cluster deployment fails, please re-run the command again with setting verbosity level to 1 `-v=1`.

For example, if bootstraps the first master node of the cluster fails, re-run the command like
[source,bash]
----
skuba node bootstrap --user sles --sudo --target <IP/FQDN> <NODE NAME> -v=1
----

However, if the `join` procedure fails at the last final steps, re-running it might _not_ help. To verify
this, please list the current member nodes of your cluster and look for the one who failed.

[source,bash]
----
kubectl get nodes
----

If the node that failed to `join` is nevertheless listed in the output as part of your cluster,
then this is a bad indicator. This node cannot be reseted back to a clean state anymore and it's not safe to keep
it online in this _unknown_ state. As a result, instead of trying to fix its existing configuration either by hand or re-running
the join/bootstrap command, we would highly recommend you to remove this node completely from your cluster and
then replace it with a new one.

[source,bash]
----
skuba node remove <NODE NAME> --drain-timeout 5s
----

== Error `x509: certificate signed by unknown authority`

When interacting with Kubernetes, you might run into the situation where your existing configuration for the authentication has changed (cluster has been rebuild, certificates have been switched.)

In such a case you might see an error message in the output of your CLI or Web browser.

----
x509: certificate signed by unknown authority
----

This message indicates that your current system does not know the Certificate Authority (CA) that signed the SSL certificates used for encrypting the communication to the cluster. You then need to add or update the Root CA certificate in your local trust store.

. Obtain the root CA certificate from on of the {kube} cluster node, at the location `/etc/kubernetes/pki/ca.crt`

. Copy the root CA certificate into your local machine directory `/etc/pki/trust/anchors/`

. Update the cache for know CA certificates
+
[source,bash]
----
sudo update-ca-certificates
----

== Replacing a Lost Node

If your cluster loses a node, for example due to failed hardware,
remove the node as explained in <<removing_nodes>>.
Then add a new node as described in <<adding_nodes>>.

== Rebooting an Undrained Node with RBD Volumes Mapped

Rebooting a cluster node always requires a preceding `drain`.
In some cases, draining the nodes first might not be possible and some problem can occur during reboot if some RBD volumes are mapped to the nodes.

In this situation, apply the following steps.

. Make sure kubelet and crio are stopped:
+
[source,bash]
----
systemctl stop kubelet crio
----

. Unmount every RBD device `/dev/rbd*` before rebooting. For example:
+
[source,bash]
----
umount -vAf /dev/rbd0
----

If there are several device mounted, this little script can be used to avoid manual unmounting:

[source,bash]
----
#!/usr/bin/env bash

while grep "rbd" /proc/mounts > /dev/null 2>&1; do
  for dev in $(lsblk -p -o NAME | grep "rbd"); do
    if $(mountpoint -x $dev > /dev/null 2>&1); then
      echo ">>> umounting $dev"
      umount -vAf "$dev"
    fi
  done
done
----
