== Requirements

=== Platform

Currently we support:

* SUSE OpenStack Cloud 8
* VMware ESXi {vmware_version}
* Bare Metal x86_64

== Nodes

You will need at least two machines:

* 1 master node
* 1 worker node

{productname} {productversion} supports deployments with a single or multiple master nodes.
Production environments must be deployed with multiple master nodes for resilience.

All communication to the cluster is done through a load balancer talking to the respective nodes.
For that reason any failure tolerant environment must provide at least two load balancers for incoming communication.

The minimal viable failure tolerant production environment configuration consists of:

.Cluster nodes:
* 3 master nodes
* 2 worker nodes

.Dedicated Cluster Nodes
[IMPORTANT]
====
All cluster nodes must be dedicated (virtual) machines reserved for the purpose of running {productname}.
====

.Additional systems:
* Fault tolerant load balancing solution
+
(for example {sle} {hasi} with `pacemaker` and `haproxy`)
* 1 management workstation

=== Hardware

==== Management Workstation

In order to deploy and control a {productname} cluster you will need at least one
machine capable of running `skuba`. This typically is a regular desktop workstation or laptop
running SLE 15 SP1 or later.

==== Master Nodes

Up to 5 worker nodes *(minimum)*:

* Storage: 50 GB+
* (v)CPU: 2
* RAM: 4 GB
* Network: Minimum 1Gb/s (faster is preferred)

Up to 10 worker nodes:

* Storage: 50 GB+
* (v)CPU: 2
* RAM: 8 GB
* Network: Minimum 1Gb/s (faster is preferred)

Up to 100 worker nodes:

* Storage: 50 GB+
* (v)CPU: 4
* RAM: 16 GB
* Network: Minimum 1Gb/s (faster is preferred)

Up to 250 worker nodes:

* Storage: 50 GB+
* (v)CPU: 8
* RAM: 16 GB
* Network: Minimum 1Gb/s (faster is preferred)

[IMPORTANT]
====
Using a minimum of 2 (v)CPUs is a hard requirement, deploying
a cluster with less processing units is not possible.
====

==== Worker nodes

[IMPORTANT]
====
The worker nodes must have sufficient memory, CPU and disk space for the
Pods/containers/applications that are planned to be hosted on these workers.
====

A worker node requires the following resources:

* CPU cores: 0.250
* RAM: 1.2 GB

Based on these values, the *minimal* configuration of a worker node is:

* Storage: Depending on workloads, minimum 20-30 GB to hold the base OS and required packages. Mount additional storage volumes as needed.
* (v)CPU: 1
* RAM: 2 GB
* Network: Minimum 1Gb/s (faster is preferred)

Calculate the size of the required (v)CPU by adding up the base requirements, the estimated additional essential cluster components (logging agent, monitoring agent, configuration management, etc.) and the estimated CPU workloads:

* 0.250 (base requirements) + 0.250 (estimated additional cluster components) + estimated workload CPU requirements

Calculate the size of the RAM using the same formula:

* 1.2 GB (base requirements) + 500 MB (estimated additional cluster components) + estimated workload RAM requirements

[NOTE]
====
These values are provided as a guide to work in most cases. They may vary based on the type of the running workloads.
====

==== Storage Performance

For master and worker nodes you must ensure storage performance of at least 500 sequential IOPS with disk bandwidth depending on your cluster size.

    "Typically 50 sequential IOPS (for example, a 7200 RPM disk) is required.
    For heavily loaded clusters, 500 sequential IOPS (for example, a typical local SSD
    or a high performance virtualized block device) is recommended."

    "Typically 10MB/s will recover 100MB data within 15 seconds.
    For large clusters, 100MB/s or higher is suggested for recovering 1GB data
    within 15 seconds."

link:https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/hardware.md#disks[]

=== Networking

The management workstation needs at least the following networking permissions:

* SSH access to all machines in the cluster
* Access to the `apiserver` (the load balancer should expose it, port `6443`), that will in turn talk to any master in the cluster
* Access to Dex on the configured `NodePort` (the load balancer should expose it, port `32000`) so when the OIDC token has expired, `kubectl` can request a new token using the refresh token

==== Ports

[cols="3*.^,.^,.>"",options="header,autowidth"]
|===
|Node |Port |Protocol | Accessibility |Description

.8+|All nodes
|22
|TCP
|Internal
|SSH (required in public clouds)

|4240
|TCP
|Internal
|Cilium health check

|8472
|UDP
|Internal
|Cilium VXLAN

|10250
|TCP
|Internal
|Kubelet (API server -> kubelet communication)

|10256
|TCP
|Internal
|kube-proxy health check

|30000 - 32767
|TCP + UDP
|Internal
|Range of ports used by Kubernetes when allocating services of type `NodePort`

|32000
|TCP
|External
|Dex (OIDC Connect)

|32001
|TCP
|External
|Gangway (RBAC Authenticate)

.3+|Masters
|2379
|TCP
|Internal
|etcd (client communication)

|2380
|TCP
|Internal
|etcd (server-to-server traffic)

|6443
|TCP
|Internal / External
|Kubernetes API server

|===

==== Port Forwarding

The link:https://kubernetes.io/docs/concepts/cluster-administration/networking/[{kube} networking model] requires that your nodes have IP forwarding enabled in the kernel.
`skuba` checks this value when installing your cluster and installs a rule in `/etc/sysctl.d/90-skuba-net-ipv4-ip-forward.conf` to make it persistent.

Other software can potentially install rules with higher priority overriding this value and causing the cluster to fail.

You can manually check if this is enabled using the following command:

[source,bash]
----
# sysctl net.ipv4.ip_forward

net.ipv4.ip_forward = 1
----

`net.ipv4.ip_forward` must be set to `1`.


==== IP Addresses

[WARNING]
====
Using IPv6 addresses is currently not supported.
====

All nodes must be assigned static IPv4 addresses, which must not be changed manually afterwards.

[IMPORTANT]
====
Plan carefully for required IP ranges and future scenarios as
it is not possible to reconfigure the IP ranges once the deployment is complete.
====

===== Communication

Please make sure that all your Kubernetes components can communicate with each other.
This might require the configuration of routing when using multiple network adapters per node.

Refer to: https://kubernetes.io/docs/setup/independent/install-kubeadm/#check-network-adapters.

Configure firewall and other network security to allow communication on the default ports required by Kubernetes: https://kubernetes.io/docs/setup/independent/install-kubeadm/#check-required-ports

==== Performance

All master nodes of the cluster must have a minimum 1Gb/s network connection to fulfill the requirements for etcd.

    "1GbE is sufficient for common etcd deployments. For large etcd clusters,
    a 10GbE network will reduce mean time to recovery."

link:https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/hardware.md#network[]

==== Security

Do not grant access to the kubeconfig file or any workstation configured with this configuration to unauthorized personnel.
In the current state, full administrative access is granted to the cluster.

Authentication is done via the kubeconfig file generated during deployment. This file will grant full access to the cluster and all workloads.
Apply best practices for access control to workstations configured to administer the {productname} cluster.
