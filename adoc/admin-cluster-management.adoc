/= Cluster Management

Cluster management refers to several processes in the life cycle of a cluster and
its individual nodes: bootstrapping, joining, removing and resetting nodes.
For maximum automation and ease {productname} uses the `skuba` tool,
which simplifies Kubernetes cluster creation and reconfiguration.

== Bootstrap and initial configuration

Bootstrapping the cluster is the initial process of starting up a minimal
viable cluster and joining the first master node. Only the first master node needs to be bootstrapped,
later nodes can simply be joined as described in <<Adding nodes>>.

Before bootstrapping any nodes to the cluster,
you need to create an initial cluster definition folder (initialize the cluster).
This is done using `skuba cluster init` and its `--control-plane` flag.

For a step by step guide on how to initialize the cluster, configure updates using `kured`
and subsequently bootstrap nodes to it, refer to the _{productname} Deployment Guide_.

== Adding nodes

Once you have added the first master node to the cluster using `skuba node bootstrap`,
use the `skuba node join` command to add more nodes.

[source,bash]
skuba node join --role <master/worker> --user <user-name> --sudo --target <IP/FQDN> <node-name>

The mandatory flags for the join command are `--role`, ``--user`, `--sudo` and `--target`.

- `--role` serves to specify if the node is a *master* or *worker*
- `--sudo` is for running the command with superuser privileges,
this is necessary for all node operations.
- `<user-name>` is the name of the user that exists on your SLES machine (default: `sles`).
- `--target <IP/FQDN>` is the IP address or FQDN of the relevant machine.
- `<node-name>` is how you decide to name the node you are adding.

To add a new *worker* node, you would run something like:

[source,bash]
skuba node join --role worker --user sles --sudo --target 10.86.2.164 worker1

== Removing nodes

=== Temporary Removal

If you wish to remove a node temporarily, the recommended approach is to first drain the node using `kubectl`.
This will also cordon off the node, so that it doesn't receive any new workloads.

When you want to bring the node back, you only have to uncordon it.

Refer to the official Kubernetes documentation for more information:
https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/#use-kubectl-drain-to-remove-a-node-from-service

=== Permanent Removal

[IMPORTANT]
====
Nodes removed with this method can not be added back to the cluster. You must
reinstall the entire node and then join it again to the cluster.
====

The `skuba node remove` command serves to *permanently* remove nodes.
Running this command will work even if the target virtual machine is down,
so it is the safest way to remove the node.

[source,bash]
skuba node remove <node-name>

== Resetting nodes

The `skuba node reset` command enables you to reset a node to a state prior to `join` or `bootstrap` being run.
Use this command to retry a failed `bootstrap` or `join` on a node or if the node has to be evacuated from the cluster.

[source,bash]
skuba node reset --target <IP/FQDN> --sudo --user <user-name>

== Reconfiguring nodes

To reconfigure a node, for example to change the node role from worker to master, you will need to use a combination of commands.

. Run `skuba node remove <node-name>`.
. Reinstall the node from scratch.
. Run `skuba node join --role <desired-role> -user <user-name> --sudo --target <IP/FQDN> <node-name>`
