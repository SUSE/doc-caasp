//= Cluster Management

Cluster management refers to several processes in the life cycle of a cluster and
its individual nodes: bootstrapping, joining and removing nodes.
For maximum automation and ease {productname} uses the `skuba` tool,
which simplifies Kubernetes cluster creation and reconfiguration.

== Bootstrap and Initial Configuration

Bootstrapping the cluster is the initial process of starting up a minimal
viable cluster and joining the first master node. Only the first master node needs to be bootstrapped,
later nodes can simply be joined as described in <<adding_nodes>>.

Before bootstrapping any nodes to the cluster,
you need to create an initial cluster definition folder (initialize the cluster).
This is done using `skuba cluster init` and its `--control-plane` flag.

For a step by step guide on how to initialize the cluster, configure updates using `kured`
and subsequently bootstrap nodes to it, refer to the _{productname} Deployment Guide_.

[[adding_nodes]]
== Adding Nodes

Once you have added the first master node to the cluster using `skuba node bootstrap`,
use the `skuba node join` command to add more nodes. Joining master or worker nodes to
an existing cluster should be done sequentially, meaning the nodes have to be added
one after another and not more of them in parallel.

[source,bash]
skuba node join --role <master/worker> --user <user-name> --sudo --target <IP/FQDN> <node-name>

The mandatory flags for the join command are `--role`, `--user`, `--sudo` and `--target`.

- `--role` serves to specify if the node is a *master* or *worker*.
- `--sudo` is for running the command with superuser privileges,
which is necessary for all node operations.
- `<user-name>` is the name of the user that exists on your SLES machine (default: `sles`).
- `--target <IP/FQDN>` is the IP address or FQDN of the relevant machine.
- `<node-name>` is how you decide to name the node you are adding.

[IMPORTANT]
====
New master nodes that you didn't initially include in your Terraform's configuration have
to be manually added to your load balancer's configuration.
====


To add a new *worker* node, you would run something like:

[source,bash]
skuba node join --role worker --user sles --sudo --target 10.86.2.164 worker1

[[removing_nodes]]
== Removing Nodes

=== Temporary Removal

If you wish to remove a node temporarily, the recommended approach is to first drain the node using `kubectl`.
This will also cordon off the node, so that it doesn't receive any new workloads.

When you want to bring the node back, you only have to uncordon it.

Refer to the official Kubernetes documentation for more information:
https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/#use-kubectl-drain-to-remove-a-node-from-service

=== Permanent Removal

[IMPORTANT]
====
Nodes removed with this method cannot be added back to the cluster or any other
skuba-initiated cluster. You must reinstall the entire node and then join it
again to the cluster.
====

The `skuba node remove` command serves to *permanently* remove nodes.
Running this command will work even if the target virtual machine is down,
so it is the safest way to remove the node.

[source,bash]
skuba node remove <node-name>

[IMPORTANT]
====
After the removal of a master node, you have to manually delete its entries
from your load balancer's configuration.
====

== Reconfiguring Nodes

To reconfigure a node, for example to change the node's role from worker to master, you will need to use a combination of commands.

. Run `skuba node remove <node-name>`.
. Reinstall the node from scratch.
. Run `skuba node join --role <desired-role> --user <user-name> --sudo --target <IP/FQDN> <node-name>`.
