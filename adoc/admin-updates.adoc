[[handling_updates]]

== Update Requirements

[IMPORTANT]
====
Attempting a cluster update without updating the installed packages pattern on the management node, can lead to an incomplete or failed update.
====

Before updating a {productname} cluster, it's required to update packages installed by the `SUSE-CaaSP-Management` pattern on the management workstation.

The cluster update depends on updated skuba, but might also require new helm / {tf} or other dependencies which will be updated with the refreshed pattern.

Run `sudo zypper update` on the management workstation before any attempt to update the cluster.

== Updating Kubernetes Components

Updating of {kube} components is handled via `skuba`.

The general procedure should look like this:

. Update the packages and reboot your management workstation to get all the latest changes to skuba, helm and their dependencies
. Run `skuba addon upgrade plan` and then `skuba add upgrade apply` on the management workstation
. Apply all the configuration files that you modified for addons, the upgrade will have reset the configurations to defaults
. Check if there are addon upgrades available for the current cluster using `skuba addon upgrade plan`
.. Check if all the deployments in the cluster are compatible with the {kube} release that will be installed (refer to your individual deployments' documentation)
If the deployment is not compatible you must update it to ensure it working with the updated {kube}.
. Upgrade all master nodes by individually running `skuba node upgrade plan` and `skuba node upgrade apply`
. Upgrade all worker nodes by individually running `skuba node upgrade plan` and `skuba node upgrade apply`
. Check if new addons are available for the new version using `skuba addon upgrade plan`
.. If necessary upgrade master nodes and re-apply configuration changes again
.. If necessary upgrade nodes and re-apply configuration changes again
. Once all nodes are up to date, update helm and tiller and subsequently the helm deployments
. If you have other deployments not installed via {kube} or helm, update them last

=== Update Management Workstation

Run `sudo zypper up` on your management workstation to get the latest version of `skuba` and its dependencies.
Reboot the machine to make sure that all system changes are correctly applied.

=== Generating an Overview of Available Platform Updates

In order to get an overview of the updates available, you can run:

----
skuba cluster upgrade plan
----

This will show you a list of updates (if available) for different components
installed on the cluster. If the cluster is already running the latest available
versions, the output should look like this:

----
Current Kubernetes cluster version: 1.16.2
Latest Kubernetes version: 1.16.2

Congratulations! You are already at the latest version available
----

If the cluster has a new patch-level or minor {kube} version available, the
output should look like this:

----
Current Kubernetes cluster version: 1.15.2
Latest Kubernetes version: 1.16.2

Upgrade path to update from 1.15.2 to 1.16.2:
 - 1.15.2 -> 1.16.2
----

Similarly, you can also fetch this information on a per-node basis with the following command:

----
skuba node upgrade plan <NODE>
----

For example, if the cluster has a node named `worker0` which is running the latest available versions, the output should look like this:

----
Current Kubernetes cluster version: 1.16.2
Latest Kubernetes version: 1.16.2

Node worker0 is up to date
----

On the other hand, if this same node has a new patch-level or minor {kube} version available, the output should look like this:

----
Current Kubernetes cluster version: 1.15.2
Latest Kubernetes version: 1.16.2

Current Node version: 1.15.2

Component versions in worker0
  - kubelet: 1.15.2 -> 1.16.2
  - cri-o: 1.15.0 -> 1.16.0
----

You will get a similar output if there is a version available on a master node
(named `master0` in this example):

----
Current Kubernetes cluster version: 1.15.2
Latest Kubernetes version: 1.16.2

Current Node version: 1.15.2

Component versions in master0
  - apiserver: 1.15.2 -> 1.16.2
  - controller-manager: 1.15.2 -> 1.16.2
  - scheduler: 1.15.2 -> 1.16.2
  - etcd: 3.3.11 -> 3.3.15
  - kubelet: 1.15.2 -> 1.16.2
  - cri-o: 1.15.0 -> 1.16.0
----

It may happen that the {kube} version on the control plane is too outdated
for the update to progress. In this case, you would get output similar to the following:

----
Current Kubernetes cluster version: 1.15.0
Latest Kubernetes version: 1.15.0

Unable to plan node upgrade: at least one control plane does not tolerate the current cluster version
----

[TIP]
=====
The control plane consists of these components:

* apiserver
* controller-manager
* scheduler
* etcd
* kubelet
* cri-o
=====


=== Generating an Overview of Available Addon Updates

[NOTE]
====
Due to changes to the way `skuba` handles addons some existing components might be shown as `new addon` in the status output.
This is expected and no cause for concern. For any upgrade afterwards the addon will be considered known and only show available upgrades.
====

In order to get an overview of the addon updates available, you can run:

----
skuba addon upgrade plan
----

This will show you a list of updates (if available) for different addons
installed on the cluster:

----
Current Kubernetes cluster version: 1.15.2
Latest Kubernetes version: 1.16.2

Addon upgrades:
  - cilium: 1.5.3 -> 1.5.3 (manifest version from 0 to 4)
  - kured: 1.2.0 -> 1.3.0
  - dex: 2.16.0 -> 2.16.1
  - gangway: 3.1.0 -> 3.1.0 (manifest version from 1 to 2)
  - psp: 1.0.0 -> 1.1.0
----

If the cluster is already running the latest available
versions, the output should look like this:

----
Current Kubernetes cluster version: 1.16.2
Latest Kubernetes version: 1.16.2

Congratulations! Addons are already at the latest version available
----

Before updating the nodes you must apply the addon upgrades to your management workstation.
Please run:

----
skuba addon upgrade apply
----

== Updating Nodes

[NOTE]
====
It is recommended to use a load balancer with active health checks and pool management that
will take care of adding/removing nodes to/from the pool during this process.
====

Updates have to be applied separately to each node, starting with the control
plane all the way down to the worker nodes.

Note that the upgrade via `skuba node upgrade apply` will:

* Upgrade the containerized control plane.
* Upgrade the rest of the {kube} system stack (`kubelet`, `cri-o`).
* Restart services.

During the upgrade to a newer version, the API server will be unavailable.

During the upgrade all the pods in the worker node will be restarted so it is
recommended to drain the pods if your application requires high availability.
In most cases, the restart is handled by `replicaSet`.


=== How To Update Nodes

. Upgrade the master nodes:
+
----
skuba node upgrade apply --target <MASTER_NODE_IP> --user <USER> --sudo
----
+
. When all master nodes are upgraded, upgrade the worker nodes as well:
+
----
skuba node upgrade apply --target <WORKER_NODE_IP> --user <USER> --sudo
----
+
. Verify that your cluster nodes are upgraded by running:
+
----
skuba cluster upgrade plan
----

[TIP]
====
The upgrade via `skuba node upgrade apply` will:

* upgrade the containerized control plane.
* upgrade the rest of the {kube} system stack (`kubelet`, `cri-o`).
* restart services.
====

=== Check for Upgrades to New Version

Once you have upgraded all nodes, please run `skuba cluster upgrade plan` again.
This will show if any upgrades are available that required the versions you just installed.
If there are upgrades available please repeat the procedure until no more new upgrades are shown.

== Base OS Updates

Base operating system updates are handled by `skuba-update`, which works together
with the `kured` reboot daemon.

=== Disabling Automatic Updates

Nodes added to a cluster have the service `skuba-update.timer`, which is responsible for running automatic updates, activated by default.
This service calls the `skuba-update` utility and it can be configured with the `/etc/sysconfig/skuba-update` file.
To disable the automatic updates on a node, simply `ssh` to it and then configure the skuba-update service by editing the `/etc/sysconfig/skuba-update` file with the following runtime options:

----
## Path           : System/Management
## Description    : Extra switches for skuba-update
## Type           : string
## Default        : ""
## ServiceRestart : skuba-update
#
SKUBA_UPDATE_OPTIONS="--annotate-only"
----

[TIP]
It is not required to reload or restart `skuba-update.timer`.

The `--annotate-only` flag makes the `skuba-update` utility only check if updates are available and annotate the node accordingly.
When this flag is activated no updates are installed at all.

=== Completely Disabling Reboots

If you would like to take care of reboots manually, either as a temporary measure or permanently, you can disable them by creating a lock:

----
kubectl -n kube-system annotate ds kured weave.works/kured-node-lock='{"nodeID":"manual"}'
----

This command modifies an annotation (`annotate`) on the daemonset (`ds`) named `kured`.

=== Manual Unlock

In exceptional circumstances, such as a node experiencing a permanent failure whilst rebooting, manual intervention may be required to remove the cluster lock:

----
kubectl -n kube-system annotate ds kured weave.works/kured-node-lock-
----

This command modifies an annotation (`annotate`) on the daemonset (`ds`) named `kured`.
It explicitly performs an "unset" (`-`) for the value for the annotation named `weave.works/kured-node-lock`.
