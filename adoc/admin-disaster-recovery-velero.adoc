:toc:
:toclevels: 5
include::entities.adoc[]

= Disaster Recovery

link:https://velero.io/[Velero] is a solution for supporting {kube} cluster disaster recovery,
data migration and data protection by backing up {kube} cluster resources and persistent volumes to external supported storage backend on demand or by schedule.

The major functions include:

* Backup {kube} resources and persistent volumes for supported storage providers
* Restore {kube} resources and persistent volumes for supported storage providers
* When backing up persistent volumes w/o supported storage provider, Velero leverages link:https://github.com/restic/restic[restic] as an agnostic solution to backup this sort of persistent volumes under some known limitations.

User can leverage these fundamental functions to achieve below user stories:

* Backup whole {kube} cluster resources then restore if any {kube} resources loss
* Backup selected {kube} resources then restore if the selected {kube} resources loss
* Backup selected {kube} resources and persistent volumes then restore if the {kube} selected {kube} resources loss or data loss
* Migrate the backup to another clusters for any purpose like migrates production cluster to develop cluster for testing.

Velero consists of below components:

* A Velero server that runs on your {kube} cluster
* A `restic` deployed on each worker nodes  that runs on your {kube} cluster (optional)
* A command-line client that runs locally

== Prerequisites

=== Helm

In order to successfully use Velero to backup and restore the {kube} cluster, you first need to install Helm and Tiller.
Refer to <<helm_tiller_install>>.
. SUSE Helm Chart

Add SUSE helm chart repository URL

[source,bash]
----
helm repo add suse https://kubernetes-charts.suse.com
----

=== Object Storage

Velero uses object storage to store backups and associated artifacts.
It can optionally snapshot persistent volume and store in object storage by `restic` if there is no supported volume snapshot provider.

==== Storage Providers

[options="header"]
|===
| Provider | Object Storage | Plugin Provider Repo

|Amazon Web Services (AWS)
|AWS S3
|link:https://github.com/vmware-tanzu/velero-plugin-for-aws[Velero plugin for AWS]

|Google Cloud Platform (GCP)
|Google Cloud Storage
|link:https://github.com/vmware-tanzu/velero-plugin-for-gcp[Velero plugin for GCP]

|Microsoft Azure
|Azure Blob Storage
|link:https://github.com/vmware-tanzu/velero-plugin-for-microsoft-azure[Velero plugin for Microsoft Azure]
|===

==== S3-compatible Storage Providers
===== SES6 Ceph Object Gateway (`radosgw`)

SUSE supports the SES6 Ceph Object Gateway (`radosgw`) as a S3-compatible object storage provider.
Refer to link:https://documentation.suse.com/ses/6/html/ses-all/cha-ceph-additional-software-installation.html[SES6 Object Gateway Manual Installation] on how to install it.

===== Minio

Besides {ses}, there is an alternative open source S3-compatible object storage provider: link:https://min.io/[minio].

Prepare an external host and install Minio on the host.

[source,bash]
----
# Download Minio server
wget https://dl.min.io/server/minio/release/linux-amd64/minio
chmod +x minio

# Expose Minio access_key and secret_key
export MINIO_ACCESS_KEY=<access_key>
export MINIO_SECRET_KEY=<secret_key>

# Start Minio server
mkdir -p bucket
./minio server bucket &

# Download Minio client
wget https://dl.min.io/client/mc/release/linux-amd64/mc
chmod +x mc

# Setup Minio server
./mc config host add Velero http://localhost:9000 $MINIO_ACCESS_KEY $MINIO_SECRET_KEY

# Create bucket on Minio server
./mc mb -p velero/velero
----

For the rests of the S3-compatible storage providers, refer to link:https://velero.io/docs/v1.2.0/supported-providers/[Velero Supported Providers].

==== Object Storage Credential
===== Public Cloud Providers
====== AWS S3

Create the credential file `credentials-velero` in the local machine.

----
[default]
aws_access_key_id=<AWS_ACCESS_KEY_ID>
aws_secret_access_key=<AWS_SECRET_ACCESS_KEY>
----

Please refer to link:https://github.com/vmware-tanzu/velero-plugin-for-aws/tree/v1.0.0[Velero Plugin For AWS].

====== Google Cloud Storage

Create the credential file `credentials-velero` in the local machine.

Please refer to link:https://github.com/vmware-tanzu/velero-plugin-for-gcp/tree/v1.0.0[Velero Plugin For GCP].

====== Azure Blob Storage

Create the credential file `credentials-velero` in the local machine.

Please refer to link:https://github.com/vmware-tanzu/velero-plugin-for-microsoft-azure/tree/v1.0.0[Velero Plugin For Azure].

===== S3-compatible Storage Providers, like `radosgw`

Create the credential file `credentials-velero` in the local machine.

----
[default]
aws_access_key_id=<S3_COMPATIBLE_STORAGE_ACCESS_KEY_ID>
aws_secret_access_key=<S3_COMPATIBLE_STORAGE_SECRET_ACCESS_KEY>
----

==== Volume Snapshotter

[NOTE]
A Volume snapshotter is able to snapshot it's persistent volumes if it's volume driver supports do volume snapshot.
If a volume provider does not support snapshot or does not have supported Velero storage plugin, Velero will leverage `restic` to do persistent volume backup and restore.

[IMPORTANT]
Please note what persistent volumes are supported for snapshot by storage providers.
Then administrators have to determine whether to deploy the `restic` DaemonSet or not.

[options="header"]
|===
|Provider | Volume Snapshotter | Plugin Provider Repo
|Amazon Web Services (AWS) | AWS EBS | link:https://github.com/vmware-tanzu/velero-plugin-for-aws[Velero plugin for AWS]
|===

For the other `snapshotter` providers refer to link:https://velero.io/docs/v1.2.0/supported-providers/[Velero Supported Providers].

. Finally, install Velero CLI
+
[source,bash]
----
sudo zypper install velero
----

== Limitations

. Velero doesn't overwrite objects in-cluster if they already exist.
. Velero supports a single set of credentials _per provider_.
It's not yet possible to use different credentials for different object storage locations for the same provider.
. Volume snapshots are limited by where your provider allows you to create snapshots.
For example, AWS and Azure do not allow you to create a volume snapshot in a different region than where the volume is located.
If you try to take a Velero backup using a volume snapshot location with a different region than where your cluster's volume are, the backup will fail.
. It is not yet possible to send a single Velero backup to multiple backup storage locations simultaneously, or a single volume snapshot to multiple locations simultaneously.
However, you can setup multiple backups manually or scheduled that differ only in the storage locations.
. Cross-provider snapshots are not supported. If you have a cluster with more than one type of volume (e.g. NFS and Ceph), but you only have a volume snapshot location configured for NFS, then Velero will _only_ snapshot the NFS volumes.
. `Restic` data is stored under a prefix/subdirectory of the main Velero bucket and will go into the bucket corresponding backup storage location selected by the user at backup creation time.
. When recovering, the {kube} version, Velero version (includes container version), and Helm version have to be _exactly_ the same as the original cluster.
. When performing cluster migration, the new cluster number of nodes should be equal or greater than the original cluster.

For more information about storage and snapshot locations, Refer to: link:https://velero.io/docs/v1.2.0/locations/[Velero: Backup Storage Locations and Volume Snapshot Locations]

== Known Issues

. Velero reports errors when when restoring Cilium CRDs. However, this does not effect Cilium functionality.
+
[NOTE]
====
You can add a label to Cilium CRDs to skip Velero backup.

[source,bash]
----
kubectl label -n kube-system customresourcedefinitions/ciliumendpoints.cilium.io velero.io/exclude-from-backup=true

kubectl label -n kube-system customresourcedefinitions/ciliumnetworkpolicies.cilium.io velero.io/exclude-from-backup=true
----
====

. When restoring `dex` and `gangway`, Velero reports `NodePort` cannot be restored since `dex` and `gangway` are deployed by an addon already and the same `NodePort` has be registered.
However, this does not break the `dex` and `gangway` service access from outside.
+
[NOTE]
====
You can add a label to services `oidc-dex` and `oidc-gangway` to skip Velero backup.

[source,bash]
----
kubectl label -n kube-system services/oidc-dex velero.io/exclude-from-backup=true

kubectl label -n kube-system services/oidc-gangway velero.io/exclude-from-backup=true
----
====

== Deployment

Use Helm CLI to install Velero deployment and `restic` (_optional_).

=== {kube} cluster on-premise and _without_ backup persistent volume.

For the case that the external storage _supports_ volume snapshot natively and does not need Velero to backup the persistent volume data with `restic`.

==== Backup location on public cloud providers

. The backup bucket name _BUCKET_NAME_. (The bucket name in S3 object storage)
. The backup region name _REGION_NAME_. (The region name for the S3 object storage. For example, `us-east-1` for AWS US East (N. Virginia))
. The Velero installed namespace _NAMESPACE_, the default namespace is `velero`. (optional)

[source,bash]
----
helm install \
    --name velero \
    --namespace <NAMESPACE> \
    --set-file credentials.secretContents.cloud=credentials-velero \
    --set configuration.provider=aws \
    --set configuration.backupStorageLocation.name=default \
    --set configuration.backupStorageLocation.name=<BUCKET_NAME> \
    --set configuration.backupStorageLocation.config.region=<REGION_NAME> \
    --set snapshotsEnabled=false \
    --set initContainers[0].name=velero-plugin-for-aws \
    --set initContainers[0].image=registry.suse.com/caasp/v4/velero-plugin-for-aws:1.0.0 \
    --set initContainers[0].volumeMounts[0].mountPath=/target \
    --set initContainers[0].volumeMounts[0].name=plugins \
    suse/velero
----

Then, suggests to create at least one additional backup locations point to different object storage server to prevent object storage server single point of failure.

[source,bash]
----
velero backup-location create slave \
    --provider aws \
    --bucket <BUCKET_NAME> \
    --config region=<REGION_NAME>
----

==== Backup location on S3-compatible storage providers

. The backup bucket name _BUCKET_NAME_. (The bucket name in S3 object storage)
. The backup region name _REGION_NAME_. (The region name for the S3 object storage. For example, radosgw _or_ master/slave if you have HA S3 object storage backups)
. The S3-compatible object storage simulates the S3 object storage.

Therefore, the configuration for S3-compatible object storage have to setup additional configurations.

----
configuration.backupStorageLocation.config.s3ForcePathStyle=true
configuration.backupStorageLocation.config.s3Url=<S3_COMPATIBLE_STORAGE_SERVER__URL>
----

===== Setting the namespace for Velero installation _NAMESPACE_,. (optional)

The default namespace is `velero`.

[source,bash]
----
helm install \
    --name velero \
    --namespace <NAMESPACE> \
    --set-file credentials.secretContents.cloud=credentials-velero \
    --set configuration.provider=aws \
    --set configuration.backupStorageLocation.name=default \
    --set configuration.backupStorageLocation.name=<BUCKET_NAME> \
    --set configuration.backupStorageLocation.config.region=<REGION_NAME> \
    --set configuration.backupStorageLocation.config.s3ForcePathStyle=true \
    --set configuration.backupStorageLocation.config.s3Url=<S3_COMPATIBLE_STORAGE_SERVER_URL> \
    --set snapshotsEnabled=false \
    --set initContainers[0].name=velero-plugin-for-aws \
    --set initContainers[0].image=registry.suse.com/caasp/v4/velero-plugin-for-aws:1.0.0 \
    --set initContainers[0].volumeMounts[0].mountPath=/target \
    --set initContainers[0].volumeMounts[0].name=plugins \
    suse/velero
----

Then, suggests to create at least one additional backup location point to different object storage server to prevent object storage server single point of failure.

[source,bash]
----
velero backup-location create slave \
    --provider aws \
    --bucket <BUCKET_NAME> \
    --config region=slave,s3ForcePathStyle=true,s3Url=<S3_COMPATIBLE_STORAGE_SERVER_URL>
----

=== {kube} cluster on-premise and _with_ backup persistent volume.

For the case that the external storage _not supports_ volume snapshot natively and need Velero to backup the persistent volume data by restic.

==== The backup location on public cloud providers

. The backup bucket name _BUCKET_NAME_. (The bucket name in S3 object storage)
. The backup region name _REGION_NAME_. (The region name for the S3 object storage. For example, `us-east-1` for AWS US East (N. Virginia))
. The Velero installed namespace _NAMESPACE_, the default namespace is `velero`. (optional)

[NOTE]
The public cloud provider supports persistent volume snapshot API. Therefore, we _do not have to_ deploy the `restic` DaemonSet.

[source,bash]
----
helm install \
    --name velero \
    --namespace <NAMESPACE> \
    --set-file credentials.secretContents.cloud=credentials-velero \
    --set configuration.provider=aws \
    --set configuration.backupStorageLocation.name=default \
    --set configuration.backupStorageLocation.name=<BUCKET_NAME> \
    --set configuration.backupStorageLocation.config.region=<REGION_NAME> \
    --set snapshotsEnabled=true \
    --set configuration.volumeSnapshotLocation.name=default \
    --set configuration.volumeSnapshotLocation.config.region=<REGION_NAME> \
    --set initContainers[0].name=velero-plugin-for-aws \
    --set initContainers[0].image=registry.suse.com/caasp/v4/velero-plugin-for-aws:1.0.0 \
    --set initContainers[0].volumeMounts[0].mountPath=/target \
    --set initContainers[0].volumeMounts[0].name=plugins \
    suse/velero
----

Then, suggest to create at least one additional backup locations point to different object storage server to prevent object storage server single point of failure.

[source,bash]
----
velero backup-location create slave \
    --provider aws \
    --bucket <BUCKET_NAME> \
    --config region=<REGION_NAME>
----

==== Backup location on S3-compatible storage providers

. The backup bucket name _BUCKET_NAME_. (The bucket name in S3 object storage)
. The backup region name _REGION_NAME_. (The region name for the S3 object storage. For example, radosgw _or_ master/slave if you have HA S3 object storage backups)
. The S3-compatible object storage simulates the S3 object storage.

Therefore, the configuration for S3-compatible object storage have to setup additional configurations

----
configuration.backupStorageLocation.config.s3ForcePathStyle=true
configuration.backupStorageLocation.config.s3Url=<S3_COMPATIBLE_STORAGE_SERVER__URL>
----

===== Setting the namespace for Velero installation _NAMESPACE_,. (optional)

The default namespace is `velero`.

[NOTE]
Mostly the on-premise persistent volume does not supports snapshot API or does not have community supportded snapshotter providers (for example, the NFS volume does not supports the snapshot API). Therefore, we _have to_ deploy the `restic` DaemonSet.

[source,bash]
----
helm install \
    --name velero \
    --namespace <NAMESPACE> \
    --set-file credentials.secretContents.cloud=credentials-velero \
    --set configuration.provider=aws \
    --set configuration.backupStorageLocation.name=default \
    --set configuration.backupStorageLocation.name=<BUCKET_NAME> \
    --set configuration.backupStorageLocation.config.region=<REGION_NAME> \
    --set configuration.backupStorageLocation.config.s3ForcePathStyle=true \
    --set configuration.backupStorageLocation.config.s3Url=<S3_COMPATIBLE_STORAGE_SERVER_URL> \
    --set snapshotsEnabled=true \
    --set deployRestic=true \
    --set configuration.volumeSnapshotLocation.name=default \
    --set configuration.volumeSnapshotLocation.config.region=minio \
    --set initContainers[0].name=velero-plugin-for-aws \
    --set initContainers[0].image=registry.suse.com/caasp/v4/velero-plugin-for-aws:1.0.0 \
    --set initContainers[0].volumeMounts[0].mountPath=/target \
    --set initContainers[0].volumeMounts[0].name=plugins \
    suse/velero
----

Then, suggest to create at least one additional backup locations point to different object storage server to prevent object storage server single point of failure.

[source,bash]
----
velero backup-location create slave \
    --provider aws \
    --bucket <BUCKET_NAME> \
    --config region=slave,s3ForcePathStyle=true,s3Url=<S3_COMPATIBLE_STORAGE_SERVER_URL>
----

[NOTE]
For troubleshooting a velero deployment, refer to: link:https://velero.io/docs/v1.2.0/debugging-install/[Velero: Debugging Installation Issues]

== Operations

=== Backup
==== Annotate Persistent Volume (optional)

If the persistent volume in the supported volume `snapshotter` provider, skip this procedure.

However, if we deploy the `restic` DaemonSet and want to backup the persistent volume by `restic`, we have to add annotation `backup.velero.io/backup-volumes=<VOLUME_NAME_1>,<VOLUME_NAME_2>,...` to the pods which have mounted the volume manually.

For example, we deploy a Elasticsearch cluster and want to backup the Elasticsearch cluster's data. Add the annotation to the Elasticsearch cluster pods:

[source,bash]
----
kubectl annotate pod/elasticsearch-master-0 backup.velero.io/backup-volumes=elasticsearch-master
kubectl annotate pod/elasticsearch-master-1 backup.velero.io/backup-volumes=elasticsearch-master
kubectl annotate pod/elasticsearch-master-2 backup.velero.io/backup-volumes=elasticsearch-master
----

[NOTE]
Velero currently does not provide a mechanism to detect persistent volume claims that are missing the `restic` backup annotation.
To solve this, there is a community provided controller link:https://github.com/bitsbeats/velero-pvc-watcher[velero-pvc-watcher] which integrates Prometheus to generate alerts for volumes that are not in the backup or backup-exclusion annotation.

==== Manual Backup

[source,bash]
----
velero backup create <BACKUP_NAME>
----

==== Setting Backup Schedule

The schedule template in cron notation, using UTC time. The schedule can also be expressed using `@every <duration>` syntax.
The duration can be specified using a combination of seconds (s), minutes (m), and hours (h), for example: `@every 2h30m`.

[source,bash]
----
# Create schedule template
# Create a backup every 6 hours
velero schedule create <SCHEDULE_NAME> --schedule="0 */6 * * *"

# Create a backup every 6 hours with the @every notation
velero schedule create <SCHEDULE_NAME> --schedule="@every 6h"

# Create a daily backup of the web namespace
velero schedule create <SCHEDULE_NAME> --schedule="@every 24h" --include-namespaces web

# Create a weekly backup, each living for 90 days (2160 hours)
velero schedule create <SCHEDULE_NAME> --schedule="@every 168h" --ttl 2160h0m0s
----

[options="header"]
|===
| Character Position | Character Period | Acceptable Values
|1 |Minute |`0-59,*`
|2 |Hour |`0-23,*`
|3 |Day of Month |`1-31,*`
|4 |Month |`1-12,*`
|5 |Day of Week |`0-7,*`
|===

==== Optional Flags

===== Granularity

====== Cluster

Without pass extra flags to `velero backup create`, Velero will backup whole {kube} cluster.

====== Namespace

Pass flag `--include-namespaces` or `--exclude-namespaces` to specifies which namespaces to include/exclude when backing up. For example:

[source,bash]
----
# Create a backup including the nginx and default namespaces
velero backup create backup-1 --include-namespaces nginx,default

# Create a backup excluding the kube-system and default namespaces
velero backup create backup-1 --exclude-namespaces kube-system,default
----

====== Resources

Pass flag `--include-resources` or `--exclude-resources` to specifies which resources to include/exclude when backing up. For example:

[source,bash]
----
# Create a backup including storageclass resource only
velero backup create backup-1 --include-resources storageclasses
----

[TIP]
Use `kubectl api-resources` to lists all API resources on the server.

====== Label Selector

Pass `--selector` to only back up resources matching the label selector.

[source,bash]
----
# Create a backup for the elasticsearch cluster only
velero backup create backup-1 --selector app=elasticsearch-master
----

===== Location

Pass `--storage-location` to specify where the backup stores to. For example, if we have HA object storage server called master and slave respectively.

[source,bash]
----
# Create a backup to master storage server
velero backup create backup2master --storage-location master

# Create a backup to slave storage server
velero backup create backup2slave --storage-location slave
----

==== Garbage collection

Pass `--ttl` to determine how long keeps the backup, after that, the backup will be garbage collected. The default backup existed time is 720 hours (30 days).

==== Exclude Specific Items from Backup

To exclude individual items from being backed up, even if they match the resource/namespace/label selectors defined in the backup spec. To do this, label the item as follows:

[source,bash]
----
kubectl label -n <ITEM_NAMESPACE> <RESOURCE>/<NAME> velero.io/exclude-from-backup=true
----

==== Troubleshooting
===== List backups

[source,bash]
----
velero backup get
----
===== Describe backups

[source,bash]
----
velero backup describe <BACKUP_NAME_1> <BACKUP_NAME_2> <BACKUP_NAME_3>
----
===== Retrieve backup logs

[source,bash]
----
velero backup logs <BACKUP_NAME>
----

== Restore

=== Manual Restore

[source,bash]
----
velero restore create <RESTORE_NAME> --from-backup <BACKUP_NAME>
----

For example:

[source,bash]
----
# Create a restore named "restore-1" from backup "backup-1"
velero restore create restore-1 --from-backup backup-1

# Create a restore with a default name ("backup-1-<timestamp>") from backup "backup-1"
velero restore create --from-backup backup-1
----

=== Setting Restore Schedule

[source,bash]
----
velero restore create <RESTORE_NAME> --from-schedule <SCHEDULE_NAME>
----

For example:

[source,bash]
----
# Create a restore from the latest successful backup triggered by schedule "schedule-1"
velero restore create --from-schedule schedule-1

# Create a restore from the latest successful OR partially-failed backup triggered by schedule "schedule-1"
velero restore create --from-schedule schedule-1 --allow-partially-failed
----

=== Optional Flags

==== Granularity

Without pass extra flags to `velero restore create`, Velero will restore whole resources from the backup or from the schedule.

===== Namespace

Pass flag `--include-namespaces` or `--exclude-namespaces` to `velero restore create` to specifies which namespaces to include/exclude when restoring. For example:

[source,bash]
----
# Create a restore including the nginx and default namespaces
velero restore create --from-backup backup-1 --include-namespaces nginx,default

# Create a restore excluding the kube-system and default namespaces
velero restore create --from-backup backup-1 --exclude-namespaces kube-system,default
----

===== Resources

Pass flag `--include-resources` or `--exclude-resources` to `velero restore create` to specifies which resources to include/exclude when restoring. For example:

[source,bash]
----
# create a restore for only persistentvolumeclaims and persistentvolumes within a backup
velero restore create --from-backup backup-1 --include-resources persistentvolumeclaims,persistentvolumes
----

[TIP]
Use `kubectl api-resources` to lists all API resources on the server.

===== Label Selector

Pass `--selector` to only restore the resources matching the label selector. For example:

[source,bash]
----
# create a restore for only the elasticsearch cluster within a backup
velero restore create --from-backup backup-1 --selector app=elasticsearch-master
----

==== Troubleshooting
===== Retrieve restores

[source,bash]
----
velero restore get
----
===== Describe restores

[source,bash]
----
velero restore describe <RESTORE_NAME_1> <RESTORE_NAME_2> <RESTORE_NAME_3>
----
===== Retrieve restore logs

[source,bash]
----
velero restore logs <RESTORE_NAME>
----

[NOTE]
For troubleshooting velero restore, refer to: link:https://velero.io/docs/v1.2.0/debugging-restores/[Velero: Debugging Restores]

== Use Cases

=== Disaster Recovery

Use schedule backup for periodical backup. When restoring, need to change backup storage location to read-only mode to avoid incorrect data integrity.
Run the backup periodically. When the {kube} cluster runs into unexpected state, recover from the backup file.

==== Setting up a scheduled backup

[source,bash]
----
velero schedule create <SCHEDULE_NAME> --schedule="@daily"
----

This creates a backup file with the name `<SCHEDULE_NAME>-<TIMESTAMP>``.

. When disaster happens, make sure the Velero server and `restic` DaemonSet exists (optional). If not, reinstall from the helm chart.
. Update the backup storage location to read-only mode (it prevents the backup file from being created or deleted in the backup storage location during the restore process)
+
[source,bash]
----
kubectl patch backupstoragelocation <STORAGE_LOCATION_NAME> \
    --namespace <NAMESPACE> \
    --type merge \
    --patch '{"spec":{"accessMode":"ReadOnly"}}'
----

. Create a restore from the most recent backup file.
+
[source,bash]
----
velero restore create --from-backup <SCHEDULE_NAME>-<TIMESTAMP>
----

. After restoring finished, change the backup storage location to read-write mode.
+
[source,bash]
----
kubectl patch backupstoragelocation <STORAGE_LOCATION_NAME> \
    --namespace <NAMESPACE> \
    --type merge \
    --patch '{"spec":{"accessMode":"ReadWrite"}}'
----

=== Cluster Migration

Migrate the {kube} cluster from `cluster 1` to `cluster 2`, as long as you point different cluster's Velero instance to the same external object storage location.

[NOTE]
====
Velero does not support the migration of persistent volumes across public cloud providers.
====

. (At cluster 1) Backup the entire {kube} cluster manually
+
[source,bash]
----
velero backup create <BACKUP_NAME>
----

. (At cluster 2) Prepare a {cluster} cluster deployed by skuba.

. (At cluster 2) Helm install Velero and make sure the backup-location and snapshot-location points to the same location as cluster 1.
+
[source,bash]
----
velero backup-location get
velero snapshot-location get
----
+
[NOTE]
The default sync interval is 1 minute. You could change the interval with flag `--backup-sync-period` when create backup location.

. (At cluster 2) Make sure the cluster 1 backup resources are sync to the external object storage server.
+
[source,bash]
----
velero backup get <BACKUP_NAME>
velero backup describe <BACKUP_NAME>
----

. (At cluster 2) Restore the cluster from the backup file.
+
[source,bash]
----
velero restore create --from-backup <BACKUP_NAME>
----

. (At cluster 2) Verify the cluster is behaving correctly
+
[source,bash]
----
velero restore get
velero restore describe <RESTORE_NAME>
velero restore logs <RESTORE_NAME>
----

. (At cluster 2) Since Velero doesn't overwrite objects in-cluster if they already exist. Manual check all addons configuration is desired after cluster restored.
.. Check dex configuration.
+
[source,bash]
----
# Download dex.yaml
kubectl get configmap oidc-dex-config -o yaml > oidc-dex-config.yaml

# Edit oidc-dex-config.yaml to desired
vim oidc-dex-config.yaml

# Apply new oidc-dex-config.yaml
kubectl apply -f oidc-dex-config.yaml

# Restart oidc-dex deployment
kubectl delete pod -l app=oidc-dex
----
.. Check gangway configuration.
+
[source,bash]
----
# Download gangway.yaml
kubectl get configmap oidc-gangway-config -o yaml > oidc-gangway-config.yaml

# Edit oidc-gangway-config.yaml to desired
vim oidc-gangway-config.yaml

# Apply new oidc-gangway-config.yaml
kubectl apply -f oidc-gangway-config.yaml

# Restart oidc-gangway deployment
kubectl delete pod -l app=oidc-dex
----
.. Check kured is disabled automatically reboots.
+
[source,bash]
----
kubectl get daemonset kured -o yaml
----
.. Check psp is what you desired.
+
[source,bash]
----
kubectl get psp suse.caasp.psp.privileged -o yaml
kubectl get clusterrole suse:caasp:psp:privileged -o yaml
kubectl get rolebinding suse:caasp:psp:privileged -o yaml

kubectl get psp suse.caasp.psp.unprivileged -o yaml
kubectl get clusterrole suse:caasp:psp:unprivileged -o yaml
kubectl get clusterrolebinding suse:caasp:psp:default -o yaml
----

== Uninstall
Remove the Velero server deployment and `restic` DaemonSet if exists.
Then, delete Velero custom resource definitions (CRDs).
[source,bash]
----
helm del --purge velero
kubectl delete crds -l app.kubernetes.io/name=velero
----
