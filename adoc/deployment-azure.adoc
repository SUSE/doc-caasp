== Deployment on Microsoft Azure

include::common_tech_preview.adoc[]

.Preparation Required
[NOTE]
====
You must have completed <<deployment-preparations>> to proceed.
====

You will use {tf} to deploy the whole infrastructure described in
<<architecture-azure>>. Then you will use the `skuba` tool to bootstrap the
{kube} cluster on top of it.


[#architecture-azure]
=== {azure} Deployment

The {azure} deployment created by our {tf} template files leads to the
creation of the infrastructure described in the next paragraphs.

==== Setup service principal for {tf} login credential

Follow the guide link:https://www.terraform.io/docs/providers/azurerm/guides/service_principal_client_secret.html#creating-a-service-principal-in-the-azure-portal[Creating a Service Principal in the Azure Portal], and set up `ARM_CLIENT_ID`, `ARM_CLIENT_SECRET`, `ARM_SUBSCRIPTION_ID`, `and ARM_TENANT_ID` in `container-openrc.sh`.

[source,bash]
----
export ARM_CLIENT_ID=00000000-0000-0000-0000-000000000000
export ARM_CLIENT_SECRET=000000000000000000000000000000000
export ARM_SUBSCRIPTION_ID=00000000-0000-0000-0000-000000000000
export ARM_TENANT_ID=00000000-0000-0000-0000-000000000000
----

Load the helper script into the environment before deploying the {tf} script.:

[source,bash]
----
source container-openrc.sh
----

==== Network

The infrastructure is created inside of a user specified {azure} region.
The resources are currently all located inside of the user specified availability
zones. All the nodes are placed inside of the same virtual network, within the same
subnet.

Worker nodes are never exposed to the public internet. On the other hand,
each master node has a public IP address by default. This allows users to
connect to them via SSH from their computers.

It's also possible to disable this behavior and make **all** the nodes private.
This can be done setting the `create_bastionhost` variable to `true`.

When this variable is set, all the master nodes cease to have a public IP address and
an link:https://docs.microsoft.com/en-us/azure/bastion/bastion-overview[Azure Bastion]
instance is created which becomes the only way to SSH into the cluster.

==== Load Balancer

The {tf} template files take care of creating a
link:https://azure.amazon.com/elasticloadbalancing/[Classic Load Balancer]
which exposes the {kube} API service deployed on the control plane
nodes.

The load balancer exposes the following ports:

* `6443`: Kubernetes API server
* `32000`: Dex (OIDC Connect)
* `32001`: Gangway (RBAC Authenticate)

==== Accessing the nodes

A default `sles` user is created on each node of the cluster. The user has
administrator capabilities by using the `sudo` utility.

By default, password based authentication is disabled. It's possible to log
using the SSH key specified via the `admin_ssh_key` variable.
It's also possible to enable password based authentication by specifying a
value for the `admin_password` variable.

[NOTE]
====
{azure} has some security checks in place to avoid the usage of weak passwords.
====

When the bastion host creation is disabled, the access to the master nodes of
the cluster is just a matter of logging in via SSH to the public IP address.

Accessing a cluster through an {azure} bastion requires a different procedure.

==== Using {azure} Bastion

{azure} bastion supports only RSA keys with PEM format. These can be created by
running:

```
ssh-keygen -f azure -t rsa -b 4096 -m pem
```

This will create a public and private key named `azure`. The **private** key has
to be provided later to {azure}, hence it's strongly recommended to create
a dedicated key pair.

Once the whole infrastructure is created, you can connect to any node of the
cluster by performing the following steps:

* Log into {azure} portal
* Choose one of the nodes of the cluster
* Click "connect" and select "bastion" as option
* Enter all the required fields

Once this is done, a new browser tab will be open with a shell session running
inside of the desired node.

[NOTE]
====
It's recommended to use Chrome or Chromium during this process.
====

You can SSH into the first master node of the cluster to drive the creation of a {productname} cluster
from there using this guide link:https://docs.microsoft.com/en-us/azure/bastion/bastion-connect-vm-ssh[Connect using SSH to a Linux virtual machine using Azure Bastion].

Once this is done, you can download a `kubeconfig` file and operate the cluster without having to go through the bastion host.

===== Caveats of {azure} Bastion:

* As of June 2020, the link:https://docs.microsoft.com/en-us/azure/bastion/bastion-overview#regions[Azure Bastion service] is not available in all {azure} regions.
* By design, it's not possible to leverage the bastion host without using the SSH session embedded into the browser.
This makes it impossible to use tools like `sftp` or `scp`.
* You have to rely on copy and paste to share data (like the `admin.conf` file generated by skuba) between the remote nodes and your local system.
You can "rely" on `cat`, `base64` and a lot of copy and paste.
* `skuba` requires a private SSH key to connect to all the nodes of the cluster.
You have to upload the private key you specified at cluster creation or create a new one inside of the first master node and copy that around the cluster.

=== Virtual Network Peering Support

It is possible to join existing networks to the cluster. It can be set up by adding a list of network IDs to `peer_virtual_network_id`.

=== Enable Multiple Zones

It is possible to enable multiple zones.  It can be set `enable_zone` to `true` and master/worker nodes will be distributed sequentially based on zones defined in `azure_availability_zones`.

* As of June 2020, the link:https://docs.microsoft.com/en-us/azure/availability-zones/az-region[Azure Availability Zones] is not available in all Azure regions.

=== Deploying the Infrastructure

On the management machine, find the {tf} template files for AZURE in
`/usr/share/caasp/terraform/azure`. These files have been installed as part of
the management pattern (`sudo zypper in -t pattern SUSE-CaaSP-Management`).

Copy this folder to a location of your choice as the files need adjustment.

----
mkdir -p ~/caasp/deployment/
cp -r /usr/share/caasp/terraform/azure/ ~/caasp/deployment/
cd ~/caasp/deployment/azure/
----

Once the files are copied, rename the `terraform.tfvars.example` file to
`terraform.tfvars`:

----
cp terraform.tfvars.example terraform.tfvars
----

Edit the `terraform.tfvars` file and add/modify the following variables:

include::deployment-terraform-example.adoc[tags=tf_azure]

[TIP]
====
You can set the timezone and other parameters before deploying the nodes
by modifying the cloud-init template:

* `~/caasp/deployment/azure/cloud-init/cloud-init.yaml.tpl`
====

You can enter the registration code for your nodes in
`~/caasp/deployment/azure/registration.auto.tfvars` instead of the
`terraform.tfvars` file.

Substitute `CAASP_REGISTRATION_CODE` for the code from <<registration-code>>.

[source,json]
----
# SUSE CaaSP Product Key
caasp_registry_code = "<CAASP_REGISTRATION_CODE>"
----

This is required so all the deployed nodes can automatically register
with {scc} and retrieve packages.

You can also enable Cloud Provider Integration with Azure.
+
----
# Enable CPI integration with Azure
cpi_enable = true
----

Now you can deploy the nodes by running:

----
terraform init
terraform plan
terraform apply
----

Check the output for the actions to be taken. Type "yes" and confirm with
kbd:[Enter] when ready.
{tf} will now provision all the cluster infrastructure.

.Note Down IP/FQDN For the Nodes
[IMPORTANT]
====
The IP addresses and FQDN of the generated machines will be displayed in the
{tf} output during the cluster node deployment. You need these information
later to deploy {productname}.

These information can be obtained at any time by executing the
`terraform output` command within the directory from which you executed
{tf}.
====

=== Logging into the Cluster Nodes

Connecting to the cluster nodes can be accomplished only via SSH key-based
authentication thanks to the ssh-public key injection done earlier via
`cloud-init`. You can use the predefined `sles` user to log in.

If the ssh-agent is running in the background, run:

----
ssh sles@<node-ip-address>
----

Without the ssh-agent running, run:

----
ssh sles@<node-ip-address> -i <path-to-your-ssh-private-key>
----

Once connected, you can execute commands using password-less `sudo`.
