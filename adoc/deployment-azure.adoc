== Deployment on Microsoft Azure

Deployment on Microsoft Azure is currently a tech preview.

.Preparation Required
[NOTE]
====
You must have completed <<deployment.preparations>> to proceed.
====

You will use {tf} to deploy the whole infrastructure described in
<<architecture-azure>>. Then you will use the `skuba` tool to bootstrap the
{kube} cluster on top of it.


[[architecture-azure]]
=== AZURE Deployment

The AZURE deployment created by our {tf} template files leads to the
creation of the infrastructure described in the next paragraphs.

==== Setup service principal for terraform login credential

Following the guide link:https://www.terraform.io/docs/providers/azurerm/guides/service_principal_client_secret.html#creating-a-service-principal-in-the-azure-portal[Creating a Service Principal in the Azure Portal], and set up `ARM_CLIENT_ID`, `ARM_CLIENT_SECRET`, `ARM_SUBSCRIPTION_ID`, `and ARM_TENANT_ID` in `container-openrc.sh`.  

[source,bash]
----
export ARM_CLIENT_ID=00000000-0000-0000-0000-000000000000
export ARM_CLIENT_SECRET=000000000000000000000000000000000
export ARM_SUBSCRIPTION_ID=00000000-0000-0000-0000-000000000000
export ARM_TENANT_ID=00000000-0000-0000-0000-000000000000
----

And sourced before deploying terraform script.:

[source,bash]
----
source container-openrc.sh
----

==== Network

All of the infrastructure is created inside of a user specified AZURE region.
The resources are currently all located inside of the user specified availability
zones. All the nodes are placed inside of the same virtual network, within the same
subnet.

Worker nodes are never exposed to the public internet. On the opposite
each master node has a public IP address by default. This allows users to
connect to them via ssh from their computers.

It's also possible to disable this behavior and make **all** the nodes private.
This can be done setting the `create_bastionhost` variable to `true`.

When this variable is set all the master nodes cease to have a public IP address.
An link:https://docs.microsoft.com/en-us/azure/bastion/bastion-overview[Azure Bastion]
instance is created which becomes the only way to ssh into the cluster.

==== Load Balancer

The {tf} template files take care of creating a
link:https://azure.amazon.com/elasticloadbalancing/[Classic Load Balancer]
which exposes the Kubernetes API service deployed on the control plane
nodes.

The load balancer exposes the following ports:

* `6443`: Kubernetes API server
* `32000`: Dex (OIDC Connect)
* `32001`: Gangway (RBAC Authenticate)

==== Accessing the nodes

A default `sles` user is created on each node of the cluster. The user has
administrator capabilities by using the `sudo` utility.

By default password based authentication is disabled. It's possible to log
using the ssh key specified via the `admin_ssh_key` variable.
It's also possible to enable password based authentication by specifying a
value for the `admin_password` variable. Note well: Azure has some security
checks in place to avoid the usage of weak passwords.

When the bastion host creation is disabled the access to the master nodes of
the cluster is just a matter of doing ssh against their public IP address.

Accessing a cluster through an Azure Bastion requires a different procedure.

==== Using Azure Bastion

Azure bastion supports only RSA keys with PEM format. These can be created by
doing:

```
ssh-keygen -f azure -t rsa -b 4096 -m pem
```

This will create a public and private key named `azure`. The **private** key has
to be provided later to Azure, hence it's strongly recommended to create
a dedicated key pair.

Once the whole infrastructure is created you can connect into any node of the
cluster by doing the following steps:

  * Log into Azure portal
  * Choose one of the nodes of the cluster
  * Click "connect" and select "bastion" as option
  * Enter all the required fields

Once this is done a new browser tab will be open with a shell session running
inside of the desired node.

It's recommended to use Chrome or Chromium during this process.

You can ssh into the first master node of the cluster of drive the creation of
a SUSE cluster from there using this guide link:https://docs.microsoft.com/en-us/azure/bastion/bastion-connect-vm-ssh[Connect using SSH to a Linux virtual machine using Azure Bastion]. Once this is done you can download a kubeconfig
file and operate the cluster without having to go through the bastion host.

Caveats of Azure Bastion:

  * As of June 2020, the link:https://docs.microsoft.com/en-us/azure/bastion/bastion-overview#regions[Azure Bastion service] is not available in all Azure regions.
  * By design it's not possible to leverage the bastion host without using the
    ssh session embedded into the browser. This makes it impossible to use tools like
    `sftp` or `scp`.
  * You have to rely on copy and paste to share data (like the `admin.conf` file
    generated by skuba) between the remote nodes and your local system.
    You can "rely" on `cat`, `base64` and a lot of copy and paste...
  * `skuba` requires a private ssh key to connect to all the nodes of the cluster.
    You have to upload the private key you specified at cluster creation
    or create a new one inside of the first master node and copy that
    around the cluster.

=== Virtual Network Peering Support

It is possible to join existing networks to the cluster.  It can be setup by adding a list of network IDs to `peer_virutal_network_id`.

=== Enable Multiple Zones

It is possible to enable multiple zones.  It can be set `enable_zone` to `true` and master/worker nodes will be distributed sequentially based on zones defined in `azure_availability_zones`. 

  * As of June 2020, the link:https://docs.microsoft.com/en-us/azure/availability-zones/az-region[Azure Availability Zones] is not available in all Azure regions.

=== Deploying the Infrastructure

On the management machine, find the {tf} template files for AZURE in
`/usr/share/caasp/terraform/azure`. These files have been installed as part of
the management pattern (`sudo zypper in -t pattern SUSE-CaaSP-Management`).

Copy this folder to a location of your choice as the files need adjustment.

----
mkdir -p ~/caasp/deployment/
cp -r /usr/share/caasp/terraform/azure/ ~/caasp/deployment/
cd ~/caasp/deployment/azure/
----

Once the files are copied, rename the `terraform.tfvars.example` file to
`terraform.tfvars`:

----
cp terraform.tfvars.example terraform.tfvars
----

Edit the `terraform.tfvars` file and add/modify the following variables:

include::deployment-terraform-example.adoc[tags=tf_azure]

[TIP]
====
You can set the timezone and other parameters before deploying the nodes
by modifying the cloud-init template:

* `~/caasp/deployment/azure/cloud-init/cloud-init.yaml.tpl`
====

You can enter the registration code for your nodes in
`~/caasp/deployment/azure/registration.auto.tfvars` instead of the
`terraform.tfvars` file.

Substitute `CAASP_REGISTRATION_CODE` for the code from <<registration_code>>.

[source,json]
----
# SUSE CaaSP Product Key
caasp_registry_code = "<CAASP_REGISTRATION_CODE>"
----

This is required so all the deployed nodes can automatically register
with {scc} and retrieve packages.

Now you can deploy the nodes by running:

----
terraform init
terraform plan
terraform apply
----

Check the output for the actions to be taken. Type "yes" and confirm with
kbd:[Enter] when ready.
{tf} will now provision all the cluster infrastructure.

.Note Down IP/FQDN For the Nodes
[IMPORTANT]
====
The IP addresses and FQDN of the generated machines will be displayed in the
{tf} output during the cluster node deployment. You need these information
later to deploy {productname}.

These information can be obtained at any time by executing the
`terraform output` command within the directory from which you executed
{tf}.
====

=== Logging into the Cluster Nodes

Connecting to the cluster nodes can be accomplished only via SSH key-based
authentication thanks to the ssh-public key injection done earlier via
`cloud-init`. You can use the predefined `sles` user to log in.

If the ssh-agent is running in the background, run:

----
ssh sles@<node-ip-address>
----

Without the ssh-agent running, run:

----
ssh sles@<node-ip-address> -i <path-to-your-ssh-private-key>
----

Once connected, you can execute commands using password-less `sudo`.
