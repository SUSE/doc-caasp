== Deployment on VMware

.Preparation Required
[NOTE]
You must have completed <<deployment-preparations>> to proceed.

=== Environment Description

[NOTE]
====
These instructions are based on `VMware ESXi {vmware_version}`.
====

[IMPORTANT]
====
These instructions currently do not describe how to set up a load balancer in a {vmware} environment.

This will be added in future versions. You must provide your own load balancing
solution that directs access to the master nodes.
====

[IMPORTANT]
====
VMware vSPhere doesn't offer a load-balancer solution. Please expose port `6443`
for the {kube} api-servers on the master nodes on a local load balancer using
round-robin 1:1 port forwarding.
====

=== Choose a Setup Type

You can deploy {productname} onto existing VMware instances using {ay} or
create a VMware template that will also create the instances.
You must choose one method to deploy the entire cluster!

Please follow the instructions for you chosen method below and ignore the instructions
for the other method.

=== Setup with {ay}

[NOTE]
====
If you choose the {ay} method, please ignore all the following steps for the
VMware template creation.
====

For each VM deployment, follow the {ay} installation method used for deployment on
bare metal machines as described in <<deployment_bare_metal>>.

=== Setup Using the VMware Template

[#choose-disk-format-template]
==== Choose a Disk Format for the Template

Before creating the template, it is important to select the right format of the root hard disk for the node. This format is then used by default when creating new instances with Terraform.

For the majority of cases, we recommend using *Thick Provision Lazy Zeroed*. This format is quick to create, provides good performance, and avoids the risk of running out of disk space due to over-provisioning.

[NOTE]
====
It is not possible to resize a disk when using *Thick Provision Eager Zeroed*.
For this reason, the Terraform variables `master_disk_size` and `worker_disk_size` must be set to the exact same size as in the original template.
====

link:https://pubs.vmware.com/vsphere-51/index.jsp?topic=%2Fcom.vmware.vsphere.storage.doc%2FGUID-4C0F4D73-82F2-4B81-8AA7-1DD752A8A5AC.html[Official VMware documentation] describes these formats as follows:

Thick Provision Lazy Zeroed::
Creates a virtual disk in a default thick format. Space required for the virtual disk is allocated when the disk is created. Data remaining on the physical device is not erased during creation, but is zeroed out on demand later on first write from the virtual machine. Virtual machines do not read stale data from the physical device.

Thick Provision Eager Zeroed::
A type of thick virtual disk that supports clustering features such as Fault Tolerance. Space required for the virtual disk is allocated at creation time. In contrast to the thick provision lazy zeroed format, the data remaining on the physical device is zeroed out when the virtual disk is created. It might take longer to create virtual disks in this format than to create other types of disks. Increasing the size of an Eager Zeroed Thick virtual disk causes a significant stun time for the virtual machine.

Thin Provision::
Use this format to save storage space. For the thin disk, you provision as much datastore space as the disk would require based on the value that you enter for the virtual disk size. However, the thin disk starts small and at first, uses only as much datastore space as the disk needs for its initial operations. If the thin disk needs more space later, it can grow to its maximum capacity and occupy the entire datastore space provisioned to it.

[IMPORTANT]
.Select the Disk Format Thoughtfully
====
It is not possible to change the format in Terraform later. Once you have selected one format, you can only create instances with that format and it is not possible to switch.
====


==== VM Preparation for Creating a Template

. Upload the ISO image {isofile} to the desired VMware datastore.

Now you can create a new base VM for {productname} within the designated resource
pool through the vSphere WebUI:

. Create a "New Virtual Machine".
. Define a name for the virtual machine (VM).
+
image::vmware_step1.png[width=80%,pdfwidth=80%]
. Select the folder where the VM will be stored.
. Select a `Compute Resource` that will run the VM.
+
image::vmware_step2.png[width=80%,pdfwidth=80%]
. Select the storage to be used by the VM.
+
image::vmware_step3.png[width=80%,pdfwidth=80%]
. Select `ESXi 6.7 and later` from compatibility.
+
image::vmware_step4.png[width=80%,pdfwidth=80%]
. Select menu:Guest OS Family[Linux] and menu:Guest OS Version[SUSE Linux Enterprise 15 (64-bit)].
+
*Note*: You will manually select the correct installation medium in the next step.
+
image::vmware_step5.png[width=80%,pdfwidth=80%]
. Now customize the hardware settings.
+
image::vmware_step6.png[width=80%,pdfwidth=80%]
.. Select menu:CPU[2].
.. Select menu:Memory[4096 MB].
.. Select menu:New Hard disk[40 GB], menu:New Hard disk[Disk Provisioning] > See <<choose-disk-format-template>> to select the appropriate disk format. For *Thick Provision Eager Zeroed*, use this value for Terraform variables `master_disk_size` and `worker_disk_size`
.. Select menu:New SCSI Controller[LSI Logic Parallel SCSI controller (default)] and change it to "VMware Paravirtualized".
.. Select menu:New Network[VM Network], menu:New Network[Adapter Type > VMXNET3].
+
("VM Network" sets up a bridged network which provides a public IP address reachable within a company.)
.. Select menu:New CD/DVD[Datastore ISO File].
.. Check the box menu:New CD/DVD[Connect At Power On] to be able boot from ISO/DVD.
.. Then click on "Browse" next to the `CD/DVD Media` field to select the downloaded ISO image on the desired datastore.
.. Go to the VM Options tab.
+
image::vmware_step6b.png[width=80%,pdfwidth=80%]
.. Select menu:Boot Options[].
.. Select menu:Firmware[BIOS].
.. Confirm the process with menu:Next[].

===== {sls} Installation

Power on the newly created VM and install the system over graphical remote console:

. Enter registration code for {sle} in {yast}.
. Confirm the update repositories prompt with "Yes".
. Remove the check mark in the "Hide Development Versions" box.
. Make sure the following modules are selected on the "Extension and Module Selection" screen:
+
image::vmware_extension.png[width=80%,pdfwidth=80%]
** SUSE CaaS Platform 4.0 x86_64
+
[NOTE]
====
Due to a naming convention conflict, all Versions of 4.x will be released in the 4.0 Module.
====
** Basesystem Module
** Containers Module (this will automatically be checked when you select {productname})
** Public Cloud Module
. Enter the registration code to unlock the {productname} extension.
. Select menu:System Role[Minimal] on the "System Role" screen.
. Click on "Expert Partitioner" to redesign the default partition layout.
. Select "Start with current proposal".
+
image::vmware_step8.png[width=80%,pdfwidth=80%]
.. Keep `sda1` as BIOS partition.
.. Remove the root `/` partition.
+
Select the device in "System View" on the left (default: `/dev/sda2`) and click "Delete". Confirm with "Yes".
+
image::vmware_step9.png[width=80%,pdfwidth=80%]
.. Remove the `/home` partition.
.. Remove the `swap` partition.
. Select the `/dev/sda/` device in "System View" and then click menu:Partitions[Add Partition].
+
image::vmware_step10.png[width=80%,pdfwidth=80%]
. Accept the default maximum size (remaining size of the hard disk defined earlier without the boot partition).
+
image::vmware_step11.png[width=80%,pdfwidth=80%]
.. Confirm with "Next".
.. Select menu:Role[Operating System]
+
image::vmware_step12.png[width=80%,pdfwidth=80%]
.. Confirm with "Next".
.. Accept the default settings.
+
image::vmware_step13.png[width=80%,pdfwidth=80%]
*** Filesystem: BtrFS
*** Enable Snapshots
*** Mount Device
*** Mount Point `/`
. You should be left with two partitions. Now click "Accept".
+
image::vmware_step7.png[width=80%,pdfwidth=80%]
. Confirm the partitioning changes.
+
image::vmware_step14.png[width=80%,pdfwidth=80%]
. Click "Next".
. Configure your timezone and click "Next".
. Create a user with the username `sles` and specify a password.
.. Check the box menu:Local User[Use this password for system administrator].
+
image::vmware_step15.png[width=80%,pdfwidth=80%]
. Click "Next".
. On the "Installation Settings" screen:
.. In the "Security" section:
... Disable the Firewall (click on `(disable)`).
... Enable the SSH service (click on `(enable)`).
.. Scroll to the `kdump` section of the software description and click on the title.
. In the "Kdump Start-Up" screen, select menu:Enable/Disable Kdump[Disable Kdump].
+
[NOTE]
====
`Kdump` needs to be disabled because it defines a certain memory limit. If you later
wish to deploy the template on a machine with different memory allocation (e.g. template created for 4GB, new machine has 2GB),
the results of Kdump will be useless.

You can always configure `Kdump` on the machine after deploying from the template.

Refer to: link:https://documentation.suse.com/sles/15-SP1/single-html/SLES-tuning/#cha-tuning-kdump-basic[{sls} 15 SP1 System Analysis and Tuning Guide: Basic Kdump Configuration]
====
.. Confirm with "OK".
+
image::vmware_step16.png[width=80%,pdfwidth=80%]
. Click "Install". Confirm the installation by clicking "Install" in the pop-up dialog.
. Finish the installation and confirm system reboot with "OK".
+
image::vmware_step17.png[width=80%,pdfwidth=80%]

==== Preparation of the VM as a Template

In order to run {productname} on the created VMs, you must configure and install some additional packages
like `sudo`, `cloud-init` and `open-vm-tools`.

.Activate extensions during {sle} installation with {yast}
[TIP]
Steps 1-4 may be skipped, if they were already performed in {yast} during the {sle} installation.

. Register the SLES15-SP1 system. Substitute `<CAASP_REGISTRATION_CODE>` for the code from <<registration_code>>.
+
----
SUSEConnect -r CAASP_REGISTRATION_CODE
----
. Register the `Containers` module (free of charge):
+
----
SUSEConnect -p sle-module-containers/15.1/x86_64
----
. Register the `Public Cloud` module for basic `cloud-init` package (free of charge):
+
----
SUSEConnect -p sle-module-public-cloud/15.1/x86_64
----
. Register the {productname} module. Substitute `<CAASP_REGISTRATION_CODE>` for the code from <<registration_code>>.
+
----
SUSEConnect -p caasp/4.0/x86_64 -r CAASP_REGISTRATION_CODE
----
. Install required packages. As root, run:
+
----
zypper in sudo cloud-init cloud-init-vmware-guestinfo open-vm-tools
----
. Enable the installed `cloud-init` services. As root, run:
+
----
systemctl enable cloud-init cloud-init-local cloud-config cloud-final
----

. Deregister from `scc`:
+
----
SUSEConnect -d; SUSEConnect --cleanup
----

. Do a cleanup of the SLE image for converting into a VMware template. As root, run:
+
----
rm /etc/machine-id /var/lib/zypp/AnonymousUniqueId \
/var/lib/systemd/random-seed /var/lib/dbus/machine-id \
/var/lib/wicked/*
----
. Clean up btrfs snapshots and create one with initial state:
+
----
snapper list
snapper delete <list_of_nums_of_unneeded_snapshots>
snapper create -d "Initial snapshot for caasp template" -t single
----
. Power down the VM. As root, run:
+
----
shutdown -h now
----

==== Creating the VMware Template

Now you can convert the VM into a template in VMware (or repeat this action block for each VM).

. In the vSphere WebUI, right-click on the VM and select menu:Template[Convert to Template].
Name it reasonably so you can later identify the template. The template will be created.

==== Deploying VMs from the Template

===== Using Terraform

. Find the {tf} template files for {vmware} in `/usr/share/caasp/terraform/vmware` which was installed as part of the management
pattern (`sudo zypper in patterns-caasp-Management`).
Copy this folder to a location of your choice; as the files need to be adjusted.
+
----
mkdir -p ~/caasp/deployment/
cp -r /usr/share/caasp/terraform/vmware/ ~/caasp/deployment/
cd ~/caasp/deployment/vmware/
----
. Once the files are copied, rename the `terraform.tfvars.example` file to
`terraform.tfvars`:
+
----
mv terraform.tfvars.example terraform.tfvars
----
. Edit the `terraform.tfvars` file and add/modify the following variables:

include::deployment-terraform-example.adoc[tag=tf_vmware,leveloffset=+3]

[start=4]
. Enter the registration code for your nodes in `~/caasp/deployment/vmware/registration.auto.tfvars`:
+
Substitute `<CAASP_REGISTRATION_CODE>` for the code from <<registration_code>>.
+
[source,json]
----
# SUSE CaaSP Product Product Key
caasp_registry_code = "CAASP_REGISTRATION_CODE"
----
+
This is required so all the deployed nodes can automatically register with {scc} and retrieve packages.

. You can also enable Cloud Provider Integration with vSphere.
+
----
# Enable CPI integration with vSphere
cpi_enable = true
----

. You can also disable node hostnames set from DHCP server. It is recommended when you enabled CPI integration with vSphere. This sets ~vSphere~ virtual machine's hostname with a naming convention (`"<stack_name>-master-<index>"` or `"<stack_name>-worker-<index>"`). This can be used as node name when using `skuba` command to bootstrapping or joining nodes.
+
[IMPORTANT]
====
It is mandatory that each virtual machine's hostname must match its cluster node name.
====
+
----
# Set node's hostname from DHCP server
hostname_from_dhcp = false
----
+
Once the files are adjusted, `terraform` needs to know about the `vSphere` server
and the login details for it; these can be exported as environment variables or
entered every time `terraform` is invoked.
+
. Additionally, the `ssh-key` that is specified in the `tfvars` file must be added
to the key agent so the machine running `skuba` can `ssh` into the machines:
+
----
export VSPHERE_SERVER="<server_address"
export VSPHERE_USER="<username>"
export VSPHERE_PASSWORD="<password>"
export VSPHERE_ALLOW_UNVERIFIED_SSL=true # In case you are using custom certificate for accessing vsphere API

ssh-add <path_to_private_ssh_key_from_tfvars>
----
+
.Specify a key expiration time
[WARNING]
====
The ssh key is decrypted when loaded into the key agent.  Though the key itself is not
accesible, anyone with access to the agent's control socket file can use the private key
contents to impersonate the key owner.  By default, socket access is limited to the user
who launched the agent.  None the less, it is still good security practice to specify an
expiration time for the decrypted key using the `-t` option.

For example: `ssh-add -t 1h30m $HOME/.ssh/id.ecdsa` would expire the decrypted key in 1.5
hours.
See `man ssh-agent` and `man ssh-add` for more information.
====
+
. Run {tf} to create the required machines for use with `skuba`:
+
----
terraform init
terraform plan
terraform apply
----

===== Setup by Hand

[NOTE]
====
Full instructions for the manual setup and configuration are currently not in scope of this document.
====

Deploy the template to your created VMs. After that, boot into the node and configure
the OS as needed.

. Power on the newly created VMs
. Generate new machine IDs on each node
. You need to know the FQDN/IP for each of the created VMs during the bootstrap process
. Continue with bootstrapping/joining of nodes

[TIP]
====
To manually generate the unique `machine-id` please refer to: <<machine-id-regen>>.
====

=== Container Runtime Proxy

[IMPORTANT]
====
{crio} proxy settings must be adjusted on all nodes before joining the cluster!
====

In some environments you must configure the container runtime to access the container registries through a proxy.
In this case, please refer to: {docurl}single-html/caasp-admin/#_configuring_httphttps_proxy_for_cri_o[{productname} Admin Guide: Configuring HTTP/HTTPS Proxy for CRI-O]
