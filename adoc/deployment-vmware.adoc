== Deployment on VMWare

.Preparation Required
[NOTE]
You must have completed <<deployment.preparations>> to proceed.

=== Environment description

[NOTE]
====
These instructions are based on `VMware ESXi {vmware_version}`.
====

[IMPORTANT]
====
These instructions currently do not describe how to set up a load balancer.
This will be added in future versions. You must provide your own load balancing
solution that directs access to the master nodes.
====

[IMPORTANT]
====
VMWare vSPhere doesn't offer a load-balancer solution. Please expose port `6443`
for the {kube} api-servers on the master nodes on a local load balancer using
round-robin 1:1 port forwarding.
====

=== VM preparation for creating a template

. Temporarily enable SSH access to one of the ESXi nodes in the vSphere WebUI
as described at link:https://kb.vmware.com/s/article/2004746[]
. Log in to the node.
Replace `vmware.esxi.node` with the hostname of your SSH enabled node.
+
----
ssh root@vmware.esxi.node
----
. Change to a data volume and create a new directory to hold the installation media
+
----
cd /vmfs/volumes/my-directory/
----
. Download the pre-built image of SUSE SLES15-SP1 for VMWare from {jeos_product_page_url}

Now you can create a new base VM for {productname} within the designated resource
pool through the vSphere WebUI:

. Create a "New Virtual Machine".
. Define a name for the virtual machine (VM).
+
image::vmware_step1.png[width=80%,pdfwidth=80%]
. Select the folder where the VM will be stored.
. Select a `Compute Resource` that will run the VM.
+
image::vmware_step2.png[width=80%,pdfwidth=80%]
. Select the storage used by the VM.
+
image::vmware_step3.png[width=80%,pdfwidth=80%]
. Select `ESXi 6.7 and later` from compatibility.
+
image::vmware_step4.png[width=80%,pdfwidth=80%]
. Select menu:Guest OS Family[Linux] and menu:Guest OS Version[SUSE Linux Enterprise 15 (64 Bit)].
+
*Note*: You will manually select the correct installation media in the next step.
+
image::vmware_step5.png[width=80%,pdfwidth=80%]
. Now customize the hardware settings.
+
image::vmware_step6.png[width=80%,pdfwidth=80%]
.. Select menu:CPU[2].
.. Select menu:Memory[4096 MB].
.. Select menu:New Hard disk[40GB].
.. Select menu:New SCSI Controller[LSI Logic Parallel SCSI controller (default)] and change it to "VMware Paravirtualized".
.. Select menu:New Network[VM Network], menu:New Network[Adapter Type > VMXNET3].
+
("VM Network" sets up a bridged network which provides a public IP address reachable within a company)
.. Select menu:New CD/DVD[Datastore ISO File].
.. Tick the box menu:New CD/DVD[Connect At Power On] to be able boot from ISO/DVD.
.. The click on "Browse" next to the `CD/DVD Media` field to select the downloaded ISO image on desired datastore.
.. Go to tab VM Options.
+
image::vmware_step6b.png[width=80%,pdfwidth=80%]
.. Select menu:Boot Options[].
.. Select menu:Firmware[Bios].
.. Confirm the process with menu:Next[].

==== {sls} installation

[NOTE]
====
Use {ay} and ensure to use a staged frozen patchlevel via SMT/SUSE Manager to ensure a 100% reproducible setup.
For the moment (until an {ay} template is available) follow the procedure below.
====

Power on the newly created VM and install the system over graphical remote console:

. Enter registration code for SLES in YaST.
. Confirm the update repositories prompt with "Yes".
. Remove the check mark in the "Hide Development Versions" box.
.
. Make sure the following modules are selected on the "Extension and Module Selection" screen:
+
image::vmware_extension.png[width=80%,pdfwidth=80%]
** SUSE CaaS Platform 4.0 x86_64 (BETA)
** Basesystem Module
** Containers Module (this will automatically be ticked when you select {productname})
** Public Cloud Module
. Enter the registration code to unlock the {productname} extension.
. Select menu:System Role[Minimal] on the "System Role" screen.
. Click on "Expert Partitioner" to redesign the default partition layout.
. Select "Start with current proposal".
+
image::vmware_step8.png[width=80%,pdfwidth=80%]
.. Keep `sda1` as BIOS partition
.. Remove the root `/` partition.
+
Select the device in "System View" on the left (Default: `/dev/sda2`) and click "Delete". Confirm with "Yes".
+
image::vmware_step9.png[width=80%,pdfwidth=80%]
.. Remove the `/home` partition.
.. Remove the `swap` partition.
. Select the `/dev/sda/` device in "System View" and then click menu:Partitions[Add Partition].
+
image::vmware_step10.png[width=80%,pdfwidth=80%]
. Accept the default maximum size (remaining size of the hard disk defined earlier without the boot partition).
+
image::vmware_step11.png[width=80%,pdfwidth=80%]
.. Confirm with "Next".
.. Select menu:Role[Operating System]
+
image::vmware_step12.png[width=80%,pdfwidth=80%]
.. Confirm with "Next".
.. Accept the default settings.
+
image::vmware_step13.png[width=80%,pdfwidth=80%]
*** Filesystem: BtrFS
*** Enable Snapshots
*** Mount Device
*** Mount Point `/`
. You should be left with 2 partitions. Now click "Accept".
+
image::vmware_step7.png[width=80%,pdfwidth=80%]
. Confirm the partitioning changes.
+
image::vmware_step14.png[width=80%,pdfwidth=80%]
. Click "Next".
. Configure your timezone and click "Next".
. Create a user with the Username `sles` and specify a password.
.. Tick the box menu:Local User[Use this password for system administrator].
+
image::vmware_step15.png[width=80%,pdfwidth=80%]
. Click "Next".
. On the "Installation Settings" screen:
.. In the "Network Configuration" section:
... Disable the Firewall (click on `(disable)`).
... Enable the SSH service (clock on `(enable)`).
.. Scroll to the `kdump` section of the software description and click on the title.
. In the "Kdump Start-Up" screen select menu:Enable/Disable Kdump[Disable Kdump].
.. Confirm with "OK".
+
image::vmware_step16.png[width=80%,pdfwidth=80%]
. Click "Install". Confirm the installation by clicking "Install" in the popup dialog.
. Finish the installation and confirm system reboot with "OK".
+
image::vmware_step17.png[width=80%,pdfwidth=80%]

==== Preparation of the VM as a template

In order to run {productname} on the created VMs, you must configure and install some additional packages
like `sudo`, `VMWare tools` and adding the {kube} dependencies.

.Activate extensions during {sle} installation with YaST
[TIP]
Steps 1-3 may be skipped, if they were already performed in YaST during the {sle} installation.

. Register the SLES15-SP1 system. Substitute `CAASP_REGISTRATION_CODE` for the code from <<registration_code>>.
+
----
SUSEConnect -r CAASP_REGISTRATION_CODE
----
. Register the `Containers` module (free of charge)
+
----
SUSEConnect -p sle-module-containers/15.1/x86_64
----
. Register the {productname} module. Substitute `CAASP_REGISTRATION_CODE` for the code from <<registration_code>>.
+
----
SUSEConnect -p caasp/4.0/x86_64 -r CAASP_REGISTRATION_CODE
----
// . Install `open-vm-tools`
// +
// ----
// zypper in open-vm-tools # in my case it was installed and enabled automatically during sles installation
// systemctl status vmtoolsd; systemctl status vgauthd.service
// ----
. Copy your public SSH key for the user used for `skuba` bootstrap (normally `sles`) which will be used for {productname} deployment:
+
----
ssh-copy-id -i ~/.ssh/id_rsa.pub sles@vm-template.host
----
. Install required packages. As root, run:
+
----
zypper in -t pattern SUSE-CaaSP-Node
----
. Configure `sudo` for the user to be able authenticate without password. Replace `USERNAME` with the user you created during installation. As root, run:
+
----
echo "USERNAME ALL=(ALL) NOPASSWD: ALL" >> /etc/sudoers
----

. Enable the installed `cloud-init` services. As root, run:
+
----
systemctl enable cloud-init cloud-init-local cloud-config cloud-final
----

. Deregister from `scc`:
+
----
SUSEConnect -d
----

. Do a cleanup of the SLE image for converting into a VMWare template. As root, run:
+
----
rm /etc/machine-id /var/lib/zypp/AnonymousUniqueId \
/var/lib/systemd/random-seed /var/lib/dbus/machine-id \
/var/lib/wicked/*
----
. Cleanup btrfs snapshots and create one with initial state:
+
----
snapper list
snapper delete <list_of_nums_of_unneeded_snapshots>
snapper create -d "Initial snapshot for caasp template" -t single
----
. Power down the VM. As root, run:
+
----
shutdown -h now
----

==== Creating the VMWare Template

Now you can convert the VM into a template in VMware (or repeat this action block for each VM).

. In the vSphere WebUI, right-click on the VM and select menu:Template[Convert to Template].
Name it reasonably so you can later identify the template. The template will be created.

=== Deploying VMs from the Template

==== Terraform

. Find the {tf} template files for {soc} in `/usr/share/caasp/terraform/vmware` (which was installed as part of the management
pattern (`sudo zypper in -t pattern SUSE-CaaSP-Management`)).
Copy this folder to a location of your choice as the files need adjustment.
+
----
mkdir -p ~/caasp/deployment/
cp -r /usr/share/caasp/terraform/vmware/ ~/caasp/deployment/
cd ~/caasp/deployment/vmware/
----
. Once the files are copied, rename the `terraform.tfvars.example` file to
`terraform.tfvars`:
+
----
mv terraform.tfvars.example terraform.tfvars
----
. Edit the `terraform.tfvars` file and add modify the following variables:
+
include::deployment-terraform-example.adoc[tag=tf_vmware]
. Enter the registration code for your nodes in `~/my-cluster/registration.auto.tfvars`:
+
Substitute `CAASP_REGISTRATION_CODE` for the code from <<registration_code>>.
+
[source,json]
----
# SUSE CaaSP Product Registration Code
caasp_registry_code = "CAASP_REGISTRATION_CODE"
----
+
This is required so all the deployed nodes can automatically register to {scc} and retrieve packages.

Once the files are adjusted, `terraform` needs to know about the `vSphere` server
and the login details for it; these can be exported as environment variables or
entered every time `terraform` is invoked.

Additionally, the `ssh-key` that is specified in the `tfvars` file must be added
to the keyring, so the machine running `skuba` can `ssh` into the machines:

----
export VSPHERE_SERVER="<server_address"
export VSPHERE_USER="<username>"
export VSPHERE_PASSWORD="<password>"

ssh-add <path_to_private_ssh_key_from_tfvars>
----

Run terraform to create the required machines for use with `skuba`:

----
terraform init
terraform plan
terraform apply
----

.Skip self-signed certificate verification
[TIP]
In case you are using a self-signed certificate you need to skip its verification, as Terraform vsphere provider is not supporting it, thus `terraform plan` will fail. To do this, type the following in your terminal session: `export VSPHERE_ALLOW_UNVERIFIED_SSL=true`

==== Setup by hand

. Right-click on the template and select menu:New VM from This Template...[] and name it something like `caasp4-master-0`.
.. Select storage `datastore` and the designated resource pool.

Repeat these steps until you have created the desired number of machines comprising your cluster.

[IMPORTANT]
====
Make sure to give each VM a clear name that shows it's purpose in the cluster e.g.

* `caasp-worker-0`
* `caasp-worker-1`
* `caasp-master-0`
* `caasp-master-1`

You will need these names during bootstrapping of the cluster.
====
. Power on the newly created VMs.
. You need to know the FQDN/IP for each of the created VMs during the bootstrap process.
. Once the VMs booted up, log in via SSH and run the following command to regenerate the Machine ID `/etc/machine-id`:
+
----
sudo dbus-uuidgen --ensure
sudo systemd-machine-id-setup
----

=== Installing {productname} packages

Install {productname} related packages to each node. You can alternatively pre-install them during the creation of the template but
then you are limited to installing old versions of the packages with the template and updating manually afterwards.

.Use terminal multiplexing
[TIP]
====
These steps have to be performed on each VM that you just created; thus it can be time consuming.
Use a terminal with multiplex support or something like link:https://parallel-ssh.org/[Parallel SSH (pssh)]
to perform commands or control SSH sessions across multiple machines simultaneously.
====

Install the following packages on each node:

* kubernetes-kubeadm
* kubernetes-client

----
sudo zypper in kubernetes-kubeadm kubernetes-kubelet kubernetes-client cri-o cni-plugins
----
