== Deployment on SUSE OpenStack Cloud

.Preparation Required
[NOTE]
You must have completed <<deployment.preparations>> to proceed.

You will use {tf} to deploy the required master and worker cluster nodes (plus a load balancer) to {soc} and then use the
`skuba` tool to bootstrap the {kube} cluster on top of those.

. Download the SUSE OpenStack Cloud RC file.
.. Log in to SUSE OpenStack Cloud.
.. Click on your username in the upper right hand corner to reveal the dropdown menu.
.. Click on menu:Download OpenStack RC File v3[].
.. Save the file to your workstation.
.. Load the file into your shell environment.
+
----
source container-openrc.sh
----
.. Enter the password for the RC file. This should be same credentials that you use to log in to {soc}.
. Get the SLES15-SP1 image.
.. Download the pre-built image of SUSE SLES15-SP1 for {soc} from {jeos_product_page_url}.
.. Upload the image to your {soc}.

.The default user is 'sles'
[NOTE]
The SUSE SLES15-SP1 images for {soc} come with pre-defined user `sles`, which you use to log into the cluster nodes. This user has been configured for password-less 'sudo' and is the one recommended to be used by {tf} and `skuba`.

=== Deploying the cluster nodes

. Find the {tf} template files for {soc} in `/usr/share/caasp/terraform/openstack`.
Copy this folder to a location of your choice as the files need adjustment.
+
----
mkdir -p ~/caasp/deployment/
cp -r /usr/share/caasp/terraform/openstack/ ~/caasp/deployment/
cd ~/caasp/deployment/openstack/
----
. Once the files are copied, rename the `terraform.tfvars.example` file to
`terraform.tfvars`:
+
----
mv terraform.tfvars.example terraform.tfvars
----
. Edit the `terraform.tfvars` file and add modify the following variables:
+
include::deployment-terraform-example.adoc[tags=tf_openstack]
+
[TIP]
====
You can set the timezone before deploying the nodes by modifying the following file:

* `~/my-cluster/cloud-init/common.tpl`
====
. (Optional) If you absolutely need to be able to SSH into your cluster nodes using password instead of key-based authentication, this is the best time to set it globally for all of your nodes. If you do this later, you will have to do it manually. To set this, modify the cloud-init configuration and comment-out the related SSH configuration:
`~/my-cluster/cloud-init/common.tpl`
+
----
# Workaround for bsc#1138557 . Disable root and password SSH login
# - sed -i -e '/^PermitRootLogin/s/^.*$/PermitRootLogin no/' /etc/ssh/sshd_config
# - sed -i -e '/^#ChallengeResponseAuthentication/s/^.*$/ChallengeResponseAuthentication no/' /etc/ssh/sshd_config
# - sed -i -e '/^#PasswordAuthentication/s/^.*$/PasswordAuthentication no/' /etc/ssh/sshd_config
# - systemctl restart sshd
----
+
. Register your nodes by using the SUSE CaaSP Registration Code or by registering nodes against local SUSE Repository Mirroring Server in `~/my-cluster/registration.auto.tfvars`:
+
Substitute `REGISTRATION_CODE` for the code from <<registration_code>>.
+
----
## To register CaaSP product please use one of the following method
# - register against SUSE Customer Service, with SUSE CaaSP Product Registration Code
# - register against local SUSE Repository Mirroring Server

# SUSE CaaSP Product Registration Code
caasp_registry_code = "REGISTRATION_CODE"

# SUSE Repository Mirroring Server Name (FQDN)
#rmt_server_name = "rmt.example.com"
----
+
This is required so all the deployed nodes can automatically register to {scc} and retrieve packages.
+
. You can also enable Cloud Provider Integration with OpenStack in `~/my-cluster/cpi.auto.tfvars`:
+
----
# Enable CPI integration with OpenStack
cpi_enable = true

# Used to specify the name of to your custom CA file located in /etc/ssl/certs.
# Upload CUSTOM_CA_FILE to this path on nodes before joining them to your cluster.
#ca_file = "/etc/ssl/certs/<CUSTOM_CA_FILE>"
----
. Now you can deploy the nodes by running:
+
----
terraform init
terraform plan
terraform apply
----
+
Check the output for the actions to be taken. Type "yes" and confirm with Enter when ready.
Terraform will now provision all the machines and network infrastructure for the cluster.
+
.Note down IP/FQDN for nodes
[IMPORTANT]
====
The IP addresses of the generated machines will be displayed in the terraform
output during the cluster node deployment. You need these IP addresses to
deploy {productname} to the cluster.

If you need to find an IP addresses later on you can run `terraform output` within the directory you performed the deployment from `~/my-cluster` directory or perform the following steps:

. Log in to {soc} and click on menu:Network[Load Balancers]. Find the one with the string you entered in the terraform configuration above e.g. "testing-lb".
. Note down the "Floating IP". If you have configured a FQDN for this IP, use the hostname instead.
+
image::deploy-loadbalancer-ip.png[]
. Now click on menu:Compute[Instances].
. Switch the filter dropdown to `Instance Name` and enter the string you specified for `stack_name` in the `terraform.tfvars` file.
. Find the Floating IPs on each of the nodes of your cluster.
====

=== Logging into the cluster nodes

. Connecting into the cluster nodes can be accomplished only via SSH key-based authentication thanks to the ssh-public key injection done earlier via {tf}. You can use the predefined `sles` user to log in.
+
If the ssh-agent is running in the background run:
+
----
ssh sles@<node-ip-address>
----
+
Without the ssh-agent running run:
+
----
ssh sles@<node-ip-address> -i <path-to-your-ssh-private-key>
----
+
. Once connected, you can execute commands using password-less `sudo`. In addition to that, you can also set a password if you prefer to.
+
To set the *root password* run:
+
----
sudo passwd
----
+
To set the *sles user's password* run:
+
----
sudo passwd sles
----


.Password authentication has been disabled
[IMPORTANT]
====
Under the default settings you always need your SSH key to access the machines. Even after setting a password for either `root` or `sles` user, you will be unable to log in via SSH using their passwords respectively. You will most likely receive a `Permission denied (publickey)` error. This mechanism has been deliberately disabled because of security best practices. However, if this environment does not fit your workflows, you can change it at your own risk by modifying the SSH configuration:
under `/etc/ssh/sshd_config`

To allow password SSH authentication set:
----
+ PasswordAuthentication yes
----
To allow login as root via SSH set:
----
+ PermitRootLogin yes
----
For the changes to take effect you need to restart the SSH service by running
----
sudo systemctl restart sshd.service
----
====
