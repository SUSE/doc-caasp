include::entities.adoc[]

[#deployment-bare-metal]
== Deployment on Bare Metal or KVM

.Preparation Required
[NOTE]
You must have completed <<deployment-preparations>> to proceed.

[NOTE]
====
If deploying on KVM virtual machines, you may use a tool such as `virt-manager`
to configure the virtual machines and begin the {sls} {base_os_version} installation.
====

=== Environment Description

[NOTE]
====
You must have a load balancer configured as described in <<loadbalancer>>.
====

[NOTE]
====
The {ay} file found in `skuba` is a template. It has the base requirements.
This {ay} file should act as a guide and should be updated with your company's standards.
====

[NOTE]
====
To account for hardware/platform-specific setup criteria (legacy BIOS vs. (U)EFI, drive partitioning, networking, etc.),
you must adjust the {ay} file to your needs according to the requirements.

Refer to the official {ay} documentation for more information: link:https://documentation.suse.com/sles/15-SP1/single-html/SLES-autoyast/#book-autoyast[{ay} Guide].
====

==== Machine Configuration Prerequisites

Deployment with {ay} will require a minimum *disk size of 40 GB*.
That space is reserved for container images without any workloads (10 GB),
for the root partition (30 GB) and the EFI system partition (200 MB).

=== {ay} Preparation

. On the management machine, get an example {ay} file from `/usr/share/caasp/autoyast/bare-metal/autoyast.xml`,
(which was installed earlier on as part of the management pattern (`sudo zypper in -t pattern SUSE-CaaSP-Management`).
. Copy the file to a suitable location to modify it. Name the file `autoyast.xml`.
. Modify the following places in the {ay} file (and any additional places as required by your specific configuration/environment):
.. `<ntp-client>`
+
Change the pre-filled value to your organization's NTP server. Provide multiple servers if possible by adding new `<ntp_server>` subentries.
.. `<timezone>`
+
Adjust the timezone your nodes will be set to. Refer to: link:https://documentation.suse.com/sles/15-SP1/single-html/SLES-autoyast/#id-1.7.5.13.6[{sls} {ay} Guide: Country Settings]
.. `<username>sles</username>`
+
Insert your authorized key in the placeholder field.
.. `<users>`
+
You can add additional users by creating new blocks in the configuration containing their data.
+
[NOTE]
====
If the users are configured to not have a password like in the example, ensure the system's `sudoers` file is updated.
Without updating the sudoers file the user will only be able to perform basic operations that will prohibit many administrative tasks.

The default {ay} file provides examples for a disabled `root` user and a `sles` user with authorized key SSH access.

The password for root can be enabled by using the `passwd` command.
====
.. `<suse_register>`
+
Insert the email address and {productname} registration code in the placeholder fields. This activates {sls} {base_os_version}.
.. `<addon>`
+
Insert the {productname} registration code in the placeholder field. This enables the {productname} extension module.
Update the {ay} file with your registration keys and your company's best practices and hardware configurations.
+
[NOTE]
====
Your {productname} registration key can be used to both activate {sls} {base_os_version} and enable the extension.
====

+
Refer to the official {ay} documentation for more information: link:https://documentation.suse.com/sles/15-SP1/single-html/SLES-autoyast/#book-autoyast[{ay} Guide].
. Host the {ay} files on a Web server reachable inside the network you are installing the cluster in.

==== Deploying with local {RMT} server

In order to use a local {RMT} server for deployment of packages, you need to specify
the server configuration in your {ay} file. To do so add the following section:

[source,xml,subs="-macros"]
----
<suse_register>
<do_registration config:type="boolean">true</do_registration>
<install_updates config:type="boolean">true</install_updates>

<reg_server>https://rmt.example.org</reg_server> // <1>
<reg_server_cert>https://rmt.example.org/rmt.crt</reg_server_cert> // <2>
<reg_server_cert_fingerprint_type>SHA1</reg_server_cert_fingerprint_type>
<reg_server_cert_fingerprint>0C:A4:A1:06:AD:E2:A2:AA:D0:08:28:95:05:91:4C:07:AD:13:78:FE</reg_server_cert_fingerprint> // <3>
<slp_discovery config:type="boolean">false</slp_discovery>
<addons config:type="list">
  <addon>
    <name>sle-module-containers</name>
    <version>15.1</version>
    <arch>x86_64</arch>
  </addon>
  <addon>
    <name>caasp</name>
    <version>4.0</version>
    <arch>x86_64</arch>
  </addon>
</addons>
</suse_register>
----
<1> Provide FQDN of the {RMT} server
<2> Provide the location on the server where the certificate can be found
<3> Provide the certificate fingerprint for the {rmt} server

=== Provisioning the Cluster Nodes

Once the {ay} file is available in the network that the machines will be configured in, you can start deploying machines.

The default production scenario consists of 6 nodes:

* 1 load balancer
* 3 masters
* 2 workers

Depending on the type of load balancer you wish to use, you need to deploy at least 5 machines to serve as cluster nodes and provide a load balancer from the environment.

The load balancer must point at the machines that are assigned to be used as `master` nodes in the future cluster.

[TIP]
If you do not wish to use infrastructure load balancers, please deploy additional machines and refer to <<loadbalancer>>.

Install {sls} {base_os_version} from your preferred medium and follow the steps for link:https://documentation.suse.com/sles/15-SP1/single-html/SLES-autoyast/#invoking-autoinst[Invoking the Auto-Installation Process]

Provide `autoyast=https://[webserver/path/to/autoyast.xml]` during the {sls} {base_os_version} installation.

==== {sls} Installation

[NOTE]
====
Use AutoYaST and make sure to use a staged frozen patchlevel via RMT/SUSE Manager to ensure a 100% reproducible setup.
link:https://documentation.suse.com/sles/15-SP1/single-html/SLES-rmt/#cha-rmt-client[RMT Guide]
====

Once the machines have been installed using the {ay} file, you are now ready proceed with <<bootstrap>>.

=== Container Runtime Proxy

[IMPORTANT]
====
{crio} proxy settings must be adjusted on all nodes before joining the cluster!

Please refer to: link:{docurl}single-html/caasp-admin/#_configuring_httphttps_proxy_for_cri_o[]

In some environments you must configure the container runtime to access the container registries through a proxy.
In this case, please refer to: {docurl}single-html/caasp-admin/#_configuring_httphttps_proxy_for_cri_o[{productname} Admin Guide: Configuring HTTP/HTTPS Proxy for CRI-O]
====
