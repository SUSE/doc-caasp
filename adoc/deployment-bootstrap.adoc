[[bootstrap]]
== Bootstrapping the Cluster

Bootstrapping the cluster is the initial process of starting up the cluster
and defining which of the nodes are masters and which workers. For maximum automation of this process
{productname} uses the `skuba` package.

=== Preparation

==== Install `skuba`

First you need to install `skuba` on your local workstation:

. Add the SLE15 SP1 extension containing `skuba`. This also requires the "containers" module.
+
----
SUSEConnect -p sle-module-containers/15.1/x86_64
SUSEConnect -p caasp/4.0/x86_64 -r <Registration Code>
----
. Install `skuba`:
+
----
sudo zypper in skuba
----

=== Cluster Deployment

Make sure you have added the SSH identity (corresponding to the public SSH key distributed above)
to the ssh-agent on your workstation. For instructions on how to add the SSH identity,
refer to <<ssh.configuration>>.
This is a requirement for `skuba` (https://github.com/SUSE/skuba#prerequisites).


By default `skuba` connects to the nodes as `root` user. A different user can
be specified by the following flags:

----
--sudo --user <username>
----

[IMPORTANT]
====
You must configure `sudo` for the user to be able authenticate without password.
Replace `USERNAME` with the user you created during installation. As root, run:

----
echo "USERNAME ALL=(ALL) NOPASSWD: ALL" >> /etc/sudoers
----
====

==== Initializing the Cluster

Now you can initialize the cluster on the deployed machines.
As `--control-plane` enter the IP/FQDN of your load balancer.
If you do not use a load balancer use your first master node.

----
skuba cluster init --control-plane <LB IP/FQDN> my-cluster
----
`cluster init` generates the folder named `my-cluster` and initializes the directory that will hold the configuration (`kubeconfig`) for the cluster.

[IMPORTANT]
====
The IP/FQDN must be reachable by every node of the cluster and therefore 127.0.0.1/localhost can't be used.
====

==== Transitioning from Docker to CRI-O

{productname} {productversion} *default configuration* uses the CRI-O Container Engine in conjunction with Docker Linux capabilities.
This means {productname} {productversion} containers run on top of CRI-O with the following additional
Linux capabilities: `audit_write`, `setfcap` and `mknod`.
This measure ensures a transparent transition and seamless compatibility with workloads running
on the previous {productname} versions and out-of-the-box Docker compatibility.

In case you wish to use *unmodified CRI-O*,
use the `--strict-cap-defaults` option during the initial setup when you run `skuba cluster init`,
which will create the vanilla CRI-O configuration:

[source,bash]
skuba cluster init --strict-cap-defaults

Please be aware that this might result in
incompatibility with your previously running workloads,
unless you explicitly define the additional Linux capabilities required
on top of CRI-O defaults.

[IMPORTANT]
====
After the bootstrap of the {kube} cluster there will be no easy
way to revert this modification. Please choose wisely.
====


==== Cluster configuration

Before bootstrapping the cluster, it is advisable to perform some additional configuration.

===== Enabling cloud provider integration

Enable cloud provider integration to take advantage of the underlying cloud platforms
and automatically manage resources like the Load Balancer, Nodes (Instances), Network Routes
and Storage services.

If you want to enable cloud provider integration with different cloud platforms,
initialize the cluster with flag `--cloud-provider <CLOUD PROVIDER>`.
The only currently available option is `openstack`, but more options are planned:


----
skuba cluster init --control-plane <LB IP/FQDN> --cloud-provider openstack my-cluster
----


Running the above command will create a directory `my-cluster/cloud/openstack` with a
`README.md` and an `openstack.conf.template` in it. Copy `openstack.conf.template`
or create an `openstack.conf` file inside `my-cluster/cloud/openstack`,
according to the supported format.
The supported format and content can be found in the official Kubernetes documentation:

https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/#openstack

[WARNING]
====
The file `my-cluster/cloud/openstack/openstack.conf` must not be freely accessible.
Please remember to set proper file permissions for it, for example `600`.
====

===== Example OpenStack cloud provider configuration
You can find those required parameters in OpenStack RC File v3.
====
    [Global]
    auth-url=<OS_AUTH_URL> // <1>
    username=<OS_USERNAME> // <2>
    password=<OS_PASSWORD> // <3>
    tenant-id=<OS_PROJECT_ID> // <4>
    domain-name=<OS_USER_DOMAIN_NAME> // <5>
    region=<OS_REGION_NAME> // <6>
    ca-file="/etc/ssl/certs/SUSE_Trust_Root.pem" // <7>
    [LoadBalancer]
    lb-version=v2 // <8>
    subnet-id=<PRIVATE_SUBNET_ID> // <9>
    floating-network-id=<PUBLIC_NET_ID> // <10>
    create-monitor=yes // <11>
    monitor-delay=1m // <12>
    monitor-timeout=30s // <13>
    monitor-max-retries=3 // <14>
    [BlockStorage]
    bs-version=v2 // <15>
    ignore-volume-az=true // <16>
====
<1> (required) Specifies the URL of the Keystone API used to authenticate the user.
This value can be found in Horizon (the OpenStack control panel).
under Project > Access and Security > API Access > Credentials.
<2> (required) Refers to the username of a valid user set in Keystone.
<3> (required) Refers to the password of a valid user set in Keystone.
<4> (required) Used to specify the ID of the project where you want to create your resources.
<5> (optional) Used to specify the name of the domain your user belongs to.
<6> (optional) Used to specify the identifier of the region to use when running on
a multi-region OpenStack cloud. A region is a general division of an OpenStack deployment.
<7> (optional) Used to specify the path to your custom CA file.
<8> (optional) Used to override automatic version detection.
Valid values are `v1` or `v2`. Where no value is provided automatic detection
will select the highest supported version exposed by the underlying OpenStack cloud.
<9> (optional) Used to specify the ID of the subnet you want to create your load balancer on.
Can be found at Network > Networks. Click on the respective network to get its subnets.
<10> (optional) If specified, will create a floating IP for the load balancer.
<11> (optional) Indicates whether or not to create a health monitor for the Neutron load balancer.
Valid values are true and false. The default is false.
When true is specified then monitor-delay, monitor-timeout, and monitor-max-retries must also be set.
<12> (optional) The time between sending probes to members of the load balancer.
Ensure that you specify a valid time unit.
<13> (optional) Maximum time for a monitor to wait for a ping reply before it times out.
The value must be less than the delay value. Ensure that you specify a valid time unit.
<14> (optional) Number of permissible ping failures before changing the load balancer
memberâ€™s status to INACTIVE. Must be a number between 1 and 10.
<15> (optional) Used to override automatic version detection.
Valid values are v1, v2, v3 and auto. When auto is specified automatic detection
will select the highest supported version exposed by the underlying OpenStack cloud.
<16> (optional) Influence availability zone use when attaching Cinder volumes.
When Nova and Cinder have different availability zones, this should be set to `true`.



After setting options in `openstack.conf` file, please proceed with bootstrapping procedure <<cluster.bootstrap>>.

[IMPORTANT]
====
When the cloud provider integration is enabled, it's very important to bootstrap and join nodes with the same node names that they have inside `Openstack`, as
this name will be used by the `Openstack` cloud controller manager to reconcile node metadata.
====

===== Integrate External LDAP TLS

. Open the `Dex` `ConfigMap` in `my-cluster/addons/dex/dex.yaml`
. Adapt the `ConfigMap` by adding LDAP configuration to the connector section of the `config.yaml` file. For detailed configurations for the LDAP connector, refer to https://github.com/dexidp/dex/blob/v2.16.0/Documentation/connectors/ldap.md.
====
# Example LDAP connector

    connectors:
    - type: ldap
      id: 389ds
      name: 389ds
      config:
        host: ldap.example.org:636 // <1> <2>
        rootCAData: <base64 encoded PEM file> // <3>
        bindDN: cn=user-admin,ou=Users,dc=example,dc=org // <4>
        bindPW: <Password of Bind DN> // <5>
        usernamePrompt: Email Address // <6>
        userSearch:
          baseDN: ou=Users,dc=example,dc=org // <7>
          filter: "(objectClass=person)" // <8>
          username: mail // <9>
          idAttr: DN // <10>
          emailAttr: mail // <11>
          nameAttr: cn // <12>
====
<1> Host name of LDAP server reachable from the cluster.
<2> The port on which to connect to the host (e.g. StartTLS: `389`, TLS: `636`).
<3> LDAP server base64 encoded root CA certificate file (e.g. `cat <root-ca-pem-file> | base64 | awk '{print}' ORS='' && echo`)
<4> Bind DN of user that can do user searches.
<5> Password of the user.
<6> Label of LDAP attribute users will enter to identify themselves (e.g. `username`).
<7> BaseDN where users are located (e.g. `ou=Users,dc=example,dc=org`).
<8> Filter to specify type of user objects (e.g. "(objectClass=person)").
<9> Attribute users will enter to identify themselves (e.g. mail).
<10> Attribute used to identify user within the system (e.g. DN).
<11> Attribute containing the user's email.
<12> Attribute used as username used within OIDC tokens.

Besides the LDAP connector you can also setup other connectors. For additional connectors, refer to the available connector configurations in the Dex repository: https://github.com/dexidp/dex/tree/v2.16.0/Documentation/connectors.

===== Prevent Nodes Running Special Workloads From Being Rebooted

. Open the `kured` deployment in `my-cluster/addons/kured/kured.yaml`
. Adapt the `DaemonSet` by adding one of the following flags to the `command` section of the `kured` container:
+
----
---
apiVersion: apps/v1
kind: DaemonSet
...
spec:
  ...
    ...
      ...
      containers:
        ...
          command:
            - /usr/bin/kured
            - --blocking-pod-selector=name=<NAME OF POD>
----

You can add any key/value labels to this selector:
----
--blocking-pod-selector=<LABEL KEY 1>=<LABEL VALUE 1>,<LABEL KEY 2>=<LABEL VALUE 2>
----

Alternatively you can adapt the `kured` DaemonSet also later during runtime (after bootstrap) by editing `my-cluster/addons/kured/kured.yaml` and executing:
----
kubectl apply -f my-cluster/addons/kured/kured.yaml
----

This will restart all `kured` pods with the additional configuration flags.

==== Prevent Nodes With Any Prometheus Alerts From Being Rebooted

[NOTE]
====
By default, **any** prometheus alert blocks a node from reboot. However you can filter specific alerts to be ignored via the `--alert-filter-regexp` flag.
====

. Open the `kured` deployment in `my-cluster/addons/kured/kured.yaml`
. Adapt the `DaemonSet` by adding one of the following flags to the `command` section of the `kured` container:
+
----
---
apiVersion: apps/v1
kind: DaemonSet
...
spec:
  ...
    ...
      ...
      containers:
        ...
          command:
            - /usr/bin/kured
            - --prometheus-url=<PROMETHEUS SERVER URL>
            - --alert-filter-regexp=^(RebootRequired|AnotherBenignAlert|...$
----

[IMPORTANT]
====
The <PROMETHEUS SERVER URL> needs to contain the protocol (`http://` or `https://`)
====

Alternatively you can adapt the `kured` DaemonSet also later during runtime (after bootstrap) by editing `my-cluster/addons/kured/kured.yaml` and executing:
----
kubectl apply -f my-cluster/addons/kured/kured.yaml
----

This will restart all `kured` pods with the additional configuration flags.

[[cluster.bootstrap]]
==== Cluster bootstrap
. Switch to the new directory.
. Now bootstrap a master node.
For `--target` enter the IP address of your first master node.
Replace `<NODE NAME>` with a unique identifier for example "master-one".
+
.Secure configuration files access
[WARNING]
====
The directory created during this step contains configuration files
that allow full administrator access to your cluster.
Apply best practices for access control to this folder.
====
+
----
cd my-cluster
skuba node bootstrap --user sles --sudo --target <IP/FQDN> <NODE NAME>
----
This will bootstrap the specified node as the first master in the cluster.
The process will generate authentication certificates and the `admin.conf`
file that is used for authentication against the cluster.
The files will be stored in the `my-cluster` directory specified in step one.
. Add additional master nodes to the cluster.
+
Replace the `<IP/FQDN>` with the IP for the machine.
Replace `<NODE NAME>` with a unique identifier for example "master-two".
+
----
skuba node join --role master --user sles --sudo --target <IP/FQDN> <NODE NAME>
----
. Add a worker to the cluster.
+
Replace the `<IP/FQDN>` with the IP for the machine.
Replace `<NODE NAME>` with a unique identifier for example "worker-one".
+
----
skuba node join --role worker --user sles --sudo --target <IP/FQDN> <NODE NAME>
----
. Verify the nodes that you added
+
----
skuba cluster status
----
+
The output should look like this:
+# instance user name
username = "sles"
----
NAME         OS-IMAGE                              KERNEL-VERSION        CONTAINER-RUNTIME   HAS-UPDATES   HAS-DISRUPTIVE-UPDATES
master-one   SUSE Linux Enterprise Server 15 SP1   4.12.14-110-default   cri-o://1.13.3      <none>        <none>
worker-one   SUSE Linux Enterprise Server 15 SP1   4.12.14-110-default   cri-o://1.13.3      <none>        <none>
----

[IMPORTANT]
====
The IP/FQDN must be reachable by every node of the cluster and therefore 127.0.0.1/localhost can't be used.
====

=== Using kubectl

You can install and use kubectl by installing the kubernetes-client package from the {productname} extension.

----
sudo zypper in kubernetes-client
----

[TIP]
====
Alternatively you can install from upstream: https://kubernetes.io/docs/tasks/tools/install-kubectl/.
====

To talk to your cluster, simply symlink the generated configuration file to `~/.kube/config`.

[source,bash]
----
ln -s ~/clusters/my-cluster/admin.conf ~/.kube/config
----

Then you can perform all cluster operations as usual. For example checking cluster status with either:

* `skuba cluster status`
+
or
* `kubectl get nodes -o wide`
+
or
* `kubectl get pods --all-namespaces`
+
[source,bash]
----
# kubectl get pods --all-namespaces

NAMESPACE     NAME                                READY     STATUS    RESTARTS   AGE
kube-system   coredns-86c58d9df4-5zftb            1/1       Running   0          2m
kube-system   coredns-86c58d9df4-fct4m            1/1       Running   0          2m
kube-system   etcd-my-master                      1/1       Running   0          1m
kube-system   kube-apiserver-my-master            1/1       Running   0          1m
kube-system   kube-controller-manager-my-master   1/1       Running   0          1m
kube-system   kube-flannel-ds-amd64-b6krs         1/1       Running   0          53s
kube-system   kube-flannel-ds-amd64-v7kt7         1/1       Running   0          2m
kube-system   kube-proxy-5qxnt                    1/1       Running   0          2m
kube-system   kube-proxy-746ws                    1/1       Running   0          53s
kube-system   kube-scheduler-my-master            1/1       Running   0          1m
----
