[[deployment.preparations]]
== Deployment Preparations

In order to deploy {productname} you need a workstation running {sle} 15 SP1 or similar {opensuse} equivalent.
This workstation is called the "Management machine". Important files are generated
and must be maintained on this machine, but it is not a member of the skuba cluster.

[[ssh.configuration]]
=== Basic SSH Key Configuration

[NOTE]
====
The use of `ssh-agent` comes with some implications for security that you should take into consideration.

link:http://rabexc.org/posts/pitfalls-of-ssh-agents[The pitfalls of using ssh-agent]

To avoid these risks please make sure to either use `ssh-agent -t <timeout>` and specify a time
after which the agent will self-terminate or terminate the agent yourself before logging out by running
`ssh-agent -k`.
====

To log in to the created cluster nodes, you need to configure an SSH key pair and
load it into your user's `ssh-agent` program. This is also mandatory in order to be able to use
the installation tools `terraform` and `skuba`. In a later deployment step,
skuba will ensure that the key is distributed across all the nodes and trusted by them.
For now, you only need to make sure that an ssh-agent is running and that it has
the SSH key added:

. The `ssh-agent` is usually started automatically by graphical
desktop environments. If that is not your case, run:
+
----
eval "$(ssh-agent)"
----
This will start the agent and set environment variables used for agent
communication within the current session. This has to be the same terminal session
that you run the `skuba` commands in. A new terminal usually requires a new ssh-agent.
In some desktop environments the ssh-agent will also automatically load the SSH keys.
To add an SSH key manually, use the `ssh-add` command:
+
----
ssh-add <path_to_key>
----
+
[TIP]
====
If you are adding the SSH key manually, specify the full path.
For example: `/home/sles/.ssh/id_rsa`
====

You can load multiple keys into your agent using the `ssh-add <path_to_key>` command.
Keys should be password protected as a security measure. The
ssh-add command will prompt for your password, then the agent caches the
private key for a configurable lifetime. The `-t lifetime` option to
ssh-add specifies a maximum time to cache the key. See `man ssh-add` for
more information.

.Usage of multiple identities with `ssh-agent`
[NOTE]
====
Skuba will try all the identities loaded into the `ssh-agent` until one of
them grants access to the node, or until the SSH servers' maximum authentication attempts are exhausted.
This could lead to undesired messages in SSH or other security/authentication logs on your local machine.

If you wish to avoid authentication attempts with identities irrelevant to {productname}, please configure
a specific `Host` and `IdentityFile` combination in your local `~/.ssh/config`.

For example:

----
Host caasp.cluster
  HostName cluster.fqdn.domain
  ForwardAgent yes
  User sles
  IdentityFile /home/sles/.ssh/id_rsa
----

====

==== Forwarding the Authentication Agent Connection
It is also possible to *forward the authentication agent connection* from a
host to another, which can be useful if you intend to run skuba on
a "jump host" and don't want to copy your private key to this node.
This can be achieved using the `ssh -A` command. Please refer to the man page
of `ssh` to learn about the security implications of using this feature.


[[registration_code]]
=== Product Key

[NOTE]
====
The registration code for {productname} {productmajor} also contains the activation
permissions for the underlying {sle} operating system. You can use your {productname}
registration code to activate the {sle} 15 SP1 subscription during installation.
====

You need a subscription registration code to use {productname}. You can retrieve your
registration code from {scc}.

* Login to https://scc.suse.com
* Navigate to menu:Products[]]
* Search for "caasp"
* Select menu:SUSE CaaS Platform[4.0 > x86_64]
* Copy the menu:Registration Code[]

=== Installation Tools

For any deployment type you will need `skuba` and `{tf}`. These packages are
available from the {productname} package sources. They are provided as an installation
"pattern" that will install dependencies and other required packages in one simple step.

Access to the packages requires the `{productname}` and `Containers` extension modules.
Enable the modules during the operating system installation or activate them using {suse} Connect.

[source,bash]
----
sudo SUSEConnect -r CAASP_REGISTRATION_CODE # <1>
sudo SUSEConnect -p sle-module-containers/15.1/x86_64 # <2>
sudo SUSEConnect -p caasp/4.0/x86_64 -r CAASP_REGISTRATION_CODE # <3>
----
<1> Activate {sle}
<2> Add the free `Containers` module
<3> Add the {productname} extension with your registration code

Install the required tools:
----
sudo zypper in -t pattern SUSE-CaaSP-Management
----

This will install the `skuba` command line tool and `{tf}`; as well
as various default configurations and examples.


[NOTE]
.Using a Proxy Server
====

Sometimes, you need a proxy server to be able to connects the SUSE Customer Center.
To do that:

. Expose the environmental variable `http_proxy`:
+
----
export http_proxy=http://PROXY_IP_FQDN:PROXY_PORT
----
+
. Replace `PROXY_IP_FQDN` by the IP address or a fully qualified domain name (FQDN) of the
proxy server and `PROXY_PORT` by its port.
. If you need a proxy server with basic authentication, create the file `/root/.curlrc`
with the following content:
+
----
--proxy-user "USER:PASSWORD"
----
+
Replace `USER` and `PASSWORD` with the credentials of an allowed user for the proxy server.

====

[[loadbalancer]]
=== Load Balancer

[IMPORTANT]
====
Setting up a load balancer is mandatory in any production environment.
====

{productname} requires a load balancer to distribute workload between the deployed
master nodes of the cluster. A failure-tolerant {productname} cluster will always
use more than one control plane node as well as more than one load balancer,
so there isn't a single point of failure.

There are many ways to configure a load balancer. This documentation cannot
describe all possible combinations of load balancer configurations and thus
does not aim to do so. Please apply your organization's load balancing best
practices.

For {soc}, the {tf} configurations shipped with this version will automatically deploy
a suitable load balancer for the cluster.

For VMware you must configure a load balancer manually and allow it access to
all master nodes created during <<bootstrap>>.

The load balancer should be configured before the actual deployment. It is needed
during the cluster bootstrap, and also during upgrades. To simplify configuration,
you can reserve the IPs needed for the cluster nodes and pre-configure these in
the load balancer.

The load balancer needs access to port `6443` on the `apiserver` (all master nodes)
in the cluster. It also needs access to Gangway port `32001` and Dex port `32000`
on all master and worker nodes in the cluster for RBAC authentication.

We recommend performing regular HTTPS health checks on each master node `/healthz`
endpoint to verify that the node is responsive. This is particularly important during
upgrades, when a master node restarts the apiserver. During this rather short time
window, all requests have to go to another master node's apiserver. The master node
that is being upgraded will have to be marked INACTIVE on the load balancer pool
at least during the restart of the apiserver. We provide reasonable defaults
for that on our default openstack load balancer {tf} configuration.

The following contains examples for possible load balancer configurations based on {sle} 15 SP1 and `nginx` or `HAProxy`.

include::deployment-loadbalancer.adoc[Load Balancer,leveloffset=+2]
