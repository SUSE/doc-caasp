[#deployment-preparations]
== Deployment Preparations

In order to deploy {productname} you need a workstation running {sls} {base_os_version} or similar {opensuse} equivalent.
This workstation is called the "Management machine". Important files are generated
and must be maintained on this machine, but it is not a member of the {productname} cluster.

[#ssh-configuration]
=== Basic SSH Key Configuration

*In order to successfully deploy {productname}, you need to have SSH keys loaded into an SSH agent.* This is important, because it is required in order to use the installation tools `skuba` and `terraform`.

[NOTE]
====
The use of `ssh-agent` comes with some implications for security that you should take into consideration.

link:http://rabexc.org/posts/pitfalls-of-ssh-agents[The pitfalls of using ssh-agent]

To avoid these risks please make sure to either use `ssh-agent -t <TIMEOUT>` and specify a time
after which the agent will self-terminate, or terminate the agent yourself before logging out by running `ssh-agent -k`.
====

To log in to the created cluster nodes from the Management machine, you need to configure an SSH key pair.
This key pair needs to be trusted by the user account you will log in with into each cluster node; that user is called "sles" by default.
In order to use the installation tools `terraform` and `skuba`, this trusted keypair must be loaded into the SSH agent.

. If you do not have an existing ssh keypair to use, run:
+
----
ssh-keygen -t ed25519
----
+
. The `ssh-agent` or a compatible program is sometimes started automatically by graphical desktop
environments. If that is not your situation, run:
+
----
eval "$(ssh-agent)"
----
+
This will start the agent and set environment variables used for agent
communication within the current session. This has to be the same terminal session
that you run the `skuba` commands in. A new terminal usually requires a new ssh-agent.
In some desktop environments the ssh-agent will also automatically load the SSH keys.
To add an SSH key manually, use the `ssh-add` command:
+
----
ssh-add <PATH_TO_KEY>
----
+
[TIP]
====
If you are adding the SSH key manually, specify the full path.
For example: `/home/sles/.ssh/id_rsa`
====

You can load multiple keys into your agent using the `ssh-add <PATH_TO_KEY>` command.
Keys should be password protected as a security measure. The
ssh-add command will prompt for your password, then the agent caches the
decrypted key material for a configurable lifetime. The `-t lifetime` option to
ssh-add specifies a maximum time to cache the specific key. See `man ssh-add` for
more information.

.Usage of multiple identities with `ssh-agent`
[NOTE]
====
Skuba will try all the identities loaded into the `ssh-agent` until one of
them grants access to the node, or until the SSH server's maximum authentication attempts are exhausted.
This could lead to undesired messages in SSH or other security/authentication logs on your local machine.
====

==== Forwarding the Authentication Agent Connection
It is also possible to *forward the authentication agent connection* from a
host to another, which can be useful if you intend to run skuba on
a "jump host" and don't want to copy your private key to this node.
This can be achieved using the `ssh -A` command. Please refer to the man page
of `ssh` to learn about the security implications of using this feature.

[#registration-code]
=== Registration Code

[NOTE]
====
The registration code for {productname}.{productmajor} also contains the activation
permissions for the underlying {sle} operating system. You can use your {productname}
registration code to activate the {sls} {base_os_version} subscription during installation.
====

You need a subscription registration code to use {productname}. You can retrieve your
registration code from {scc}.

* Login to https://scc.suse.com
* Navigate to menu:MY ORGANIZATIONS -> <YOUR ORG>[]
* Select the menu:Subscriptions[] tab from the menu bar at the top
* Search for "CaaS Platform"
* Select the version you wish to deploy (should be the highest available version)
* Click on the Link in the menu:Name[] column
* The registration code should be displayed as the first line under "Subscription Information"

[TIP]
====
If you can not find {productname} in the list of subscriptions please contact
your local administrator responsible for software subscriptions or {suse} support.
====

[#machine-id]
=== Unique Machine IDs
During deployment of the cluster nodes, each machine will be assigned a unique ID in the `/etc/machine-id` file by {tf} or {ay}.
If you are using any (semi-)manual methods of deployments that involve cloning of machines and deploying from templates,
you must make sure to delete this file before creating the template.

If two nodes are deployed with the same `machine-id`, they will not be correctly recognized by `skuba`.

[#machine-id-regen]
.Regenerating Machine ID
[IMPORTANT]
====
In case you are not using {tf} or {ay} you must regenerate machine IDs manually.

During the template preparation you will have removed the machine ID from the template image.
This ID is required for proper functionality in the cluster and must be (re-)generated on each machine.

Log in to each virtual machine created from the template and run:

----
dbus-uuidgen --ensure
systemd-machine-id-setup
systemctl restart systemd-journald
----

This will regenerate the `machine id` values for `DBUS` (`/var/lib/dbus/machine-id`) and `systemd` (`/etc/machine-id`) and restart the logging service to make use of the new IDs.
====

=== Installation Tools

For any deployment type you will need `skuba` and `{tf}`. These packages are
available from the {productname} package sources. They are provided as an installation
"pattern" that will install dependencies and other required packages in one simple step.

Access to the packages requires the `{productname}` and `Containers` extension modules.
Enable the modules during the operating system installation or activate them using {suse} Connect.

[source,bash]
----
sudo SUSEConnect -r  <CAASP_REGISTRATION_CODE> # <1>
sudo SUSEConnect -p sle-module-containers/15.1/x86_64 # <2>
sudo SUSEConnect -p caasp/4.0/x86_64 -r <CAASP_REGISTRATION_CODE> # <3>
----
<1> Activate {sle}
<2> Add the free `Containers` module
<3> Add the {productname} extension with your registration code

Install the required tools:
----
sudo zypper in -t pattern SUSE-CaaSP-Management
----

This will install the `skuba` command line tool and `{tf}`; as well
as various default configurations and examples.


[NOTE]
.Using a Proxy Server
====

Sometimes you need a proxy server to be able to connect to the {scc}.
If you have not already configured a system-wide proxy, you can temporarily do
so for the duration of the current shell session like this:

. Expose the environmental variable `http_proxy`:
+
----
export http_proxy=http://PROXY_IP_FQDN:PROXY_PORT
----
+
. Replace `<PROXY_IP_FQDN>` by the IP address or a fully qualified domain name (FQDN) of the
proxy server and `<PROXY_PORT>` by its port.
. If you use a proxy server with basic authentication, create the file `$HOME/.curlrc`
with the following content:
+
----
--proxy-user "<USER>:<PASSWORD>"
----
+
Replace `<USER>` and `<PASSWORD>` with the credentials of an allowed user for the proxy server, and consider limiting access to the file (`chmod 0600`).

====

[#loadbalancer]
=== Load Balancer

[IMPORTANT]
====
Setting up a load balancer is mandatory in any production environment.
====

{productname} requires a load balancer to distribute workload between the deployed
master nodes of the cluster. A failure-tolerant {productname} cluster will always
use more than one control plane node as well as more than one load balancer,
so there isn't a single point of failure.

There are many ways to configure a load balancer. This documentation cannot
describe all possible combinations of load balancer configurations and thus
does not aim to do so. Please apply your organization's load balancing best
practices.

For {soc}, the {tf} configurations shipped with this version will automatically deploy
a suitable load balancer for the cluster.

For bare metal, KVM, or VMware, you must configure a load balancer manually and
allow it access to all master nodes created during <<bootstrap>>.

The load balancer should be configured before the actual deployment. It is needed
during the cluster bootstrap, and also during upgrades. To simplify configuration,
you can reserve the IPs needed for the cluster nodes and pre-configure these in
the load balancer.

The load balancer needs access to port `6443` on the `apiserver` (all master nodes)
in the cluster. It also needs access to Gangway port `32001` and Dex port `32000`
on all master and worker nodes in the cluster for RBAC authentication.

We recommend performing regular HTTPS health checks on each master node `/healthz`
endpoint to verify that the node is responsive. This is particularly important during
upgrades, when a master node restarts the apiserver. During this rather short time
window, all requests have to go to another master node's apiserver. The master node
that is being upgraded will have to be marked INACTIVE on the load balancer pool
at least during the restart of the apiserver. We provide reasonable defaults
for that on our default openstack load balancer {tf} configuration.

The following contains examples for possible load balancer configurations based on {sls} {base_os_version} and `nginx` or `HAProxy`.

include::deployment-loadbalancer.adoc[Load Balancer,leveloffset=+2]
