[[deployment.preparations]]
== Deployment Preparations

In order to deploy {productname} you need a workstation running {sle} 15 SP1 or similar {opensuse} equivalent.
This workstation is called the "Management node".

[[ssh.configuration]]
=== Basic SSH key configuration

To log into the created cluster nodes, you need to configure an SSH key pair and
load it into your users `ssh-agent` program. This is also mandatory in order to be able to use
the installation tools `terraform` and `skuba`. In a later deployment step,
skuba will ensure that the key is distributed across all the nodes and trusted by them.
For now, you only need to make sure that an ssh-agent is running and that it has
the SSH key added:

. The `ssh-agent` is usually started automatically by graphical
desktop environments. If that is not your case run:
+
----
eval "$(ssh-agent)"
----
This will start the agent and set environment variables used for agent
communication within the current session. This has to be the same terminal session
that you run the `skuba` commands in. A new terminal usually requires a new ssh-agent.
In some desktop environments the ssh-agent will also automatically load the SSH keys.
To add an SSH key manually, use the `ssh-add` command:
+
----
ssh-add <path_to_key>
----
+
[TIP]
====
If you are adding the SSH key manually, specify the full path.
For example: `/home/sles/.ssh/id_rsa`
====

You can load multiple keys into your agent using the `ssh-add <path_to_key>` command.
Keys should be password protected as a security measure. The
ssh-add command will prompt for your password, then the agent caches the
private key for a configurable lifetime. The `-t lifetime` option to
ssh-add specifies a maximum time to cache the key. See `man ssh-add` for
more information.

[NOTE]
====
Skuba will try all the identities loaded into the ssh-agent until one of
them grants access to the node, or until the ssh server maximum
authentication attempts are exhausted.
====

==== Forwarding the authentication agent connection
It is also possible to *forward the authentication agent connection* from a
host to another one, which can be useful if you intend to run skuba on
a "jump host" and don't want to copy your private key to this node.
This can be achieved using the `ssh -A` command. Please refer to the man page
of `ssh` to learn about the security implications of using this feature.


[[registration_code]]
=== Registration Code

[NOTE]
====
The registration code for {productname} {productmajor} also contains the activation
permissions for the underlying {sle} operating system. You can use your {productname}
registration code to activate the {sle} 15 SP1 subscription during installation.
====

ifeval::['{release_type}' == 'internal']
You need a subscription registration code to use {productname}. You can retrieve your
registration code from {scc}.

* Login to https://scc.suse.com
* Navigate to menu:Internal tools[Products]
* Search for "caasp"
* Select menu:SUSE CaaS Platform[4.0 > x86_64 alpha]
* Copy the menu:Registration Code[]
endif::[]

ifeval::['{release_type}' == 'public']
If you wish to beta test {productname} {productmajor}, please send an e-mail
to beta-programs@lists.suse.com to request a {scc} subscription and a registration code.
endif::[]

=== Installation tools

For any deployment type you will need `skuba` and `{tf}`. These packages are
available from the {productname} package sources. They are provided as an installation
"pattern" that will install dependencies and other required packages in one simple step.

Access to the packages requires the `{productname}` and `Containers` extension modules.
Enable the modules during the operating system installation or activate them using {suse} Connect.

[source,bash]
----
sudo SUSEConnect -r REGISTRATION_CODE # <1>
sudo SUSEConnect -p sle-module-containers/15.1/x86_64 # <2>
sudo SUSEConnect -p caasp/4.0/x86_64 -r REGISTRATION_CODE # <3>
----
<1> Activate {sle} (Skip if you already have an activated operating system)
<2> Add the free `Containers` module
<3> Add the {productname} extension with your registration code

Install the required tools:
----
sudo zypper in -t pattern SUSE-CaaSP-Management
----

This will install the `skuba` command line tool and `{tf}`; as well
as various default configurations and examples.

=== Load Balancer

[IMPORTANT]
====
Setting up a load balancer is mandatory in any production environment.
====

{productname} requires a load balancer to distribute workload between the deployed
master nodes of the cluster. A failure tolerant {productname} cluster will
always use more than one load balancer since that becomes a "single point of failure".

There are many ways to configure a load balancer. This documentation can not
describe all possible combinations of load balancer configurations and thus
does not aim to do so. Please apply your organizations' load balancing best
practices.

For {soc} the {tf} configurations shipped with this version will automatically deploy
a suitable loadbalancer for the cluster.

For VMWare you must configure a load balancer manually and allow it access to
all master nodes created during <<bootstrap>>.

The load balancer should be configured before the actual deployment. It is needed
during the cluster bootstrap. To simplify configuration you can reserve the IPs
needed for the cluster nodes and pre-configure these in the load balancer.

The load balancer needs access to port `6443` on the `apiserver` (all master nodes)
in the cluster. It also needs access to Gangway port `32001` and Dex port `32000`
on all master and worker nodes in the cluster for RBAC authentication.

We recommend performing regular HTTPS health checks each master node `/healthz`
endpoint to verify that the node is responsive.

The following is an example of a possible load balancer configuration based on {sle} 15 SP1 and `nginx`.

include::deployment-loadbalancer.adoc[Load Balancer,leveloffset=+2]
