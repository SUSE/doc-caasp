<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.1" xml:id="cha.admin.troubleshooting"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Troubleshooting</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker/>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
  <abstract>
   <para>
    This chapter summarizes frequent problems that occur while using
    &productname; and their solutions.
   </para>
  </abstract>
 </info>

 <sect1 xml:id="sec.admin.troubleshooting.overview">
  <title>Overview</title>
  <para>
   This chapter is a collection of frequent problems that are reported
   for &productname;. Additionally, &suse; support collects problems
   and their solutions online at <link
   xlink:href="https://www.suse.com/support/kb/?id=SUSE_CaaS_Platform"
   />.
  </para>
  <para>
   In case of problems, a detailed system report can be created with
   the <command>supportconfig</command> command line tool. It will
   collect information about the system such as: current kernel
   version, hardware, installed packages, partition setup, and much
   more. The result is a TAR archive of files. After opening a Service
   Request (SR), you can upload the TAR archive to Global Technical
   Support. It will help to locate the issue you reported and to assist
   you in solving the problem. For details, see <link xlink:href=
   "https://www.suse.com/documentation/sles-12/book_sle_admin/data/cha_adm_support.html"
   />.
  </para>
  <para>
   While resolving a technical difficulty with SUSE Support, you may
   receive a so-called Program Temporary Fix (PTF). PTFs may be issued
   for various packages as RPMs. For details about handling PTFs, see
   <xref linkend="sec.admin.software.patch" />.
  </para>
  <para>
   For details about how to contact &suse; support, refer to <link
   xlink:href=
   "https://www.suse.com/media/flyer/suse_customer_support_quick_reference_guide_flyer.pdf"
   /> and <link xlink:href="https://www.suse.com/support/" />.
  </para>
 </sect1>

 <sect1 xml:id="sec.admin.troubleshooting.failed_bootstrap">
  <title>Debugging Failed Bootstrap</title>
  <para>
   If the bootstrapping as described in <xref
   linkend="sec.deploy.install.bootstrap" /> fails, there are several
   places to look for errors. The following procedure outlines where to
   start debugging.
  </para>
  <procedure>
   <step>
    <para>
     Check the logs from Velum dashboard. Execute on the &admin_node;:
    </para>
    <screen>&prompt.root.admin;<command>docker logs $(docker ps | grep "velum-dashboard" | awk '{print $1}')</command></screen>
   </step>
   <step>
    <para>
     Convert the log into a human readable format and open it with
     <command>less</command>.
    </para>
<screen>&prompt.root.admin;<command>/var/lib/supportutils-plugin-suse-caasp/debug-salt --json_output=salt.json \
    --summary_output=summary.txt --text-status --no-color</command>
&prompt.root.admin;<command>less summary.txt</command></screen>
   </step>
   <step>
    <para>
     Retry bootstrapping from &admin_node; console.
    </para>
<screen>&prompt.root.admin;<command>docker exec -it $(docker ps | grep salt-master | awk '{print $1}') \
    salt-run -l debug state.orchestrate orch.kubernetes > salt-orchestration.logs</command></screen>
   </step>
   <step>
    <para>
     If the orchestration failed in late state, you can check if
     <command>etcd</command> and the &kube; cluster is up and running.
     Log in on a &master_node; and check if <command>etcd</command>
     cluster is up and running.
    </para>
<screen>&prompt.root.master;<command>set -a</command>
&prompt.root.master;<command>source /etc/sysconfig/etcdctl</command>
&prompt.root.master;<command>etcdctl member list</command>
&prompt.root.master;<command>etcdctl endpoint health</command>
&prompt.root.master;<command>etcdctl endpoint status</command>
&prompt.root.master;<command>journalctl -u etcd</command></screen>
    <para>
     Check if &kube; cluster is up and running.
    </para>
<screen>&prompt.root.master;<command>kubectl get nodes</command>
&prompt.root.master;<command>kubectl cluster-info</command>
&prompt.root.master;<command>journalctl -u kubelet</command>
&prompt.root.master;<command>journalctl -u kube-apiserver</command>
&prompt.root.master;<command>journalctl -u kube-scheduler</command>
&prompt.root.master;<command>journalctl -u kube-controller-manager</command></screen>
   </step>
   <step>
    <para>
     Collect all relevant logs with <command>supportconfig</command> on
     the &master_node; and &admin_node;.
    </para>
<screen>&prompt.root.admin;<command>supportconfig</command>
&prompt.root.admin;<command>tar -xvjf /var/log/nts_*.tbz</command>
&prompt.root.admin;<command>cd nts*</command>
&prompt.root.admin;<command>cat etcd.txt kubernetes.txt salt-minion.txt</command></screen>
<screen>&prompt.root.master;<command>supportconfig</command>
&prompt.root.master;<command>tar -xvjf /var/log/nts_*.tbz</command>
&prompt.root.master;<command>cd nts*</command>
&prompt.root.master;<command>cat etcd.txt kubernetes.txt salt-minion.txt</command></screen>
   </step>
  </procedure>
 </sect1>

 <sect1 xml:id="sec.admin.troubleshooting.failed_update">
  <title>Recovering from Failed Update</title>
  <para>
   See <xref
   linkend="sec.admin.software.transactional-updates.recovering" />.
  </para>
 </sect1>

 <sect1 xml:id="sec.admin.troubleshooting.etcd">
  <title>Checking <literal>etcd</literal> Health</title>
  <para>
   To work with <command>etcdctl</command>, you have to pass parameters
   with paths to required certificates.
  </para>
  <para>
   Use SSH to log into one of the cluster nodes, for example your first master.
   The following command provides an example for using <command>etcdctl</command>.
  </para>
<screen>&prompt.root;<command>set -a</command>
&prompt.root;<command>source /etc/sysconfig/etcdctl</command>
&prompt.root;<command>etcdctl cluster-health</command></screen>
 </sect1>

 <sect1 xml:id="sec.admin.troubleshooting.velum_ptf">
  <title>Locking Installed Program Temporary Fixes</title>
  <para>
   It can happen that <command>zypper</command> removes an installed
   <emphasis>Program Temporary Fix</emphasis> (<emphasis>PTF</emphasis>)
   when updates are started with Velum. To prevent this, execute the
   following procedure.
  </para>
  <important>
   <title>Locks Disable Updates for Package</title>
   <para>
    If a package is locked, it will no longer receive any updates until
    the lock is released with <command>zypper rl</command>.
   </para>
  </important>
  <procedure>
   <step>
    <para>
     Install the PTF manually.
    </para>
    <screen>&prompt.root;<command>transactional-update reboot <replaceable>PTF_FILE</replaceable>.rpm install
</command></screen>
   </step>
   <step>
    <para>
     As soon as the node has restarted, verify that the RPM is installed.
    </para>
    <screen>&prompt.root;<command>rpm -qa | grep PTF</command></screen>
   </step>
   <step>
    <para>
     Create a <command>zypper</command> lock for this RPM.
    </para>
    <screen>&prompt.root;<command>zypper al <replaceable>PTF_PACKAGE_NAME</replaceable></command></screen>
   </step>
   <step>
    <para>
     Verify that the lock is created.
    </para>
    <screen>&prompt.root;<command>zypper ll</command></screen>
   </step>
  </procedure>
 </sect1>

 <sect1 xml:id="sec.admin.troubleshooting.network_planning">
  <title>Network Planing and Reconfiguration</title>
  <para>
   It is not possible to reconfigure the used IP ranges of &productname;
   once the deployment is complete. Therefore, carefully plan for
   required IP ranges and future scenarios.
  </para>
  <para>
   Additionally, keep the network requirements in mind, as stated in
   <xref linkend="sec.deploy.requirements.network" />.
  </para>
 </sect1>

 <sect1 xml:id="sec.admin.troubleshooting.proxy">
  <title>Using a Proxy Server with Authentication</title>
  <para>
   If you need to register the &admin_node; with &scc; over a proxy
   server that requires authentication, it is not possible to specify
   the configuration in <filename>/etc/sysconfig/proxy</filename>.
  </para>
  <para>
   For information about setting authentication credentials and
   connecting to &scc; with <command>SUSEConnect</command>, see <xref
   linkend="sec.configuration.suseconnect.proxy" />.
  </para>
 </sect1>

 <sect1 xml:id="sec.admin.troubleshooting.replace_certificates">
  <title>Replacing TLS/SSL Certificates</title>
  <para>
   Sometimes certificates are not updated properly because they are
   outdated. To replace outdated certificates, execute the following
   procedure.
  </para>
  <procedure>
   <step>
    <para>
     Use SSH  and log in on the &admin_node;.
    </para>
   </step>
   <step>
    <para>
      Move the expired certs out of the way.
    </para>
    <screen>&prompt.root.admin;<command>mv /etc/pki/{velum,ldap,salt-api}.crt /root</command></screen>
   </step>
   <step>
    <para>
     Generate new certificates.
    </para>
<screen>&prompt.root.admin;<command>cd /etc/pki</command>
&prompt.root.admin;<command>/usr/share/caasp-container-manifests/gen-certs.sh</command></screen>
    <tip>
     <title>Generating Additional Certificates</title>
     <para>
      To regenerate additional certificates, for example
      <filename>/etc/pki/kubectl-client-cert.crt</filename>, add an
      additional line at the end of the
      <command>gen-certs.sh</command> script:
     </para>
<screen>&prompt.root.admin;<command>transactional-update shell</command>
<prompt>transactional update #</prompt> <command>echo "gencert \"kubectl-client-cert\" \"kubectl-client-cert\" \
    \"\$all_hostnames\" \"\$(ip_addresses)\"" >>/usr/share/caasp-container-manifests/gen-certs.sh</command>
<prompt>transactional update # </prompt><command>/usr/share/caasp-container-manifests/gen-certs.sh</command>
<prompt>transactional update # </prompt><command>exit</command></screen>
    </tip>
   </step>
   <step>
    <para>
     Use SSH and log in on a &master_node;.
    </para>
   </step>
   <step>
    <para>Backup and delete the dex-tls secret.</para>
<screen>&prompt.root.master;<command>kubectl -n kube-system get secret dex-tls -o yaml > /root/dex-tls</command>
&prompt.root.master;<command>kubectl -n kube-system delete secret dex-tls</command></screen>
   </step>
   <step>
    <para>
     On a master node, find and delete the Dex pods.
    </para>
    <important>
     <title>This Step Breaks Authentication</title>
     <para>
      Executing this step prevents new authentications requests from
      succeeding. However, the static credentials located on the master
      nodes will continue to function.
     </para>
     <para>
      The Dex pods will not restart by themselves until the dex-tls
      secret is recreated.
     </para>
    </important>
<screen>&prompt.root.master;<command>kubectl -n kube-system get pods | grep dex</command>
&prompt.root.master;<command>kubectl -n kube-system delete pods <replaceable>DEX_POD1</replaceable> <replaceable>DEX_POD2</replaceable> <replaceable>DEX_POD3</replaceable></command></screen>
   </step>
   <step>
    <para>
     Manually run the salt orchestration on the &admin_node;. This may
     take some time.
    </para>
<screen>&prompt.root.admin;<command>docker exec -it $(docker ps | grep salt-master | awk '{print $1}') \
    bash -c "salt-run state.orchestrate orch.kubernetes" 2&amp;>1 > salt-run.log</command></screen>
   </step>
   <step>
    <para>
     Check the tail of <filename>salt-run.log</filename> to see if the
     orchestration succeeded.
    </para>
    <screen>&prompt.root.admin;<command>tail -n 50 salt-run.log</command></screen>
   </step>
   <step>
    <para>
     On a master node, validate the dex pods are running.
    </para>
    <screen>&prompt.root.master;<command>kubectl -n kube-system get pods | grep dex</command></screen>
   </step>
   <step>
    <para>
     If you are not able to log in into Velum, reboot the &admin_node;.
     Then test and validate that the cluster is still functional.
    </para>
   </step>
  </procedure>
 </sect1>
</chapter>
