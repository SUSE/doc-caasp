<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.1" xml:id="cha-admin-troubleshooting"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Troubleshooting</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker/>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
  <abstract>
   <para>
    This chapter summarizes frequent problems that can occur while using
    &productname; and their solutions.
   </para>
  </abstract>
 </info>

 <sect1 xml:id="sec-admin-troubleshooting-overview">
  <title>Overview</title>
  <para>
   This chapter is a collection of frequent problems that are reported
   for &productname;. Additionally, &suse; support collects problems
   and their solutions online at <link
   xlink:href="https://www.suse.com/support/kb/?id=SUSE_CaaS_Platform"
   />.
  </para>
  <para>
   In case of problems, a detailed system report can be created with
   the <command>supportconfig</command> command line tool. It will
   collect information about the system such as: current kernel
   version, hardware, installed packages, partition setup, and much
   more. The result is a TAR archive of files. After opening a Service
   Request (SR), you can upload the TAR archive to Global Technical
   Support. It will help to locate the issue you reported and to assist
   you in solving the problem. For details, see <link xlink:href=
   "https://www.suse.com/documentation/sles-12/book_sle_admin/data/cha_adm_support.html"
   />.
  </para>
  <para>
   While resolving a technical difficulty with SUSE Support, you may
   receive a so-called Program Temporary Fix (PTF). PTFs may be issued
   for various packages as RPMs. For details about handling PTFs, see
   <xref linkend="sec-admin-software-patch" />.
  </para>
  <para>
   For details about how to contact &suse; support, refer to <link
   xlink:href=
   "https://www.suse.com/media/flyer/suse_customer_support_quick_reference_guide_flyer.pdf"
   /> and <link xlink:href="https://www.suse.com/support/" />.
  </para>
 </sect1>


 <sect1 xml:id="sec-admin-troubleshooting-node-acceptance">
  <title>New Nodes Not Showing In &dashboard;</title>
  <para>
   All &productname; nodes must have a <filename>/etc/salt/minion.d/master.conf</filename>
   configuration file that contains the IP address or the FQDN of (one of) the
   Salt master(s) in the cluster. During the installation you must provide the
   IP/FQDN of the Salt master (typically the admin node).
  </para>
  <para>
   To become part of the cluster, the node must be (manually) accepted.
   &dashboard; shows a list of Salt minions that are known to the Salt master
   and can be accepted.
  </para>
  <sect2>
   <title>Nodes Not Showing In "Pending Nodes"</title>
   <para>
    You have added a new machine to the cluster but there is no new node key
    displayed in the <guimenu>Pending Nodes</guimenu> list.
   </para>

   <itemizedlist>
    <listitem>
     <para>
      Check that the configured value in the <filename>master.conf</filename> of
      the machine is correct and it is communicating with the admin node.
     </para>
    </listitem>
    <listitem>
     <para>
      Check if the &admin_node; has enough free memory. In some situations
      &kube; will intermittently shut down containers to conserve resources. If
      any of <literal>salt-master</literal>, <literal>salt-api</literal> or other
      critical services are affected, this could lead to this kind of issues.
     </para>
    </listitem>
   </itemizedlist>
  </sect2>

  <sect2>
   <title>Accepted Nodes Not Showing In "Nodes" List</title>
   <para>
    If the accepted node does not show up in the  <guimenu>Nodes</guimenu>list,
    you can log in to the &admin_node; via SSH and perform the following checks:
   </para>

  <itemizedlist>
   <listitem>
   <para>
    Check that the minion ID has been actually accepted:
   </para>
<screen>&prompt.user;<command>docker exec -it $(docker ps --filter=name=salt-master -q) salt-key</command>
Accepted Keys:
admin
ca
231bb41553ac4102bb8fda8b2fd9d60d
Denied Keys:
Unaccepted Keys:
bbaf54654d5649f68a0af5f1914ecedf
db79f30700974971b5e32d62ba0d5d76
Rejected Keys:
</screen>
    <para>
     The minion ID of the node should appear on the "Accepted Keys" list.
    </para>
    <para>
     If the node does not appear on this "Accepted Keys" list, the acceptance
     request didn't reach the backend and/or the Salt API. Check with
     <command>docker ps</command> that containers on the &admin_node; have
     approximately the same age. If there is a large age differences, it's likely
     that the &admin_node; is under memory pressure and is shutting down
     containers. This can lead to multiple problems respective of which
     container is not available. For example: MariaDB could have been down when
     Salt tried to insert events into the database etc..
    </para>
    <para>
     Adjust workloads accordingly to restore available memory and retry
     accepting the node from &dashboard;.
    </para>
   </listitem>
   <listitem>
    <para>
     Check that the event processor is running on the &admin_node;:
    </para>
<screen>&prompt.user;<command>docker ps -q -f name=event-processor</command></screen>
    <para>
     If it's not running or has a significantly different age from the other
     containers, make sure the admin node is not under memory pressure.
    </para>
   </listitem>
   <listitem>
    <para>
    Check if the event processor is lagging behind processing events:
    </para>
    <para>
    If there was a previous problem with the event processor, it's possible
    that salt continued creating events while it wasn't running for a long time,
    causing it to lag behind processing events. Check the number of events in the
    processing queue.
   </para>
<screen>&prompt.user;<command>docker exec -it $(docker ps -q -f name=dashboard) \
entrypoint.sh bundle exec rails runner \
'puts "Number of events to process: #{SaltEvent.not_processed.count}"'</command>
</screen>
   </listitem>
   <listitem>
    <para>
     Check if Salt couldn't introduce an important event on the database
    </para>

    <para>
     Make sure there's enough free space on the &admin_node;.
    </para>
   </listitem>
  </itemizedlist>
 </sect2>
 </sect1>

 <sect1 xml:id="sec-admin-troubleshooting-failed-bootstrap">
  <title>Debugging Failed Bootstrap</title>
  <para>
   If the bootstrapping as described in <xref
   linkend="sec-deploy-install-bootstrap" /> fails, there are several
   places to look for errors. The following procedure outlines where to
   start debugging.
  </para>
  <procedure>
   <step>
    <para>
     Check the logs from Velum dashboard. Execute on the &admin_node;:
    </para>
    <screen>&prompt.root.admin;<command>docker logs $(docker ps -q -f name="velum-dashboard")</command></screen>
   </step>
   <step>
    <para>
     Convert the log into a human readable format and open it with
     <command>less</command>.
    </para>
<screen>&prompt.root.admin;<command>/var/lib/supportutils-plugin-suse-caasp/debug-salt --json_output=salt.json \
    --summary_output=summary.txt --text-status --no-color</command>
&prompt.root.admin;<command>less summary.txt</command></screen>
   </step>
   <step>
    <para>
     Retry bootstrapping from &admin_node; console.
    </para>
<screen>&prompt.root.admin;<command>docker exec -it $(docker ps -q -f name="salt-master") \
    salt-run -l debug state.orchestrate orch.kubernetes > salt-orchestration.logs</command></screen>
   </step>
   <step>
    <para>
     If the orchestration failed in late state, you can check if
     <command>etcd</command> and the &kube; cluster is up and running.
     Log in on a &master_node; and check if <command>etcd</command>
     cluster is up and running.
    </para>
<screen>&prompt.root.master;<command>set -a</command>
&prompt.root.master;<command>source /etc/sysconfig/etcdctl</command>
&prompt.root.master;<command>export ETCDCTL_API=3</command>
&prompt.root.master;<command>etcdctl member list</command>
&prompt.root.master;<command>etcdctl endpoint health</command>
&prompt.root.master;<command>etcdctl endpoint status</command>
&prompt.root.master;<command>journalctl -u etcd</command></screen>
    <para>
     Check if &kube; cluster is up and running.
    </para>
<screen>&prompt.root.master;<command>kubectl get nodes</command>
&prompt.root.master;<command>kubectl cluster-info</command>
&prompt.root.master;<command>journalctl -u kubelet</command>
&prompt.root.master;<command>journalctl -u kube-apiserver</command>
&prompt.root.master;<command>journalctl -u kube-scheduler</command>
&prompt.root.master;<command>journalctl -u kube-controller-manager</command></screen>
   </step>
   <step>
    <para>
     Collect all relevant logs with <command>supportconfig</command> on
     the &master_node; and &admin_node;.
    </para>
<screen>&prompt.root.admin;<command>supportconfig</command>
&prompt.root.admin;<command>tar -xvjf /var/log/nts_*.tbz</command>
&prompt.root.admin;<command>cd nts*</command>
&prompt.root.admin;<command>cat etcd.txt kubernetes.txt salt-minion.txt</command></screen>
<screen>&prompt.root.master;<command>supportconfig</command>
&prompt.root.master;<command>tar -xvjf /var/log/nts_*.tbz</command>
&prompt.root.master;<command>cd nts*</command>
&prompt.root.master;<command>cat etcd.txt kubernetes.txt salt-minion.txt</command></screen>
   </step>
  </procedure>
 </sect1>

 <sect1 xml:id="sec-admin-troubleshooting-failed-update">
  <title>Recovering From Failed Update</title>
  <para>
   See <xref
   linkend="sec-admin-software-transactional-updates-recovering" />.
  </para>
 </sect1>

 <sect1 xml:id="sec-admin-troubleshooting-etcd">
  <title>Checking <literal>etcd</literal> Health</title>
  <para>
   To work with <command>etcdctl</command>, you have to pass parameters
   with paths to required certificates.
  </para>
  <para>
   Use SSH to log into one of the cluster nodes, for example your first master.
   The following command provides an example for using <command>etcdctl</command>.
  </para>
<screen>&prompt.root;<command>set -a</command>
&prompt.root;<command>source /etc/sysconfig/etcdctl</command>
&prompt.root;<command>etcdctl cluster-health</command></screen>
 </sect1>

 <sect1 xml:id="sec-admin-troubleshooting-velum-ptf">
  <title>Locking Installed Program Temporary Fixes</title>
  <para>
   It can happen that <command>zypper</command> removes an installed
   <emphasis>Program Temporary Fix</emphasis> (<emphasis>PTF</emphasis>)
   when updates are started with Velum. To prevent this, execute the
   following procedure.
  </para>
  <important>
   <title>Locks Disable Updates for Package</title>
   <para>
    If a package is locked, it will no longer receive any updates until
    the lock is released with <command>zypper rl</command>.
   </para>
  </important>
  <procedure>
   <step>
    <para>
     Install the PTF manually.
    </para>
<screen>&prompt.root;<command>transactional-update reboot pkg install <replaceable>PTF_FILE</replaceable>.rpm
</command></screen>
   </step>
   <step>
    <para>
     As soon as the node has restarted, verify that the RPM is installed.
    </para>
<screen>&prompt.root;<command>rpm -qa | grep PTF</command></screen>
   </step>
   <step>
    <para>
     Create a <command>zypper</command> lock for this RPM.
    </para>
<screen>&prompt.root;<command>zypper al <replaceable>PTF_PACKAGE_NAME</replaceable></command></screen>
   </step>
   <step>
    <para>
     Verify that the lock is created.
    </para>
<screen>&prompt.root;<command>zypper ll</command></screen>
   </step>
  </procedure>
 </sect1>

 <sect1 xml:id="sec-admin-troubleshooting-network-planning">
  <title>Network Planning And Reconfiguration</title>
  <para>
   It is not possible to reconfigure the used IP ranges of &productname;
   once the deployment is complete. Therefore, carefully plan for
   required IP ranges and future scenarios.
  </para>
  <para>
   Additionally, keep the network requirements in mind, as stated in
   <xref linkend="sec-deploy-requirements-network" />.
  </para>
 </sect1>

 <sect1 xml:id="sec-admin-troubleshooting-proxy">
  <title>Using A Proxy Server With Authentication</title>
  <para>
   If you need to register the &admin_node; with &scc; over a proxy
   server that requires authentication, it is not possible to specify
   the configuration in <filename>/etc/sysconfig/proxy</filename>.
  </para>
  <para>
   For information about setting authentication credentials and
   connecting to &scc; with <command>SUSEConnect</command>, see <xref
   linkend="sec-configuration-suseconnect-proxy" />.
  </para>
 </sect1>

 <sect1 xml:id="sec-admin-troubleshooting-replace-certificates">
  <title>Replacing TLS/SSL Certificates</title>
  <para>
   Sometimes certificates are not updated properly because they are
   outdated. To replace outdated certificates, execute the following
   procedure.
  </para>
  <procedure>
   <step>
    <para>
     Use SSH  and log in on the &admin_node;.
    </para>
   </step>
   <step>
    <para>
      Move the expired certs out of the way.
    </para>
    <screen>&prompt.root.admin;<command>mv /etc/pki/{velum,ldap,salt-api}.crt /root</command></screen>
   </step>
   <step>
    <para>
     Generate new certificates.
    </para>
<screen>&prompt.root.admin;<command>cd /etc/pki</command>
&prompt.root.admin;<command>/usr/share/caasp-container-manifests/gen-certs.sh</command></screen>
    <tip>
     <title>Generating Additional Certificates</title>
     <para>
      To regenerate additional certificates, for example
      <filename>/etc/pki/kubectl-client-cert.crt</filename>, add an
      additional line at the end of the
      <command>gen-certs.sh</command> script:
     </para>
<screen>&prompt.root.admin;<command>transactional-update shell</command>
<prompt>transactional update #</prompt> <command>echo "gencert \"kubectl-client-cert\" \"kubectl-client-cert\" \
    \"\$all_hostnames\" \"\$(ip_addresses)\"" >>/usr/share/caasp-container-manifests/gen-certs.sh</command>
<prompt>transactional update # </prompt><command>/usr/share/caasp-container-manifests/gen-certs.sh</command>
<prompt>transactional update # </prompt><command>exit</command>
&prompt.root.admin;<command>reboot</command>
</screen>
    </tip>
   </step>
   <step>
    <para>
     Use SSH and log in on a &master_node;.
    </para>
   </step>
   <step>
    <para>Backup and delete the dex-tls secret.</para>
<screen>&prompt.root.master;<command>kubectl -n kube-system get secret dex-tls -o yaml > /root/dex-tls</command>
&prompt.root.master;<command>kubectl -n kube-system delete secret dex-tls</command></screen>
   </step>
   <step>
    <para>
     On a master node, find and delete the Dex pods.
    </para>
    <important>
     <title>This Step Breaks Authentication</title>
     <para>
      Executing this step prevents new authentications requests from
      succeeding. However, the static credentials located on the master
      nodes will continue to function.
     </para>
     <para>
      The Dex pods will not restart by themselves until the dex-tls
      secret is recreated.
     </para>
    </important>
<screen>&prompt.root.master;<command>kubectl -n kube-system get pods | grep dex</command>
&prompt.root.master;<command>kubectl -n kube-system delete pods <replaceable>DEX_POD1</replaceable> <replaceable>DEX_POD2</replaceable> <replaceable>DEX_POD3</replaceable></command></screen>
   </step>
   <step>
    <para>
     Manually run the salt orchestration on the &admin_node;. This may
     take some time.
    </para>
<screen>&prompt.root.admin;<command>docker exec -it $(docker ps -q -f name="salt-master") \
    bash -c "salt-run state.orchestrate orch.kubernetes" 2&amp;>1 > salt-run.log</command></screen>
   </step>
   <step>
    <para>
     Check the tail of <filename>salt-run.log</filename> to see if the
     orchestration succeeded.
    </para>
    <screen>&prompt.root.admin;<command>tail -n 50 salt-run.log</command></screen>
   </step>
   <step>
    <para>
     On a master node, validate the dex pods are running.
    </para>
    <screen>&prompt.root.master;<command>kubectl -n kube-system get pods | grep dex</command></screen>
   </step>
   <step>
    <para>
     If you are not able to log in into Velum, reboot the &admin_node;.
     Then test and validate that the cluster is still functional.
    </para>
   </step>
  </procedure>
 </sect1>
 <sect1 xml:id="sec-admin-troubleshooting-add-ca-cert">
  <title>Fixing <quote>x509: certificate signed by unknown authority</quote></title>
  <para>
   When interacting with &kube; you can run into the situation where your
   existing configuration for the authentication has changed (cluster has been
   rebuild, certificates have been switched.)
  </para>
  <para>
   In such a case you might see an error message in the output of your CLI tool.
  </para>
<screen>x509: certificate signed by unknown authority</screen>
  <para>
   This message indicates that your current system does not know the Certificate
   Authority (CA) that signed the SSL certificates used for encrypting the
   communication to the cluster. You then need to add or update the Root CA
   certificate in your local trust store.
  </para>
  <para>
   For details about installing the root CA certificate, see <xref
   linkend="sec-admin-security-certs-installing-rootca" />.
  </para>
 </sect1>
 <sect1 xml:id="sec-admin-troubleshooting-lost-node">
  <title>Replacing a Lost Node</title>
  <para>
   If your cluster loses a node, for example due to failed hardware,
   remove the node as explained in <xref linkend="sec-admin-nodes-remove" />.
   Then add a new node as described in <xref linkend="sec-admin-nodes-add" />.
  </para>
 </sect1>
</chapter>
