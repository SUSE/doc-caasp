<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha.deploy.requirements"
 xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>System Requirements</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker/>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <para>
  This chapter specifies the requirements to install and operate
  &productname;. Before you begin the installation, please make sure
  your system meets all requirements listed below.
 </para>
 <sect1 xml:id="sec.deploy.requirements.system.cluster">
  <title>Cluster Size Requirements</title>
  <para>
   &productname; is a dedicated cluster operating system and only functions in a
   multi-node configuration. It requires a connected group of four or more
   physical or virtual machines.
  </para>
  <para>
   The minimum supported cluster size is four nodes: a single &admin_node;, one
   &master_node;, and two &worker_node;s.
  </para>
  <note>
   <title>Test and Proof-of-Concept Clusters</title>
   <para>
    It is possible to provision a three-node cluster with only a single
    &worker_node;, but this is not a supported configuration for deployment.
   </para>
  </note>
  <para>
   For improved performance, multiple &master_node;s are supported, but there
   must always be an odd number. For cluster reliability, when using multiple
   master nodes, some form of DNS load-balancing should be used.
  </para>
  <para>Any number of &worker_node;s may be added up to the maximum cluster
   size. For the current maximum supported number of nodes, please refer to
   the Release Notes on <link xlink:href="https://www.suse.com/releasenotes/"/>.
  </para>

  <sect2 xml:id="sec.deploy.requirements.system.cluster.salt_cluster_size">
   <title>Salt Cluster Sizing</title>
   <para>
    &productname; relies on SaltStack (or Salt for short) to automate various
    tasks. The actions are performed by so called "minions". A minion can handle
    a theoretical number of nodes at the same time. Salt assigns a configured
    number of "worker threads" to the minions. The number of available
    worker threads limits the overall size of the cluster.
    </para>
    <para>
     Up until a total cluster size of <literal>40</literal> nodes, no
     adjustments to the default configuration of &productname; need to be made.
    </para>
    <para>
     For clusters that consist of more than <literal>40</literal> nodes the
     number of Salt worker threads must be adjusted as follows:
    </para>

    <table>
     <title>Salt Cluster Sizing</title>
      <tgroup cols="2" align="center">
       <colspec colname="c1"/>
       <colspec colname="c2"/>
       <thead>
        <row>
         <entry>Cluster Size (nodes)</entry>
         <entry>Salt worker threads </entry>
        </row>
       </thead>
       <tbody>
        <row>
         <entry>&gt;40</entry>
         <entry>20</entry>
        </row>
        <row>
         <entry>&gt;60</entry>
         <entry>30</entry>
        </row>
        <row>
         <entry>&gt;75</entry>
         <entry>40</entry>
        </row>
        <row>
         <entry>&gt;85</entry>
         <entry>50</entry>
        </row>
        <row>
         <entry>&gt;95</entry>
         <entry>60</entry>
        </row>
       </tbody>
      </tgroup>
     </table>
     <para>
      As a rule of thumb, if the cluster grows above <literal>100</literal>
      nodes the number of worker threads should be at about two thirds of the
      overall number of cluster nodes.
     </para>
     <para>
      To adjust the number of Salt worker threads, refer to:
      <xref linkend="sec.admin.salt.worker_threads"/>.
      </para>
   </sect2>
 </sect1>

 <sect1 xml:id="sec.deploy.requirements.environment">
  <title>Supported Environments</title>
  <para>
   Regarding deployment scenarios, &suse; supports &productname; running in
   the following environments:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     &productname; only supports x86_64 hardware.
    </para>
   </listitem>
   <listitem>
    <para>
     Aside from this, the same hardware and virtualization platforms
     as &sle; 12 SP3 are supported. For a list of certified hardware,
     see <link xlink:href="https://www.suse.com/yessearch/" />.
    </para>
   </listitem>
   <listitem>
    <para>
     Virtualized&mdash;running under the following hypervisors:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       KVM
      </para>
      <itemizedlist>
       <listitem>
        <para>
         on &sle; 11 SP4
        </para>
       </listitem>
       <listitem>
        <para>
         on &sle; 12 SP1
        </para>
       </listitem>
       <listitem>
        <para>
         on &sle; 12 SP2
        </para>
       </listitem>
       <listitem>
        <para>
         on &sle; 12 SP3
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
     <listitem>
      <para>
       Xen
      </para>
      <itemizedlist>
       <listitem>
        <para>
         same host platforms as for KVM
        </para>
       </listitem>
       <listitem>
        <para>
         full virtualization
        </para>
       </listitem>
       <listitem>
        <para>
         paravirtualization
        </para>
       </listitem>
       <listitem>
        <para>
         Citrix XenServer 6.5
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
     <listitem>
      <para>
       &vmware;
      </para>
      <important>
       <title>Disable &vmware; Memory Ballooning</title>
       <para>
        When installing &productname; on &vmware; you must disable &vmware;'s
        memory ballooning feature. &vmware; has instructions on how to do this
        here: <link xlink:href="https://kb.vmware.com/s/article/1002586"></link>
       </para>
      </important>
      <para>
       When using pre-installed disk images, read <xref
       linkend="sec.deploy.preparation.disk_images.vmware" />. After
       bootstrapping the cluster, install the &vmware; tools. For
       details, see <xref linkend= "sec.deploy.install.vmware_tools" />.
      </para>
      <itemizedlist>
       <listitem>
        <para>
         ESX 5.5
        </para>
       </listitem>
       <listitem>
        <para>
         ESXi 6.0
        </para>
       </listitem>
       <listitem>
        <para>
         ESXi 6.5+
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
     <listitem>
      <para>
       Hyper-V
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Windows Server 2008 SP2+
        </para>
       </listitem>
       <listitem>
        <para>
         Windows Server 2008 R2 SP1+
        </para>
       </listitem>
       <listitem>
        <para>
         Windows Server 2012+
        </para>
       </listitem>
       <listitem>
        <para>
         Windows Server 2012 R2+
        </para>
       </listitem>
       <listitem>
        <para>
         Windows Server 2016
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
     <listitem>
      <para>
       Oracle VM 3.3
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     Private and Public Cloud Environments
    </para>
    <itemizedlist>
     <listitem>
      <para>
       &soc;Â 7
      </para>
     </listitem>
     <listitem>
      <para>
       Amazon AWS*
      </para>
     </listitem>
     <listitem>
      <para>
       Microsoft Azure*
      </para>
     </listitem>
     <listitem>
      <para>
       Google Compute Engine*
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
  </itemizedlist>
 </sect1>

 <sect1 xml:id="sec.deploy.requirements.storage">
  <title>Container Data Storage</title>
  <para>
   Storage can be provided using:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     &ses;
    </para>
   </listitem>
   <listitem>
    <para>
     NFS
    </para>
   </listitem>
   <listitem>
    <para>
     <command>hostpath</command>
    </para>
    <note>
     <title><literal>hostpath</literal> Storage</title>
     <para>
      Storage using <literal>hostpath</literal> is still supported, but by
      default it is disabled by <literal>PodSecurityPolicies</literal>.
     </para>
    </note>
   </listitem>
  </itemizedlist>
 </sect1>

 <sect1 xml:id="sec.deploy.requirements.hardware">
  <title>Minimum Node Specification</title>
  <para>
   Each node in the cluster must meet the following minimum specifications. All
   these specifications must be adjusted according to the expected load and type
   of deployments.
  </para>
  <variablelist>
   <varlistentry>
    <term>(v)CPU</term>
    <listitem>
     <itemizedlist>
      <listitem>
       <para>
        <literal>4 Core</literal> AMD64/Intel* EM64T processor
       </para>
      </listitem>
      <listitem>
       <para>
        32-bit processors are not supported
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Memory</term>
    <listitem>
     <itemizedlist>
      <listitem>
       <para>
        <literal>8 GB</literal>
       </para>
       <para>
        Although it may be possible to install &productname; with less memory
        than recommended, there is a high risk that the operating system will
        run out of memory and subsequently causes a cluster failure.
       </para>
       <note>
        <title>Swap partitions</title>
        <para>
         &kube; does not support swap.
        </para>
        <para>
         For technical reasons, an &admin_node; installed from an ISO image will
          have a small swap partition which will be disabled after installation.
          Nodes built using &ay; do not have a swap partition.
        </para>
       </note>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Storage Size</term>
    <listitem>
     <itemizedlist>
      <listitem>
       <para>
        <literal>40 GB</literal>
        for the root file system with Btrfs and enabled snapshots.
       </para>
       <note>
        <title>Cloud default root volume size</title>
        <para>
         In some Public Cloud frameworks the default root volume size of the
         images is smaller than 40GB. You must resize the root volume before
         instance launch using the command line tools or the web interface for
         the framework of your choice.
        </para>
       </note>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Storage Performance</term>
    <listitem>
     <itemizedlist>
      <listitem>
       <para>
        IOPS: <literal>500</literal> sequential IOPS
       </para>
      </listitem>
      <listitem>
       <para>
        Write Performance: <literal>10MB/s</literal>
       </para>
       <note>
        <title>etcd Storage requirements</title>
        <para>
         Storage performance requirements are tied closely to the
         <link xlink:href="https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/hardware.md#disks">etcd hardware recommendations</link>
        </para>
       </note>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>

 <sect1 xml:id="sec.deploy.requirements.network">
  <title>Network Requirements</title>
  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     All the nodes on the cluster must be on a the same network and be able to
     communicate directly with one another.
    </para>
    <important>
     <title>Reliable Networking</title>
     <para>
      Please make sure all nodes can communicate without interruptions.
     </para>
    </important>
   </listitem>
   <listitem>
    <para>
     The admin node and the &kube; API master must have valid Fully-Qualified
     Domain Names (FQDNs), which can be resolved both by all other nodes and
     from other networks which need to access the cluster.
    </para>
    <para>
     Admin node and &kube; API master node should be configured as CNAME records
     in the local DNS. This improves portability for disaster recovery.
    </para>
   </listitem>
   <listitem>
    <para>
     A DHCP server to dynamically provide IP addresses and host names for the
     nodes in your cluster (unless you configure all nodes statically).
    </para>
   </listitem>
   <listitem>
    <para>
     A DNS server to resolve host names. If you are using host names to
     specify nodes, please make sure you have reliable DNS resolution at all
     times, especially in combination with DHCP.
    </para>
    <important>
     <title>Unique Host Names</title>
     <para>
      Host names must be unique. It is recommended to let the DHCP server
      provide not only IP addresses but also host names of the cluster nodes.
     </para>
    </important>
   </listitem>
   <listitem>
    <para>
     On the same network, a separate computer with a Web browser is required
     in order to complete bootstrap of the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     We recommend that &productname; is setup to run in two subnets in
     one network segment, also referred to as VPC or VNET. The
     &admin_node; should run in a subnet that is not accessible to the
     outside world and should be connected to your network via VPN or
     other means. Consider a security group/firewall that only allows
     ingress traffic on ports 22 (SSH) and 443 (https) for the
     Administrative node from outside the VPC. All nodes must have
     access to the Internet through some route in order to connect to
     &scc; and receive updates, or be otherwise configured to receive
     updates, for example through &smt;.
    </para>
    <para>
     Depending on the applications running in your cluster you may
     consider exposing the subnet for the cluster nodes to the outside
     world. Use a security group/firewall that only allows incoming
     traffic on ports served by your workload. For example, a
     containerized application providing the backend for REST based
     services with content served over https should only allow ingress
     traffic on port 443.
    </para>
   </listitem>
   <listitem>
    <para>
     In a &productname; cluster, internal TCP/IP ports are managed using
     <literal>iptables</literal> controlled by <literal>Salt</literal> and so
     need not be manually configured. However, for reference and for
     environments where there are existing security policies, the following
     are the standard ports in use.
    </para>
    <table xml:id="tab.deploy.requirements.ports">
     <title>Node types and open ports</title>
     <tgroup cols="4">
      <thead>
       <row>
        <entry>
         <para>
          Node
         </para>
        </entry>
        <entry>
         <para>
          Port
         </para>
        </entry>
        <entry>
         <para>
          Accessibility<superscript>1</superscript>
         </para>
        </entry>
        <entry>
         <para>
          Description
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>
         <para>
          All nodes
         </para>
        </entry>
        <entry>
         <para>
          22
         </para>
        </entry>
        <entry>
         <para>
          Internal
         </para>
        </entry>
        <entry>
         <para>
          SSH (required in public clouds)
         </para>
        </entry>
       </row>
       <row>
        <entry morerows="4" valign="middle">
         <para>
          Admin
         </para>
        </entry>
        <entry>
         <para>
          80
         </para>
        </entry>
        <entry>
         <para>
          Internal
         </para>
        </entry>
        <entry>
         <para>
          HTTP (Only used for &ay;)
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          389
         </para>
        </entry>
        <entry>
         <para>
          External
         </para>
        </entry>
        <entry>
         <para>
          LDAP (user management)
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          443
         </para>
        </entry>
        <entry>
         <para>
          External
         </para>
        </entry>
        <entry>
         <para>
          HTTPS
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          2379
         </para>
        </entry>
        <entry>
         <para>
          Internal
         </para>
        </entry>
        <entry>
         <para>
          <literal>etcd</literal> discovery
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          4505 - 4506
         </para>
        </entry>
        <entry>
         <para>
          Internal
         </para>
        </entry>
        <entry>
         <para>
          Salt
         </para>
        </entry>
       </row>
       <row>
        <entry morerows="5" valign="middle">
         <para>
          Masters
         </para>
        </entry>
        <entry>
         <para>
          2379 - 2380
         </para>
        </entry>
        <entry>
         <para>
          Internal
         </para>
        </entry>
        <entry>
         <para>
          <literal>etcd</literal> (peer-to-peer traffic)
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          6443 - 6444
         </para>
        </entry>
        <entry>
         <para>
          Both
         </para>
        </entry>
        <entry>
         <para>
          &kube; API server
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          8471 - 8472
         </para>
        </entry>
        <entry>
         <para>
          Internal
         </para>
        </entry>
        <entry>
         <para>
          VXLAN traffic (used by Flannel)
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          10250, 20255
         </para>
        </entry>
        <entry>
         <para>
          Internal
         </para>
        </entry>
        <entry>
         <para>
          Kubelet
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          10256
         </para>
        </entry>
        <entry>
         <para>
          Internal
         </para>
        </entry>
        <entry>
         <para>
          kube-proxy
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          32000
         </para>
        </entry>
        <entry>
         <para>
          External
         </para>
        </entry>
        <entry>
         <para>
          Dex (OIDC Connect)
         </para>
        </entry>
       </row>
       <row>
        <entry morerows="5" valign="middle">
         <para>
          Workers
         </para>
        </entry>
        <entry>
         <para>
          2379 - 2380
         </para>
        </entry>
        <entry>
         <para>
          Internal
         </para>
        </entry>
        <entry>
         <para>
          <literal>etcd</literal> (peer-to-peer traffic)
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          4149
         </para>
        </entry>
        <entry>
         <para>
          Internal
         </para>
        </entry>
        <entry>
         <para>
          Kubelet
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          8471 - 8472
         </para>
        </entry>
        <entry>
         <para>
          Internal
         </para>
        </entry>
        <entry>
         <para>
          VXLAN traffic (used by Flannel)
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          10250, 10255
         </para>
        </entry>
        <entry>
         <para>
          Internal
         </para>
        </entry>
        <entry>
         <para>
          Kubelet
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          10256
         </para>
        </entry>
        <entry>
         <para>
          Internal
         </para>
        </entry>
        <entry>
         <para>
          kube-proxy
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          32000
         </para>
        </entry>
        <entry>
         <para>
          External
         </para>
        </entry>
        <entry>
         <para>
          Dex (OIDC Connect)
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </table>
   </listitem>
  </itemizedlist>
  <para>
   1) Information about whether the port is used by
   <emphasis>internal</emphasis> cluster nodes or
   <emphasis>external</emphasis> networks or hosts.
  </para>
  <para>
   When some additional ingress mechanism is used, additional ports would also
   be open.
  </para>
 </sect1>

 <sect1 xml:id="sec.deploy.requirements.public_cloud">
  <title>Public Cloud Requirements</title>
  <variablelist>
   <varlistentry>
    <term>Amazon AWS*</term>
    <listitem>
     <para>
      The adminstrative instance must be launched with an IAM role that
      allows full access to the EC2 API.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Microsoft Azure*</term>
    <listitem>
     <para>
      All security credentials will be collected during setup.
     </para>
     <para>
      Microsoft Azure does not provide a time protocol service. Please
      refer to <link xlink:href=
      "https://www.suse.com/documentation/sles-12/book_sle_admin/data/cha_netz_xntp.html"
      > &sls; documentation</link> for more information about NTP
      configuration. No manual NTP configuration is required on the
      cluster nodes, they synchronize time with the &admin_node;.
      </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Google Compute Engine*</term>
    <listitem>
     <para>
      The instance must be launched with an IAM role including
      <literal>Compute Admin</literal> and <literal>Service Account
      Actor</literal> scopes.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>

 <sect1 xml:id="sec.deploy.requirements.limits">
  <title>Limitations</title>
  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
<!-- cwickert 2017-04-06: FIXME - check if still applies
      https://bugzilla.suse.com/show_bug.cgi?id=1029317
      jahalackova: that's still valid for 1.0.
      -->
    <para>
     &productname; &productnumber; does not support remote installations with
     <literal>Virtual Network Computing (VNC)</literal>.
    </para>
   </listitem>
   <listitem>
    <para>
     &productname; is a dedicated cluster operating system and does not
     support dual-booting with other operating systems. Ensure that all drives
     in all cluster nodes are empty and contain no other operating systems
     before beginning installation.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
</chapter>
