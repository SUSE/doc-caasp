<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha.deploy.requirements"
 xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>System Requirements</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker/>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <para>
  This chapter specifies the requirements to install and operate
  &productname;. Before you begin the installation, please make sure
  your system meets all requirements listed below.
 </para>
 <sect1 xml:id="sec.deploy.requirements.system.cluster">
  <title>Cluster Size Requirements</title>
  <para>
   &productname; is a dedicated cluster operating system and only functions in a
   multi-node configuration. It requires a connected group of four or more
   physical or virtual machines.
  </para>
  <para>
   The minimum supported cluster size is four nodes: a single &admin_node;, one
   &master_node;, and two &worker_node;s.
  </para>
  <important>
   <title>Cluster Update Reserves</title>
   <para>
    You should always size your cluster to accomodate your expected workloads and
    contingency resources for updates. During updates, worker nodes will become
    temporarily unavailable to process workloads and their assigned load must
    be shifted somewhere else in the cluster.
   </para>
   <para>
    As a general rule of thumb: Your cluster utilization should not exceed 70%
    and must not exceed 80%.
   </para>
   <para>
    You can make use of <link xlink:href="https://kubernetes.io/docs/concepts/policy/resource-quotas/">Resource Quotas</link>
    to limit the resources user workloads can take up in the cluster.
   </para>
  </important>
  <note>
   <title>Test and Proof-of-Concept Clusters</title>
   <para>
    It is possible to provision a three-node cluster with only a single
    &worker_node;, but this is not a supported configuration for deployment.
   </para>
  </note>
  <para>
   For improved performance, multiple &master_node;s are supported, but there
   must always be an odd number. For cluster reliability, when using multiple
   master nodes, some form of DNS load-balancing should be used.
  </para>
  <para>Any number of &worker_node;s may be added up to the maximum cluster
   size. For the current maximum supported number of nodes, please refer to
   the Release Notes on <link xlink:href="https://www.suse.com/releasenotes/"/>.
  </para>

  <sect2 xml:id="sec.deploy.requirements.system.cluster.etcd_cluster_size">
   <title>etcd Cluster Sizing</title>
   <para>
    <literal>etcd</literal> is a distributed key-value store. Some
    nodes on the cluster run <literal>etcd</literal> members that
    sync with other peers in order to provide a fault-tolerant storage
    that &kube; uses for persistence.
   </para>
   <para>
    <literal>etcd</literal> is the central component where &kube; reads and
    writes in order to have global knowledge about the cluster status
    and desired state.
   </para>
   <para>
    It's very important to note that <literal>etcd</literal>
    automatically recovers from temporary failures like machine
    reboots.
   </para>
   <para>
    <literal>etcd</literal> knows how many peers conform the
    <literal>etcd</literal> cluster; based on this information the
    <literal>etcd</literal> cluster can be in three different states:
    healthy, degraded or unavailable.
   </para>
   <variablelist>
    <varlistentry>
     <term>Healthy</term>
     <listitem>
      <para>
       All <literal>etcd</literal> members are working as expected.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Degraded</term>
     <listitem>
      <para>
       Some <literal>etcd</literal> members are not working as
       expected, but there's still a majority in the working ones. This
       still means the cluster is working, because it has quorum.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Unavailable</term>
     <listitem>
      <para>
       There is no working majority of peers. The cluster is not
       available and cannot be used because the quorum is lost.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    In order to maintain functionality of the <literal>etcd</literal> cluster,
    the number of Master nodes must always maintain quorum. This means there
    always must be a majority of nodes available to "overrule" a possible
    minority caused by network partitioning or other failure. Therefore, you
    should always run an odd number of Master nodes in your cluster.
   </para>
   <para>
    This ensures that there is a sufficiently lopsided majority. The number of
    Master nodes that can be lost without causing loss of functionality depends
    on this ratio.
   </para>
   <table>
    <title>etcd Cluster Sizing</title>
    <tgroup cols="3" align="left">
     <colspec colname="c1"/>
     <colspec colname="c2"/>
     <colspec colname="c3"/>
     <thead>
      <row>
       <entry align="center">Master Nodes</entry>
       <entry align="center">Quorum Limit</entry>
       <entry align="center">Fault Tolerance</entry>
      </row>
     </thead>
     <tbody>
      <row>
       <entry>1</entry>
       <entry>1</entry>
       <entry>0</entry>
      </row>
      <row>
       <entry>3</entry>
       <entry>2</entry>
       <entry>1</entry>
      </row>
      <row>
       <entry>5</entry>
       <entry>3</entry>
       <entry>2</entry>
      </row>
      <row>
       <entry>7</entry>
       <entry>4</entry>
       <entry>3</entry>
      </row>
      <row>
       <entry>9</entry>
       <entry>5</entry>
       <entry>4</entry>
      </row>
     </tbody>
    </tgroup>
   </table>
   <para>
   Whether <literal>etcd</literal> is available or not depends on how many
   etcd members are available/not available at a given
   moment. It is important to differentiate between transient and
   permanent failures. Transient failures happen when a member is
   temporarily not available, for example when a machine running one
   etcd member is rebooting. Permanent failures
   happen when a member was irrevocably lost, for example a machine
   hard disk failure. The etcd cluster can tolerate
   up to (N - 1) / 2 permanent failures, where N is the number of
   etcd members; a subset of masters and possibly
   workers. The number of etcd nodes must always maintain
   <literal>Majority</literal> quorum.
  </para>
  <para>
   <literal>Majority</literal> means that the number of available etcd cluster
   members must never be lower or equal to the number of unavailable nodes.
   If, for example, you have only <literal>1</literal> or <literal>2</literal>
   etcd members, the cluster has a fault tolerance of <literal>0</literal>
   because <literal>0</literal> nodes can be faulty for the cluster to maintain
   <literal>Majority</literal>.
  </para>
  <para>
   When the <literal>etcd</literal> cluster actually loses quorum, it will
   enter a "read-only" state and will refuse to work. This will cause
   scheduling of new pods and workloads to fail. You must always consider
   maintaining quorum on adding/removing Master nodes to/from the cluster.
  </para>
  <para>
   If you have <literal>6</literal> nodes, a maximum of <literal>2</literal>
   nodes can become faulty for the cluster to remain in degraded but working
   state. If <literal>3</literal> or more nodes fail, there is no longer a
   majority of nodes working, therefore the cluster becomes unavailable.
  </para>
  <para>
   For example: The fault tolerance of a cluster with <literal>7</literal>
   nodes is <literal>3</literal>, because you need at least <literal>4</literal>
   active nodes to maintain majority.
  </para>
   <para>
    When (N - 1) / 2 or fewer permanent failures happen in a given
    <literal>etcd</literal> cluster, the cluster still has a quorum. It
    is then possible to remove the faulty members and add new ones. The
    new members will synchronize with the existing ones. This does not
    require an explicit backup/restore procedure, as it is normal
    etcd operation.
   </para>
   <para>
    When more than (N - 1) / 2 permanent failures happen in a given
    <literal>etcd</literal> cluster, the quorum is lost irrevocably.
    That means that there is no way to recover from that situation,
    because it is no longer possible to remove faulty members or add
    new members. In this case, it is necessary to start a new
    etcd cluster and grow it.
   </para>
  </sect2>

  <sect2 xml:id="sec.deploy.requirements.system.cluster.salt_cluster_size">
   <title>Salt Cluster Sizing</title>
   <para>
    &productname; relies on SaltStack (or Salt for short) to automate various
    tasks. The actions are performed by so called "minions". A master can handle
    a theoretical number of minions at the same time. Salt assigns a configured
    number of "worker threads" to the minions. The number of available
    worker threads limits the overall size of the cluster.
   </para>
   <para>
    You must adjust the sizing of your &admin_node; RAM to allow for more
    minions/nodes in the cluster. 32GB can run up to 150 nodes.
    Please refer to: <xref linkend="sec.deploy.requirements.hardware"/>.
   </para>
   <para>
    Up until a total cluster size of <literal>40</literal> nodes, no
    adjustments to the default configuration of &productname; need to be made.
   </para>
   <para>
    For clusters that consist of more than <literal>40</literal> nodes the
    number of Salt worker threads must be adjusted as follows:
   </para>
   <table>
    <title>Salt Cluster Sizing</title>
     <tgroup cols="2" align="center">
      <colspec colname="c1"/>
      <colspec colname="c2"/>
      <thead>
       <row>
        <entry>Cluster Size (nodes)</entry>
        <entry>Salt worker threads </entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>&gt;40</entry>
        <entry>20</entry>
       </row>
       <row>
        <entry>&gt;60</entry>
        <entry>30</entry>
       </row>
       <row>
        <entry>&gt;75</entry>
        <entry>40</entry>
       </row>
       <row>
        <entry>&gt;85</entry>
        <entry>50</entry>
       </row>
       <row>
        <entry>&gt;95</entry>
        <entry>60</entry>
       </row>
      </tbody>
     </tgroup>
    </table>
    <para>
     As a rule of thumb, if the cluster grows above <literal>100</literal>
     nodes the number of worker threads should be at about two thirds of the
     overall number of cluster nodes.
    </para>
    <para>
     To adjust the number of Salt worker threads, refer to:
     <xref linkend="sec.admin.salt.worker_threads"/>.
     </para>
  </sect2>
 </sect1>

 <sect1 xml:id="sec.deploy.requirements.environment">
  <title>Supported Environments</title>
  <para>
   Regarding deployment scenarios, &suse; supports &productname; running in
   the following environments:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     &productname; only supports x86_64 hardware.
    </para>
   </listitem>
   <listitem>
    <para>
     Aside from this, the same hardware and virtualization platforms
     as &sle; 12 SP3 are supported. For a list of certified hardware,
     see <link xlink:href="https://www.suse.com/yessearch/" />.
    </para>
   </listitem>
   <listitem>
    <para>
     Virtualized&mdash;running under the following hypervisors:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       KVM
      </para>
      <itemizedlist>
       <listitem>
        <para>
         on &sle; 11 SP4
        </para>
       </listitem>
       <listitem>
        <para>
         on &sle; 12 SP1
        </para>
       </listitem>
       <listitem>
        <para>
         on &sle; 12 SP2
        </para>
       </listitem>
       <listitem>
        <para>
         on &sle; 12 SP3
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
     <listitem>
      <para>
       Xen
      </para>
      <itemizedlist>
       <listitem>
        <para>
         same host platforms as for KVM
        </para>
       </listitem>
       <listitem>
        <para>
         full virtualization
        </para>
       </listitem>
       <listitem>
        <para>
         paravirtualization
        </para>
       </listitem>
       <listitem>
        <para>
         Citrix XenServer 6.5
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
     <listitem>
      <para>
       &vmware;
      </para>
      <important>
       <title>Disable &vmware; Memory Ballooning</title>
       <para>
        When installing &productname; on &vmware; you must disable &vmware;'s
        memory ballooning feature. &vmware; has instructions on how to do this
        here: <link xlink:href="https://kb.vmware.com/s/article/1002586"></link>
       </para>
      </important>
      <para>
       When using pre-installed disk images, read <xref
       linkend="sec.deploy.preparation.disk_images.vmware" />. After
       bootstrapping the cluster, install the &vmware; tools. For
       details, see <xref linkend= "sec.deploy.install.vmware_tools" />.
      </para>
      <itemizedlist>
       <listitem>
        <para>
         ESX 5.5
        </para>
       </listitem>
       <listitem>
        <para>
         ESXi 6.0
        </para>
       </listitem>
       <listitem>
        <para>
         ESXi 6.5+
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
     <listitem>
      <para>
       Hyper-V
      </para>
      <itemizedlist>
       <listitem>
        <para>
         Windows Server 2008 SP2+
        </para>
       </listitem>
       <listitem>
        <para>
         Windows Server 2008 R2 SP1+
        </para>
       </listitem>
       <listitem>
        <para>
         Windows Server 2012+
        </para>
       </listitem>
       <listitem>
        <para>
         Windows Server 2012 R2+
        </para>
       </listitem>
       <listitem>
        <para>
         Windows Server 2016
        </para>
       </listitem>
      </itemizedlist>
     </listitem>
     <listitem>
      <para>
       Oracle VM 3.3
      </para>
     </listitem>
     <listitem>
      <para>
       Nutanix Acropolis
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
   <listitem>
    <para>
     Private and Public Cloud Environments
    </para>
    <itemizedlist>
     <listitem>
      <para>
       &soc; 7
      </para>
     </listitem>
     <listitem>
      <para>
       Amazon AWS*
      </para>
     </listitem>
     <listitem>
      <para>
       Microsoft Azure*
      </para>
     </listitem>
     <listitem>
      <para>
       Google Compute Engine*
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
  </itemizedlist>
 </sect1>

 <sect1 xml:id="sec.deploy.requirements.storage">
  <title>Container Data Storage</title>
  <para>
   Storage can be provided using:
  </para>
  <itemizedlist>
   <listitem>
    <para>
     &ses;
    </para>
   </listitem>
   <listitem>
    <para>
     NFS
    </para>
   </listitem>
   <listitem>
    <para>
     <command>hostpath</command>
    </para>
    <note>
     <title><literal>hostpath</literal> Storage</title>
     <para>
      Storage using <literal>hostpath</literal> is still supported, but by
      default it is disabled by <literal>PodSecurityPolicies</literal>.
     </para>
    </note>
   </listitem>
  </itemizedlist>
 </sect1>

 <sect1 xml:id="sec.deploy.requirements.hardware">
  <title>Minimum Node Specification</title>
  <para>
   Each node in the cluster must meet the following minimum specifications. All
   these specifications must be adjusted according to the expected load and type
   of deployments.
  </para>
  <variablelist>
   <varlistentry>
    <term>(v)CPU</term>
    <listitem>
     <itemizedlist>
      <listitem>
       <para>
        <literal>4 Core</literal> AMD64/Intel* EM64T processor
       </para>
      </listitem>
      <listitem>
       <para>
        32-bit processors are not supported
       </para>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Memory</term>
    <listitem>
     <itemizedlist>
      <listitem>
       <para>
        <literal>8 GB</literal>
       </para>
       <para>
        Although it may be possible to install &productname; with less memory
        than recommended, there is a high risk that the operating system will
        run out of memory and subsequently causes a cluster failure.
       </para>
       <important>
        <title>Cluster Sizing Considerations</title>
        <para>
         For the &admin_node; the size of RAM must be planned to accomodate the
         expected cluster size. The minimum configuration with 8GB of RAM will
         allow up to 40 worker nodes. 32GB will allow up to 150 workers. 150 is
         the maximum number of nodes supported with any configuration.
        </para>
       </important>

       <note>
        <title>Swap partitions</title>
        <para>
         &kube; does not support swap.
        </para>
        <para>
         For technical reasons, an &admin_node; installed from an ISO image will
          have a small swap partition which will be disabled after installation.
          Nodes built using &ay; do not have a swap partition.
        </para>
       </note>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Storage Size</term>
    <listitem>
     <itemizedlist>
      <listitem>
       <para>
        <literal>40 GB</literal>
        for the root file system with Btrfs and enabled snapshots.
       </para>
       <note>
        <title>Cloud default root volume size</title>
        <para>
         In some Public Cloud frameworks the default root volume size of the
         images is smaller than 40GB. You must resize the root volume before
         instance launch using the command line tools or the web interface for
         the framework of your choice.
        </para>
       </note>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Storage Performance</term>
    <listitem>
     <itemizedlist>
      <listitem>
       <para>
        IOPS: <literal>500</literal> sequential IOPS
       </para>
      </listitem>
      <listitem>
       <para>
        Write Performance: <literal>10MB/s</literal>
       </para>
       <note>
        <title>etcd Storage requirements</title>
        <para>
         Storage performance requirements are tied closely to the
         <link xlink:href="https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/hardware.md#disks">etcd hardware recommendations</link>
        </para>
       </note>
      </listitem>
     </itemizedlist>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>

 <sect1 xml:id="sec.deploy.requirements.network">
  <title>Network Requirements</title>
  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
    <para>
     All the nodes on the cluster must be on a the same network and be able to
     communicate directly with one another.
    </para>
    <important>
     <title>Reliable Networking</title>
     <para>
      Please make sure all nodes can communicate without interruptions.
     </para>
    </important>
   </listitem>
   <listitem>
    <para>
     All nodes in the cluster must be assigned static IP addresses. Using
     dynamically assigned IPs will break cluster functionality after
     update/reboot.
    </para>
   </listitem>
   <listitem>
    <para>
     The admin node and the &kube; API master must have valid Fully-Qualified
     Domain Names (FQDNs), which can be resolved both by all other nodes and
     from other networks which need to access the cluster.
    </para>
    <important>
     <title>Hostname 64 Character Limit</title>
     <para>
      &productname; generates SSL certificates for the cluster.
      The <literal>Common Name (CN)</literal> field is derived from the
      supplied hostnames. The <literal>CN</literal> field is limited to 64
      characters.
     </para>
     <para>
      Your hostnames must not exceed this limit or they will fail to be added
      to the cluster.
     </para>
    </important>
    <para>
     Admin node and &kube; API master node should be configured as CNAME records
     in the local DNS. This improves portability for disaster recovery.
    </para>
   </listitem>
   <listitem>
    <para>
     A DNS server to resolve host names. If you are using host names to
     specify nodes, please make sure you have reliable DNS resolution at all
     times, especially in combination with DHCP.
    </para>
    <important>
     <title>Unique Host Names</title>
     <para>
      Host names must be unique. It is recommended to let the DHCP server
      provide not only IP addresses but also host names of the cluster nodes.
     </para>
    </important>
   </listitem>
   <listitem>
    <para>
     On the same network, a separate computer with a Web browser is required
     in order to complete bootstrap of the cluster.
    </para>
   </listitem>
   <listitem>
    <para>
     We recommend that &productname; is setup to run in two subnets in
     one network segment, also referred to as VPC or VNET. The
     &admin_node; should run in a subnet that is not accessible to the
     outside world and should be connected to your network via VPN or
     other means. Consider a security group/firewall that only allows
     ingress traffic on ports 22 (SSH) and 443 (https) for the
     Administrative node from outside the VPC. All nodes must have
     access to the Internet through some route in order to connect to
     &scc; and receive updates, or be otherwise configured to receive
     updates, for example through &smt;.
    </para>
    <para>
     Depending on the applications running in your cluster you may
     consider exposing the subnet for the cluster nodes to the outside
     world. Use a security group/firewall that only allows incoming
     traffic on ports served by your workload. For example, a
     containerized application providing the backend for REST based
     services with content served over https should only allow ingress
     traffic on port 443.
    </para>
   </listitem>
   <listitem>
    <para>
     In a &productname; cluster, internal TCP/IP ports are managed using
     <literal>iptables</literal> controlled by <literal>Salt</literal> and so
     need not be manually configured. However, for reference and for
     environments where there are existing security policies, the following
     are the standard ports in use.
    </para>
    <table xml:id="tab.deploy.requirements.ports">
     <title>Node types and open ports</title>
     <tgroup cols="4">
      <thead>
       <row>
        <entry>
         <para>
          Node
         </para>
        </entry>
        <entry>
         <para>
          Port
         </para>
        </entry>
        <entry>
         <para>
          Accessibility<superscript>1</superscript>
         </para>
        </entry>
        <entry>
         <para>
          Description
         </para>
        </entry>
       </row>
      </thead>
      <tbody>
       <row>
        <entry>
         <para>
          All nodes
         </para>
        </entry>
        <entry>
         <para>
          22
         </para>
        </entry>
        <entry>
         <para>
          Internal
         </para>
        </entry>
        <entry>
         <para>
          SSH (required in public clouds)
         </para>
        </entry>
       </row>
       <row>
        <entry morerows="4" valign="middle">
         <para>
          Admin
         </para>
        </entry>
        <entry>
         <para>
          80
         </para>
        </entry>
        <entry>
         <para>
          Internal
         </para>
        </entry>
        <entry>
         <para>
          HTTP (Only used for &ay;)
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          389
         </para>
        </entry>
        <entry>
         <para>
          External
         </para>
        </entry>
        <entry>
         <para>
          LDAP (user management)
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          443
         </para>
        </entry>
        <entry>
         <para>
          External
         </para>
        </entry>
        <entry>
         <para>
          HTTPS
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          2379
         </para>
        </entry>
        <entry>
         <para>
          Internal
         </para>
        </entry>
        <entry>
         <para>
          <literal>etcd</literal> discovery
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          4505 - 4506
         </para>
        </entry>
        <entry>
         <para>
          Internal
         </para>
        </entry>
        <entry>
         <para>
          Salt
         </para>
        </entry>
       </row>
       <row>
        <entry morerows="5" valign="middle">
         <para>
          Masters
         </para>
        </entry>
        <entry>
         <para>
          2379 - 2380
         </para>
        </entry>
        <entry>
         <para>
          Internal
         </para>
        </entry>
        <entry>
         <para>
          <literal>etcd</literal> (peer-to-peer traffic)
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          6443 - 6444
         </para>
        </entry>
        <entry>
         <para>
          Both
         </para>
        </entry>
        <entry>
         <para>
          &kube; API server
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          8471 - 8472
         </para>
        </entry>
        <entry>
         <para>
          Internal
         </para>
        </entry>
        <entry>
         <para>
          VXLAN traffic (used by Flannel)
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          10250, 20255
         </para>
        </entry>
        <entry>
         <para>
          Internal
         </para>
        </entry>
        <entry>
         <para>
          Kubelet
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          10256
         </para>
        </entry>
        <entry>
         <para>
          Internal
         </para>
        </entry>
        <entry>
         <para>
          kube-proxy
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          32000
         </para>
        </entry>
        <entry>
         <para>
          External
         </para>
        </entry>
        <entry>
         <para>
          Dex (OIDC Connect)
         </para>
        </entry>
       </row>
       <row>
        <entry morerows="5" valign="middle">
         <para>
          Workers
         </para>
        </entry>
        <entry>
         <para>
          2379 - 2380
         </para>
        </entry>
        <entry>
         <para>
          Internal
         </para>
        </entry>
        <entry>
         <para>
          <literal>etcd</literal> (peer-to-peer traffic)
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          4149
         </para>
        </entry>
        <entry>
         <para>
          Internal
         </para>
        </entry>
        <entry>
         <para>
          Kubelet
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          8471 - 8472
         </para>
        </entry>
        <entry>
         <para>
          Internal
         </para>
        </entry>
        <entry>
         <para>
          VXLAN traffic (used by Flannel)
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          10250, 10255
         </para>
        </entry>
        <entry>
         <para>
          Internal
         </para>
        </entry>
        <entry>
         <para>
          Kubelet
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          10256
         </para>
        </entry>
        <entry>
         <para>
          Internal
         </para>
        </entry>
        <entry>
         <para>
          kube-proxy
         </para>
        </entry>
       </row>
       <row>
        <entry>
         <para>
          32000
         </para>
        </entry>
        <entry>
         <para>
          External
         </para>
        </entry>
        <entry>
         <para>
          Dex (OIDC Connect)
         </para>
        </entry>
       </row>
      </tbody>
     </tgroup>
    </table>
   </listitem>
  </itemizedlist>
  <para>
   1) Information about whether the port is used by
   <emphasis>internal</emphasis> cluster nodes or
   <emphasis>external</emphasis> networks or hosts.
  </para>
  <para>
   When some additional ingress mechanism is used, additional ports would also
   be open.
  </para>
 </sect1>

 <sect1 xml:id="sec.deploy.requirements.public_cloud">
  <title>Public Cloud Requirements</title>
  <variablelist>
   <varlistentry>
    <term>Amazon AWS*</term>
    <listitem>
     <para>
      The adminstrative instance must be launched with an IAM role that
      allows full access to the EC2 API.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Microsoft Azure*</term>
    <listitem>
     <para>
      All security credentials will be collected during setup.
     </para>
     <para>
      Microsoft Azure does not provide a time protocol service. Please
      refer to <link xlink:href=
      "https://www.suse.com/documentation/sles-12/book_sle_admin/data/cha_netz_xntp.html"
      > &sls; documentation</link> for more information about NTP
      configuration. No manual NTP configuration is required on the
      cluster nodes, they synchronize time with the &admin_node;.
      </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>Google Compute Engine*</term>
    <listitem>
     <para>
      The instance must be launched with an IAM role including
      <literal>Compute Admin</literal> and <literal>Service Account
      Actor</literal> scopes.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>
 </sect1>

 <sect1 xml:id="sec.deploy.requirements.limits">
  <title>Limitations</title>
  <itemizedlist mark="bullet" spacing="normal">
   <listitem>
<!-- cwickert 2017-04-06: FIXME - check if still applies
      https://bugzilla.suse.com/show_bug.cgi?id=1029317
      jahalackova: that's still valid for 1.0.
      -->
    <para>
     &productname; &productnumber; does not support remote installations with
     <literal>Virtual Network Computing (VNC)</literal>.
    </para>
   </listitem>
   <listitem>
    <para>
     &productname; is a dedicated cluster operating system and does not
     support dual-booting with other operating systems. Ensure that all drives
     in all cluster nodes are empty and contain no other operating systems
     before beginning installation.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
</chapter>
