<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha.admin.monitoring"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Monitoring</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker/>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <sect1 xml:id="sec.admin.monitoring.cluster">
  <title>Monitoring Concepts</title>

  <para>
   There are two basic ways how you can monitor your cluster:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     By directly accessing the <emphasis>cAdvisor</emphasis> on
     <literal>http://<replaceable>WORKER NODE
     ADDRESS</replaceable>:4194/containers/</literal>.
    </para>
    <para>
     The <emphasis>cAdvisor</emphasis> runs on all worker nodes by default.
    </para>
   </listitem>
   <listitem>
    <para>
     By using <emphasis>prometheus</emphasis> and <emphasis>Grafana</emphasis>,
     for details refer to <xref linkend="sec.admin.monitoring.stack"/>.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>

 <sect1 xml:id="sec.admin.monitoring.stack">
   <title>Monitoring Stack On Kubernetes</title>
   <para>
     This documentation aim to document Monitoring in a kubernetes
     environment.
   </para>
   <para>
     <emphasis role="strong">Prometheus Server &amp;
     Alertmanager</emphasis>
   </para>
   <para>
     Prometheus is an open-source monitoring system with a dimensional
     data model, flexible query language, efficient time series database
     and modern alerting approach.
   </para>
   <para>
     <emphasis role="strong">Grafana</emphasis>
   </para>
   <para>
     Grafana is an open-source system for querying, analysing and
     visualizing metrics.
   </para>
   <para>
     <emphasis role="strong">Nginx Ingress Controller</emphasis>
   </para>
   <para>
     Deploying Nginx Ingress Controller allows us to provide TLS
     termination to our services and to provide basic authentication to
     Prometheus Expression browser/API.
   </para>
   <para>
     We have two networking configuration possible:
   </para>
   <itemizedlist spacing="compact">
     <listitem>
       <para>
         <emphasis role="strong">NodePort</emphasis>: Our services will
         be publicly exposed on each node of the cluster, including
         master nodes, at port 30080 for HTTP and 30443 for HTTPS.
       </para>
     </listitem>
     <listitem>
       <para>
         <emphasis role="strong">ClusterIP with external
         IP(s)</emphasis>: Our services will be exposed on specific nodes
         of the cluster, at port 80 for HTTP and 443 for HTTPS.
       </para>
     </listitem>
   </itemizedlist>
   <sect2 xml:id="prerequisites">
     <title>Prerequisites</title>
     <orderedlist numeration="arabic" spacing="compact">
       <listitem>
         <para>
           Monitoring namespace
         </para>
       </listitem>
     </orderedlist>
     <para>
       We will deploy our monitoring stack in its own namespace:
     </para>
     <programlisting>
 $ kubectl create namespace monitoring
 </programlisting>
     <orderedlist numeration="arabic" spacing="compact">
       <listitem override="2">
         <para>
           Create DNS entries
         </para>
       </listitem>
     </orderedlist>
     <para>
       In this example, we will use the a worker node with IP
       10.84.152.113 to expose our services.
     </para>
     <programlisting>
 monitoring.example.com                      IN  A       10.84.152.113
 prometheus.example.com                      IN  CNAME   monitoring.example.com
 prometheus-alertmanager.example.com         IN  CNAME   monitoring.example.com
 grafana.example.com                         IN  CNAME   monitoring.example.com
 </programlisting>
     <para>
       Or add this entry to /etc/hosts
     </para>
     <programlisting>
 10.84.152.113 prometheus.example.com prometheus-alertmanager.example.com grafana.example.com
 </programlisting>
     <orderedlist numeration="arabic" spacing="compact">
       <listitem override="3">
         <para>
           Create certificates
         </para>
       </listitem>
     </orderedlist>
     <para>
       SSL certificates are required for the domains
       prometheus.example.com, prometheus-alertmanager.example.com and
       grafana.example.com
     </para>
   </sect2>
   <sect2 xml:id="nginx-ingress-controller">
     <title>Nginx Ingress Controller</title>
     <orderedlist numeration="arabic" spacing="compact">
       <listitem>
         <para>
           Choose a network configuration
         </para>
       </listitem>
     </orderedlist>
     <para>
       <emphasis role="strong">NodePort</emphasis>
     </para>
     <para>
       Create a configuration file
       <emphasis>nginx-ingress-config-values.yaml</emphasis>
     </para>
     <programlisting language="yaml">
 # Enable the creation of pod security policy
 podSecurityPolicy:
   enabled: true

 # Create a specific service account
 serviceAccount:
   create: true
   name: nginx-ingress

 # Publish services on port HTTP/30080
 # Publish services on port HTTPS/30443
 # These services are exposed on each node
 controller:
   service:
     type: NodePort
     nodePorts:
       http: 30080
       https: 30443
 </programlisting>
     <para>
       <emphasis role="strong">ClusterIP with external IP(s)</emphasis>
     </para>
     <para>
       Create a configuration file
       <emphasis>nginx-ingress-config-values.yaml</emphasis>
     </para>
     <programlisting language="yaml">
 # Enable the creation of pod security policy
 podSecurityPolicy:
   enabled: true

 # Create a specific service account
 serviceAccount:
   create: true
   name: nginx-ingress

 # Publish services on port HTTP/80
 # Publish services on port HTTPS/443
 # These services are exposed on the node with IP 10.84.152.113
 controller:
   service:
     externalIPs:
       - 10.84.152.113
 </programlisting>
     <orderedlist numeration="arabic" spacing="compact">
       <listitem override="2">
         <para>
           Deploy
         </para>
       </listitem>
     </orderedlist>
     <para>
       Deploy the upstream helm chart and pass our configuration values
       file.
     </para>
     <programlisting>
 $ helm install --name nginx-ingress stable/nginx-ingress \
   --namespace monitoring \
   --values nginx-ingress-config-values.yaml
 </programlisting>
     <para>
       =&gt; we need two pods running
     </para>
     <programlisting>
 $ kubectl -n monitoring get po
 NAME                                             READY     STATUS    RESTARTS   AGE
 nginx-ingress-controller-74cffccfc-p8xbb         1/1       Running   0          4s
 nginx-ingress-default-backend-6b9b546dc8-mfkjk   1/1       Running   0          4s
 </programlisting>
</sect2>
   <sect2 xml:id="tls">
     <title>TLS</title>
     <para>
       The next step explains how to create a self-signed certificate if
       you donâ€™t have a Certificate Authority, this is not recommended as
       this is subject to MITM attacks. When deploying for production,
       get the certificates from your company CA and skip this step.
     </para>
     <itemizedlist spacing="compact">
       <listitem>
         <para>
           self-signed certificate generation (optional)
         </para>
       </listitem>
     </itemizedlist>
     <para>
       This certificate uses Subject Alternative Names so it can be used
       for Prometheus and Grafana.
     </para>
     <itemizedlist spacing="compact">
       <listitem>
         <para>
           Create a file <emphasis>openssl.conf</emphasis> with the
           appropriate values (optional)
         </para>
       </listitem>
     </itemizedlist>
     <programlisting>
 [req]
 distinguished_name = req_distinguished_name
 req_extensions = v3_req
 default_md = sha256
 default_bits = 4096
 prompt=no

 [req_distinguished_name]
 C = CZ
 ST = CZ
 L = Prague
 O = example
 OU = monitoring
 CN = example.com
 emailAddress = admin@example.com

 [ v3_req ]
 basicConstraints = CA:FALSE
 keyUsage = keyEncipherment, dataEncipherment
 extendedKeyUsage = serverAuth
 subjectAltName = @alt_names

 [alt_names]
 DNS.1 = prometheus.example.com
 DNS.2 = prometheus-alertmanager.example.com
 DNS.3 = grafana.example.com
 </programlisting>
     <itemizedlist spacing="compact">
       <listitem>
         <para>
           Generate certificate (optional)
         </para>
       </listitem>
     </itemizedlist>
     <programlisting>
 $ openssl req -x509 -nodes -days 365 -newkey rsa:4096 -keyout ./monitoring.key -out ./monitoring.crt -config ./openssl.conf -extensions 'v3_req'
 </programlisting>
     <orderedlist numeration="arabic" spacing="compact">
       <listitem>
         <para>
           Create the TLS secret in Kubernetes
         </para>
       </listitem>
     </orderedlist>
     <para>
       If you use one certificate per service, do not forget to create a
       secret for each certificate and to change the secret name in the
       TLS ingress configuration of the service.
     </para>
     <programlisting>
 $ kubectl create -n monitoring secret tls monitoring-tls  \
   --key  ./monitoring.key \
   --cert ./monitoring.crt
 </programlisting>
</sect2>
   <sect2 xml:id="prometheus">
     <title>Prometheus</title>
     <para>
       Deploying Prometheus Pushgateway is out of the scope of this
       document.
     </para>
     <orderedlist numeration="arabic" spacing="compact">
       <listitem>
         <para>
           Manage permissions
         </para>
       </listitem>
     </orderedlist>
     <para>
       <emphasis role="strong">DEPRECATED IN 7.3.0, use
       podSecurityPolicy</emphasis>
     </para>
     <para>
       Prometheus node-exporter is in charge of getting the host metrics,
       to do so, it needs access to /proc or /sys path on the host. This
       is achieve with the use of HostPath in the pod specs, however
       using HostPath is forbidden by default in CaaSP so we need to
       assign the privileged PodSecurityPolicy to the node-exporter
       ServiceAccount, this ServiceAccount will be in charge of creating
       the pods.
     </para>
     <programlisting>
 $ kubectl create rolebinding node-exporter-psp-privileged \
   --namespace monitoring \
   --clusterrole=suse:caasp:psp:privileged \
   --serviceaccount=monitoring:prometheus-node-exporter
 </programlisting>
     <orderedlist numeration="arabic" spacing="compact">
       <listitem override="2">
         <para>
           Deploy
         </para>
       </listitem>
     </orderedlist>
     <para>
       We need to configure the storage for our deployment.
     </para>
     <para>
       Choose among the options and uncomment the line in the config
       file.
     </para>
     <para>
       <emphasis role="strong">Mandatory for production: Persistent
       Storage</emphasis>
     </para>
     <itemizedlist>
       <listitem>
         <para>
           Use an existing PersistentVolumeClaim
         </para>
       </listitem>
       <listitem>
         <para>
           Use a StorageClass
           <emphasis role="strong">(prefered)</emphasis>
         </para>
       </listitem>
       <listitem>
         <para>
           Create a file
           <emphasis>prometheus-config-values.yaml</emphasis> with the
           appropriate values.
         </para>
       </listitem>
     </itemizedlist>
     <programlisting language="yaml">
 # Alertmanager configuration
 alertmanager:
   enabled: true
   ingress:
     enabled: true
     hosts:
     -  prometheus-alertmanager.example.com
     annotations:
       kubernetes.io/ingress.class: nginx
       nginx.ingress.kubernetes.io/auth-type: basic
       nginx.ingress.kubernetes.io/auth-secret: prometheus-basic-auth
       nginx.ingress.kubernetes.io/auth-realm: &quot;Authentication Required&quot;
     tls:
       - hosts:
         - prometheus-alertmanager.example.com
         secretName: monitoring-tls
   persistentVolume:
     enabled: true
     ## Use a StorageClass
     storageClass: my-storage-class
     ## Create a PersistentVolumeClaim of 2Gi
     size: 2Gi
     ## Use an existing PersistentVolumeClaim (my-pvc)
     #existingClaim: my-pvc

 ## AlertManager is configured through alertmanager.yml. This file and any others
 ## listed in alertmanagerFiles will be mounted into the alertmanager pod.
 ## See configuration options https://prometheus.io/docs/alerting/configuration/
 #alertmanagerFiles:
 #  alertmanager.yml:

 # Create a specific service account
 serviceAccounts:
   nodeExporter:
     name: prometheus-node-exporter

 # Allow scheduling of node-exporter on master nodes
 nodeExporter:
   hostNetwork: false
   hostPID: false
   podSecurityPolicy:
     enabled: true
   tolerations:
     - key: node-role.kubernetes.io/master
       operator: Exists
       effect: NoSchedule

 # Disable Pushgateway
 pushgateway:
   enabled: false

 # Prometheus configuration
 server:
   ingress:
     enabled: true
     hosts:
     - prometheus.example.com
     annotations:
       kubernetes.io/ingress.class: nginx
       nginx.ingress.kubernetes.io/auth-type: basic
       nginx.ingress.kubernetes.io/auth-secret: prometheus-basic-auth
       nginx.ingress.kubernetes.io/auth-realm: &quot;Authentication Required&quot;
     tls:
       - hosts:
         - prometheus.example.com
         secretName: monitoring-tls
   persistentVolume:
     enabled: true
     ## Use a StorageClass
     storageClass: my-storage-class
     ## Create a PersistentVolumeClaim of 8Gi
     size: 8Gi
     ## Use an existing PersistentVolumeClaim (my-pvc)
     #existingClaim: my-pvc

 ## Prometheus is configured through prometheus.yml. This file and any others
 ## listed in serverFiles will be mounted into the server pod.
 ## See configuration options
 ## https://prometheus.io/docs/prometheus/latest/configuration/configuration/
 #serverFiles:
 #  prometheus.yml:
 </programlisting>
     <itemizedlist spacing="compact">
       <listitem>
         <para>
           Deploy the upstream helm chart and pass our configuration
           values file.
         </para>
       </listitem>
     </itemizedlist>
     <programlisting>
 $ helm install --name prometheus stable/prometheus \
   --namespace monitoring \
   --values prometheus-config-values.yaml
 </programlisting>
     <para>
       =&gt; We need these pods running (3 node-exporter pods because we
       have 3 nodes)
     </para>
     <programlisting>
 $ kubectl -n monitoring get po | grep prometheus
 NAME                                             READY     STATUS    RESTARTS   AGE
 prometheus-alertmanager-5487596d54-kcdd6         2/2       Running   0          2m
 prometheus-kube-state-metrics-566669df8c-krblx   1/1       Running   0          2m
 prometheus-node-exporter-jnc5w                   1/1       Running   0          2m
 prometheus-node-exporter-qfwp9                   1/1       Running   0          2m
 prometheus-node-exporter-sc4ls                   1/1       Running   0          2m
 prometheus-server-6488f6c4cd-5n9w8               2/2       Running   0          2m
 </programlisting>
     <orderedlist numeration="arabic" spacing="compact">
       <listitem override="3">
         <para>
           Configure Authentication
         </para>
       </listitem>
     </orderedlist>
     <para>
       We need to create a basic-auth secret so the Nginx Ingress
       Controller can perform authentication.
     </para>
     <itemizedlist spacing="compact">
       <listitem>
         <para>
           Install htpasswd
         </para>
       </listitem>
     </itemizedlist>
     <programlisting>
 $ sudo zypper in apache2-utils
 </programlisting>
     <itemizedlist spacing="compact">
       <listitem>
         <para>
           Create secret file <emphasis>auth</emphasis>
         </para>
       </listitem>
     </itemizedlist>
     <programlisting>
 htpasswd -c auth admin
 New password:
 Re-type new password:
 Adding password for user admin
 </programlisting>
     <itemizedlist spacing="compact">
       <listitem>
         <para>
           Create secret in Kubernetes
         </para>
       </listitem>
     </itemizedlist>
     <programlisting>
 $ kubectl create secret generic -n monitoring prometheus-basic-auth --from-file=auth
 </programlisting>
     <para>
       It is very important that the file name is
       <emphasis role="strong">auth</emphasis> so the key in the secret
       will be name auth because the ingress controller will look for
       that key.
     </para>
     <para>
       =&gt; At this stage, the Prometheus Expression browser/API is
       accessible, depending on you network configuration at
       https://prometheus.example.com or
       https://prometheus.example.com:30443
     </para>
   </sect2>
   <sect2 xml:id="grafana">
     <title>Grafana</title>
     <orderedlist numeration="arabic" spacing="compact">
       <listitem>
         <para>
           Configure provisoning
         </para>
       </listitem>
     </orderedlist>
     <para>
       Starting from Grafana 5.0, it is possible to dynamically provision
       the data sources and dashbords via files. In Kubernetes, these
       files are provided via the utilization of ConfigMap, editing a
       ConfigMap will result by the modification of the configuration
       without having to delete/recreate the pod.
     </para>
     <itemizedlist spacing="compact">
       <listitem>
         <para>
           Create the default datasource configuration file
           <emphasis>grafana-datasources.yaml</emphasis> which point to
           our Prometheus server
         </para>
       </listitem>
     </itemizedlist>
     <programlisting language="yaml">
 ---
 kind: ConfigMap
 apiVersion: v1
 metadata:
   name: grafana-datasources
   namespace: monitoring
   labels:
      grafana_datasource: &quot;1&quot;
 data:
   datasource.yaml: |-
     apiVersion: 1
     deleteDatasources:
       - name: Prometheus
         orgId: 1
     datasources:
     - name: Prometheus
       type: prometheus
       url: http://prometheus-server.monitoring.svc.cluster.local:80
       access: proxy
       orgId: 1
       isDefault: true
 </programlisting>
     <itemizedlist spacing="compact">
       <listitem>
         <para>
           Create the ConfigMap in Kubernetes
         </para>
       </listitem>
     </itemizedlist>
     <programlisting>
 $ kubectl create -f grafana-datasources.yaml
 </programlisting>
     <itemizedlist spacing="compact">
       <listitem>
         <para>
           This repo provides a demo dashboard to visualize the resources
           of the Kubernetes nodes leveraging Prometheus node-exporter
           metrics only. (optional)
         </para>
       </listitem>
     </itemizedlist>
     <programlisting>
 $ cat grafana-dashboards-caasp* | kubectl apply -f -
 </programlisting>
     <orderedlist numeration="arabic" spacing="compact">
       <listitem override="2">
         <para>
           Deploy stack
         </para>
       </listitem>
     </orderedlist>
     <para>
       We need to configure the storage for our deployment.
     </para>
     <para>
       Choose among the options and uncomment the line in the config
       file.
     </para>
     <para>
       <emphasis role="strong">Mandatory for production: Persistent
       Storage</emphasis>
     </para>
     <itemizedlist>
       <listitem>
         <para>
           Use an existing PersistentVolumeClaim
         </para>
       </listitem>
       <listitem>
         <para>
           Use a StorageClass
           <emphasis role="strong">(prefered)</emphasis>
         </para>
       </listitem>
       <listitem>
         <para>
           Create a file <emphasis>grafana-config-values.yaml</emphasis>
           with the appropriate values.
         </para>
       </listitem>
     </itemizedlist>
     <programlisting language="yaml">
 # Configure admin password
 adminPassword: 3357b57b-25a1-4780-ac78-8b77fd7ed9e1

 # Ingress configuration
 ingress:
   enabled: true
   annotations:
     kubernetes.io/ingress.class: nginx
   hosts:
     - grafana.example.com
   tls:
     - hosts:
       - grafana.example.com
       secretName: monitoring-tls

 # Configure persistent storage
 persistence:
   enabled: true
   accessModes:
     - ReadWriteOnce
   ## Use a StorageClass
   storageClassName: my-storage-class
   ## Create a PersistentVolumeClaim of 10Gi
   size: 10Gi
   ## Use an existing PersistentVolumeClaim (my-pvc)
   #existingClaim: my-pvc

 # Enable sidecar for provisioning
 sidecar:
   datasources:
     enabled: true
     label: grafana_datasource
   dashboards:
     enabled: true
     label: grafana_dashboard
 </programlisting>
     <itemizedlist spacing="compact">
       <listitem>
         <para>
           Deploy the upstream helm chart and pass our configuration
           values file.
         </para>
       </listitem>
     </itemizedlist>
     <programlisting>
 $ helm install --name grafana stable/grafana \
   --namespace monitoring \
   --values grafana-config-values.yaml
 </programlisting>
     <para>
       =&gt; We need this pod running
     </para>
     <programlisting>
 $ kubectl -n monitoring get po | grep grafana
 NAME                                             READY     STATUS    RESTARTS   AGE
 grafana-dbf7ddb7d-fxg6d                          3/3       Running   0          2m
 </programlisting>
     <para>
       =&gt; At this stage, Grafana is accessible, depending on you
       network configuration at https://grafana.example.com or
       https://grafana.example.com:30443
     </para>
   </sect2>
 </sect1>

 <sect1 xml:id="sec.admin.monitoring.health">
  <title>Health Checking</title>

  <para>
   Although &kube; takes care of a lot of the traditional deployment problems
   with its self-healing capabilities, it is considered good practice to
   monitor the availability and health of your services and applications to
   react to problems should they go beyond these automated measures.
  </para>

  <para>
   A complete set of instructions on how to monitor and maintain the health of
   you cluster is, however, beyond the scope of this document.
  </para>

  <para>
   There are three levels of health checks.
  </para>

  <itemizedlist>
   <listitem>
    <para>
     Cluster
    </para>
   </listitem>
   <listitem>
    <para>
     Node
    </para>
   </listitem>
   <listitem>
    <para>
     Application / Service
    </para>
   </listitem>
  </itemizedlist>

  <sect2 xml:id="sec.admin.monitoring.health.cluster">
   <title>Cluster Health Checks</title>
   <para>
    The basic check if a cluster is working correctly is based on a few
    criteria:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Are all services running as expected?
     </para>
    </listitem>
    <listitem>
     <para>
      Is there at least one &kube; master fully working? Even if the
      deployment is configured to be highly available, it's useful to know if
      <literal>kube-controller-manager</literal> is down on one of the machines.
     </para>
    </listitem>
   </itemizedlist>

   <note>
    <title>Understanding cluster health</title>
    <para>
     For further information consider reading
     <link xlink:href="https://kubernetes.io/docs/tasks/debug-application-cluster/debug-cluster/">&kube;:
     Troubleshoot Clusters</link>
    </para>
   </note>

   <sect3 xml:id="sec.admin.monitoring.health.cluster.kubernetes">
    <title>&kube; master</title>
    <para>
     All components in &kube; expose a <literal>/healthz</literal> endpoint. The
     expected (healthy) response is a <literal>200 HTTP</literal> and
     a response body containing <literal>ok</literal>.
    </para>
    <para>
     The minimal services for the master to work properly are:
    </para>
    <variablelist>
     <varlistentry>
      <term>kube-apiserver</term>
      <listitem>
      <para>
       The component that receives your requests from <command>kubectl</command>
       and from the rest of the &kube; components.
      </para>
      <para>
       Endpoint: <literal>https://<replaceable>MASTER NODE FQDN</replaceable>:6444/healthz</literal> (HTTPS)
      </para>
<screen>&prompt.user;<command>curl -i https://localhost:6444/healthz</command>
ok
      </screen>
     </listitem>
    </varlistentry>
    <varlistentry>
      <term>kube-controller-manager</term>
      <listitem>
      <para>
       The component that contains the control loop, driving current state to
       the desired state.
      </para>
      <para>
       Endpoint: <literal>http://<replaceable>MASTER NODE FQDN</replaceable>:10252/healthz</literal> (HTTP)
      </para>
<screen>&prompt.user;<command>curl -i http://localhost:10252/healthz</command>
ok
      </screen>
     </listitem>
    </varlistentry>
     <varlistentry>
      <term>kube-scheduler</term>
      <listitem>
      <para>
       The component that schedules workloads to nodes.
      </para>
      <para>
       Endpoint: <literal>http://<replaceable>MASTER NODE FQDN</replaceable>:10251/healthz</literal> (HTTP)
      </para>
<screen>&prompt.user;<command>curl -i http://localhost:10251/healthz</command>
ok
      </screen>
     </listitem>
    </varlistentry>
   </variablelist>

   <note>
    <title>High-Availability Environments</title>
    <para>
     In a HA environment you can monitor <literal>kube-apiserver</literal> on
     <literal>https://<replaceable>MASTER NODE LOADBALANCER</replaceable>:6443/healthz</literal>.
    </para>
    <para>
     If any master node is running correctly you will receive a valid response.
    </para>
    <para>
     This does, however, not mean that all master nodes necessarily work
     correctly. To ensure that all master nodes work properly, the health checks
     must be repeated individually for each master node deployed.
    </para>
    <para>
     This endpoint will return a successful HTTP response if the cluster is
     operational; otherwise it will fail. It will for example check that it can
     access <literal>etcd</literal> too. This should not be used to infer that
     the overall cluster health is ideal. It will return a a successful response
     even when only minimal operational cluster health exists.
    </para>
    <para>
     To probe for full cluster health, you must perform individual health
     checking for all machines individually.
    </para>
   </note>
  </sect3>

   <sect3 xml:id="sec.admin.monitoring.cluster.health.etcd">
    <title><literal>etcd</literal> Cluster</title>
    <para>
     Check that all machines that have the <literal>etcd</literal> role on the cluster see the
     etcd cluster as healthy.
    </para>
<screen>&prompt.user;<command>docker exec -it $(docker ps -q -f name="salt-master") salt -G 'roles:etcd' \
cmd.run 'set -a; source /etc/sysconfig/etcdctl; etcdctl cluster-health'</command>

f69e7af2880f42d68dca26ca892cb945:
    member af7ffa9bb1cb7c67 is healthy: got healthy result from https://caasp-master:2379
    member cc40a990d09b4705 is healthy: got healthy result from https://caasp-worker-1:2379
    member fe9b5ee9e1cc3cf7 is healthy: got healthy result from https://caasp-worker-2:2379
    cluster is healthy
ab040b25c2584bc8904971c0acbb250f:
    member af7ffa9bb1cb7c67 is healthy: got healthy result from https://caasp-master:2379
    member cc40a990d09b4705 is healthy: got healthy result from https://caasp-worker-1:2379
    member fe9b5ee9e1cc3cf7 is healthy: got healthy result from https://caasp-worker-2:2379
    cluster is healthy
63008aabc75b471b9a1aa2f64e4d30eb:
    member af7ffa9bb1cb7c67 is healthy: got healthy result from https://caasp-master:2379
    member cc40a990d09b4705 is healthy: got healthy result from https://caasp-worker-1:2379
    member fe9b5ee9e1cc3cf7 is healthy: got healthy result from https://caasp-worker-2:2379
    cluster is healthy
   </screen>
   <para>
    More information on etcd cluster health can be found in <xref linkend="sec.admin.nodes.graceful_shutdown.etcd"/>.
   </para>
   </sect3>

   <sect3 xml:id="sec.admin.monitoring.cluster.health.components">
    <title>Running Components</title>
    <para>
     Check if the cluster has all required components running:
    </para>
<screen>&prompt.user;<command>kubectl cluster-info</command>

Kubernetes master is running at https://api.infra.caasp.local:6443
Dex is running at https://api.infra.caasp.local:6443/api/v1/namespaces/kube-system/services/dex:dex/proxy
KubeDNS is running at https://api.infra.caasp.local:6443/api/v1/namespaces/kube-system/services/kube-dns:dns/proxy
Tiller is running at https://api.infra.caasp.local:6443/api/v1/namespaces/kube-system/services/tiller:tiller/proxy

To further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.
    </screen>
    <para>
     You can optionally run <command>kubectl cluster-info dump</command> to
     obtain a much more detailed output
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="sec.admin.monitoring.health.node">
   <title>Node Health Checks</title>
   <para>
    The basic check if a node is healthy consists of checking if
    <literal>kubelet</literal> and the CNI (Container Networking Interface) are
    working properly.
   </para>

   <sect3 xml:id="sec.admin.monitoring.health.node.kubelet">
    <title><literal>kubelet</literal></title>
    <para>
     Is the <literal>kubelet</literal> up and working in this node?
    </para>
    <para>
     The <literal>kubelet</literal> has a port exposed <literal>10250</literal>
     on all machines; it's possible to perform an HTTP request to the endpoint
     to find out if the kubelet is healthy on that machine. The
     expected (healthy) response is a <literal>200 HTTP</literal> and
     a response body containing <literal>ok</literal>.
    </para>
    <para>
     Endpoint: <literal>https://<replaceable>NODE</replaceable>:10250/healthz</literal> (HTTPS)
    </para>
<screen>&prompt.user;<command>curl -i https://localhost:10250/healthz</command>
ok
    </screen>
   </sect3>

   <sect3 xml:id="sec.admin.monitoring.health.node.cni">
    <title><literal>CNI</literal></title>
    <para>
     Is CNI (Container Networking Interface) working as expected in this node?
     If not, <literal>kube-dns</literal> can not start. Check if the
     <literal>kube-dns</literal> service is running.
    </para>
<screen>&prompt.user;<command>kubectl get deployments -n kube-system</command>
NAME            DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE
dex             3         3         3            3           7d
kube-dns        3         3         3            3           7d
tiller-deploy   1         1         1            1           7d
    </screen>

    <para>
     If kube-dns is running and you are able to create pods then you can be certain
     that CNI and your CNI plugin are working correctly.
    </para>
    <para>
     There's also the
     <link xlink:href="https://kubernetes.io/docs/tasks/debug-application-cluster/monitor-node-health/">Monitor Node Health</link>
     check. This is a <literal>DaemonSet</literal> that runs on every node, and
     reports to the <literal>apiserver</literal> back as
     <literal>NodeCondition</literal> and <literal>Events</literal>.
    </para>
   </sect3>
   </sect2>


  <sect2 xml:id="sec.admin.monitoring.health.service">
   <title>Service/Application Health Checks</title>
   <para>
    If the deployed services contain a health endpoint, or if they contain an
    endpoint that can be used to determine if the service is up, you can use
    <literal>livenessProbes</literal> and/or <literal>readinessProbes</literal>.
   </para>
   <note>
    <title>Health check endpoints vs. functional endpoints</title>
    <para>
     A proper health check is always preferred if designed correctly.
    </para>
    <para>
     Despite the fact that any endpoint could potentially be used to infer if
     your application is up, a specific health endpoint in your application is
     preferred. Such an endpoint will only respond affirmatively when all your
     setup code on the server has finished and the application is running in a
     desired state.
    </para>
   </note>

   <para>
    <literal>livenessProbes</literal> and <literal>readinessProbes</literal>
    share configuration options and probe types.
   </para>

   <variablelist>
    <varlistentry>
     <term>initialDelaySeconds</term>
     <listitem>
      <para>
       Number of seconds to wait before performing the very first liveness
       probe.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>periodSeconds</term>
     <listitem>
      <para>
       Number of seconds that the kubelet should wait between liveness probes.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>

    <variablelist>
     <varlistentry>
      <term>successThreshold</term>
     <listitem>
      <para>
        Number of minimum consecutive successes for the probe to be considered
        successful (Default: 1).
      </para>
     </listitem>
    </varlistentry>
     <varlistentry>
      <term>failureThreshold</term>
     <listitem>
      <para>
        Number of times this probe is allowed to fail in order to assume that
        the service is not responding (Default: 3).
      </para>
     </listitem>
    </varlistentry>
     <varlistentry>
      <term>timeoutSeconds</term>
     <listitem>
      <para>
       Number of seconds after which the probe times out (Default: 1).
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    There are different options for the liveness probe to check:
   </para>
   <variablelist>
    <varlistentry>
     <term>Command</term>
     <listitem>
     <para>
      A command executed within a container; a retcode of 0 means success.
     </para>
      <para>
       All other return codes mean failure.
     </para>
    </listitem>
    </varlistentry>
    <varlistentry>
     <term>TCP</term>
     <listitem>
     <para>
      If a TCP connection can be established is considered success.
     </para>
    </listitem>
    </varlistentry>
    <varlistentry>
     <term>HTTP</term>
     <listitem>
     <para>
      Any HTTP response between <literal>200</literal> and <literal>400</literal>
      indicates success.
     </para>
    </listitem>
    </varlistentry>
   </variablelist>

   <sect3 xml:id="sec.admin.monitoring.health.service.livenessprobe">
    <title>livenessProbe</title>
    <para>
     <link xlink:href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/">livenessProbes</link>
     are used to detect running but misbehaving pods/a service that
     might be running (the process didn't die), but that is not responding as
     expected.
    </para>
    <para>
     Probes are executed by each <literal>kubelet</literal> against the pods that define them
     and that are running in that specific node.
    </para>
    <para>
     When a <literal>livenessProbe</literal> fails, &kube; will automatically restart the
     pod and increase the <literal>RESTARTS</literal> count for that pod.
    </para>
    <para>
     These probes will be executed every <literal>periodSeconds</literal> starting from
     <literal>initialDelaySeconds</literal>.
    </para>
   </sect3>

   <sect3 xml:id="sec.admin.monitoring.health.service.readinessprobe">
    <title>readinessProbe</title>
    <para>
     <link xlink:href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/#define-readiness-probes">readinessProbes</link>
     are used to wait for processes that take some time to start. Despite
     the container running, it might be performing some time consuming
     initializatoin operations. During this time, you don't want &kube; to route
     traffic to that specific pod; also, you don't want that container to be
     restarted because it will appear unresponsive.
    </para>
    <para>
     These probes will be executed every <literal>periodSeconds</literal>
     starting from <literal>initialDelaySeconds</literal> until the service is ready.
    </para>
    <para>
     They support the same kind of probes as the <literal>livenessProbe</literal>
    </para>

    <para>
     Both probe types can be used at the same time. The
     <literal>livenessProbe</literal> will ensure that if a service is running
     yet misbehaving, it will be restarted, and <literal>readinessProbe</literal>
     will ensure that &kube; won't route traffic to that specific pod until it's
     considered to be fully functional and running.
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="sec.admin.monitoring.health.general">
   <title>General Health Checks</title>
   <para>
    We recommend to apply other best practices from system administration
    to your monitoring and health checking approach. These steps are not
    specific to &productname; and are beyond the scope of this document. To
    simplify performing tasks like disk usage checks, you can use <literal>salt</literal>.
    For more information see: <xref linkend="sec.admin.salt"/>
   </para>
  </sect2>
 </sect1>
</chapter>
