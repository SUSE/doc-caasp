<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha.deployment.preparation"
 xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Preparing the Installation</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker/>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <para>
  This chapter prepares the installation of &productname; on physical
  machines, manually configured virtual machines or in private cloud
  environments.
 </para>

 <para>
  There are several ways to install &productname;:
 </para>
 <variablelist>
  <varlistentry>
   <term><xref linkend="sec.deploy.preparation.dvd" /></term>
   <listitem>
    <para>
     This method requires lots of manual interaction with the
     physical or virtual machines. This method is suitable for small to
     medium cluster sizes.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry>
   <term><xref linkend="sec.deploy.preparation.disk_images" /></term>
   <listitem>
    <para>
     You can use prepared disk images ready to deploy on virtual
     machines. This increases the deployment speed.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry>
   <term><xref linkend="sec.deploy.preparation.pxe" /></term>
   <listitem>
    <para>
     With this method the boot into the installation is done with a
     network installation server. This reduces the need to interact
     with every single node. This method is suitable for medium cluster
     sizes.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry>
   <term><xref linkend="sec.deploy.preparation.openstack" /></term>
   <listitem>
    <para>
     You can deploy &productname; on &soc;. This method is suitable
     for large cluster sizes.
    </para>
   </listitem>
  </varlistentry>
  <varlistentry>
   <term><xref linkend="sec.deploy.preparation.public_cloud" /></term>
   <listitem>
    <para>
     This section is about installing &productname; in a public cloud,
     for example <emphasis>Microsoft Azure</emphasis>*,
     <emphasis>Amazon AWS</emphasis>* and <emphasis>Google Compute
     Engine</emphasis>*.
    </para>
   </listitem>
  </varlistentry>
 </variablelist>
 <para>
  To customize the setup of the nodes, use <command>cloud-init</command>.
  For details, see <xref linkend="sec.deploy.cloud-init" />.
 </para>

 <sect1 xml:id="sec.deploy.preparation.dvd">
  <title>Installing from USB, DVD or ISO Images</title>
  <para>
   This procedure provides an overview of the steps for the
   cluster deployment with classical boot devices like DVD drives. This
   method is suitable for small to medium cluster sizes.
  </para>
  <procedure>
   <step>
    <para>
     Choose an installation medium. You can install from DVD or USB
     sticks. On virtual machines, you can install from ISO images.
    </para>
   </step>
   <step>
    <para>
     Boot the machine designated for the &admin_node; from your
     selected medium. Then follow the installation and configuration
     instructions detailed in <xref
     linkend="sec.deploy.nodes.admin_install" /> and <xref
     linkend="sec.deploy.nodes.admin_configuration" />.
    </para>
   </step>
   <step>
    <para>
     Boot the machines designated to be the Master and Worker Nodes
     from your selected medium. Then follow the installation and
     configuration instructions detailed in <xref
     linkend="sec.deploy.nodes.worker_install" />. To automate the
     installation, you can use an &ay; file provided by the
     &admin_node;. We recommend to use &ay; to speed up the node
     deployment. For details, see <xref
     linkend="sec.deploy.nodes.worker_install.manual.autoyast" />.
    </para>
   </step>
   <step>
    <para>
     Finish the installation by bootstrapping the cluster. For details,
     see <xref linkend="sec.deploy.install.bootstrap" />.
    </para>
   </step>
  </procedure>
 </sect1>

 <sect1 xml:id="sec.deploy.preparation.disk_images">
  <title>Installing from Virtual Disk Images</title>
  <para>
   For building clusters from virtual machines on supported
   hypervisors, it is not necessary to individually install each
   node. &suse; offers pre-installed VM disk images in the following
   formats:
  </para>
  <variablelist>
   <varlistentry>
    <term>&kvm; and &xen;</term>
    <listitem>
     <para>
      In QCOW2 format, for &kvm; and for &xen; using full virtualization.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&xen;</term>
    <listitem>
     <para>
      In QCOW2 format, for &xen; using paravirtualization.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>&vmware;</term>
    <listitem>
     <para>
      In VMDK format, for &vmware; ESXi.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     VHD
    </term>
    <listitem>
     <para>For Microsoft Hyper-V.</para>
    </listitem>
   </varlistentry>
  </variablelist>

  <sect2 xml:id="sec.deploy.preparation.disk_images.overview">
   <title>Overview</title>
   <para>
    When deploying a cluster node from pre-installed disk images, the
    setup program never runs. Therefore, the configuration must happen
    while the node is starting up. For this purpose, &productname;
    includes <command>cloud-init</command>. The described process is
    very similar for all hypervisors. The examples in this manual use
    &kvm; running on &sle;. The following procedure outlines the
    overall process.
   </para>
   <procedure>
    <step>
     <para>
      Only for &vmware;: Convert VMDK images. See <xref linkend=
      "sec.deploy.preparation.disk_images.vmware" />.
     </para>
    </step>
    <step>
     <para>
      Write <command>cloud-init</command> configuration files. See
      <xref linkend="sec.deploy.preparation.disk_images.configuration"
      />.
     </para>
    </step>
    <step>
     <para>
      Prepare ISO images containing the <command>cloud-init</command>
      configuration files. See <xref
      linkend="sec.deploy.preparation.disk_images.iso-image" />.
     </para>
    </step>
    <step>
     <para>
      Create a copy of the downloaded disk image for each virtual
      machine, naming them appropriately: for example,
      <literal>caas-admin</literal>, <literal>caas-master</literal>,
      <literal>caas-worker1</literal>, <literal>caas-worker2</literal>
      and so on.
     </para>
    </step>
    <step>
     <para>
      Boot and configure the &admin_node;. See <xref
      linkend="sec.deploy.preparation.qcow2.admin" />.
     </para>
    </step>
    <step>
     <para>
      Boot and configure the worker nodes. See <xref
      linkend="sec.deploy.preparation.disk_images.worker" />.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.deploy.preparation.disk_images.vmware">
   <title>Converting Images for &vmware; ESX and ESXi</title>
   <para>
    Downloaded disk images need to be converted for usage with &vmware;
    ESX and ESXi. On the ESX/ESXi host, run the following command on the
    downloaded disk images:
   </para>
   <screen>&prompt.root;<command>vmkfstools -i <replaceable>DOWNLOADED_IMAGE</replaceable>.vmdk <replaceable>CONVERTED_IMAGE</replaceable>.vmdk</command></screen>
  </sect2>

  <sect2 xml:id="sec.deploy.preparation.disk_images.configuration">
   <title>Configuration Files</title>
   <para>
    There are two separate configuration files: <filename>user-data</filename>
    and <filename>meta-data</filename>. Each node needs both. Thus, you need to
    prepare at a minimum one pair of files for the &admin_node;, and another
    pair of files for all the &worker_node;s.
   </para>
   <para>
    Place the files into subdirectories named <filename>cc-admin</filename>
    for the &admin_node; and <filename>cc-worker</filename> for the
    &worker_node;s.
   </para>
   <para>
    So, for instance, if your working directory is <filename>~/cloud-config</filename>,
    then for the admin node, you need these two files:
   </para>
   <screen>~/cloud-config/cc-admin/user-data
~/cloud-config/cc-admin/meta-data</screen>
   <para>
    For a &worker_node;, you need:
   </para>
   <screen>~/cloud-config/cc-worker/user-data
~/cloud-config/cc-worker/meta-data</screen>
   <para>
    The same <filename>meta-data</filename> file can be used for both node types.
    Here is an sample <filename>meta-data</filename> file:
    </para>
    <screen>#cloud-config
instance-id: <replaceable>iid-CAAS01</replaceable>
network-interfaces: |
   auto <replaceable>eth0</replaceable>
   iface <replaceable>eth0</replaceable> inet dhcp</screen>
   <para>
    The <filename>user-data</filename> file contains settings such as time
    servers, the &rootuser; password, and the node type.
   </para>
   <para>
    Here is an example <filename>cc-admin/user-data</filename> file for an
    &admin_node;:
   </para>
   <screen>#cloud-config
debug: True
disable_root: False
ssh_deletekeys: False
ssh_pwauth: True
chpasswd:
   list: |
     root:<replaceable>MY_PASSWORD</replaceable>
     expire: False
ntp:
   servers:
     - <replaceable>ntp1.example.com</replaceable>
     - <replaceable>ntp2.example.com</replaceable>
runcmd:
   - /usr/bin/systemctl enable --now ntpd
suse_caasp:
  role: <replaceable>admin</replaceable></screen>
   <para>
    Here is an example <filename>cc-worker/user-data</filename>  for a
    &worker_node;. Rather than providing the &rootuser; password in
    clear text, you can use a hash instead; this example is hashed with
    SHA-256.
   </para>
   <screen>#cloud-config
debug: True
disable_root: False
ssh_deletekeys: False
ssh_pwauth: True
chpasswd:
   list: |
   root:<replaceable>$5$eriogqzq$Dg7PxHsKGzziuEGkZgkLvacjuEFeljJ.rLf.hZqKQLA</replaceable>
     expire: False
suse_caasp:
    role: <replaceable>cluster</replaceable>
    admin_node: <replaceable>caas-admin.example.com</replaceable></screen>
    <para>
     For more informatin, also refer to <xref
     linkend="sec.deploy.cloud-init" />.
    </para>
  </sect2>
  <sect2 xml:id="sec.deploy.preparation.disk_images.iso-image">
   <title>Preparing an ISO image</title>
   <para>
    First, edit the configuration files as described in <xref linkend=
    "sec.deploy.preparation.disk_images.configuration" />. Then
    create an ISO image with the volume label <literal>cidata</literal>
    containing only the subdirectory for that node type.
   </para>
   <para>
    On &sle; 12 or &opensuse; 42, use <command>genisoimage</command> to do this.
    On &sle; or &opensuse; 15, use <command>mkisofs</command>. The parameters
    are the same for both commands.
   </para>
   <para>
    For example, to create the ISO image for an admin node on a computer
    running &opensuse; 42:
   </para>
   <screen>&prompt.user;<command>sudo genisoimage -output <replaceable>cc-admin.iso</replaceable> -volid <replaceable>cidata</replaceable> -joliet -rock <replaceable>cc-admin</replaceable></command></screen>
   <para>
    To create the ISO image for a worker node on a computer running &opensuse;
    15, substituting the name of the folder containing the configuration
    files for a &worker_node; and titling the volume <literal>cc-worker</literal>:
   </para>
   <screen>&prompt.user;<command>sudo mkisofs -output <replaceable>cc-worker.iso</replaceable> -volid <replaceable>cidata</replaceable> -joliet -rock <replaceable>cc-worker</replaceable></command></screen>
  </sect2>

  <sect2 xml:id="sec.deploy.preparation.qcow2.admin">
   <title>Bringing Up an &Admin_Node;</title>
   <procedure>
    <step>
     <para>
      Create a new VM for the &admin_node;.
     </para>
    </step>
    <step>
     <para>
      Attach a copy of the downloaded disk image as its main hard disk.
     </para>
    </step>
    <step>
     <para>
      Attach the <filename>cc-admin.iso</filename> image as a
      virtual DVD unit. For details about preparing the image, see <xref
      linkend="sec.deploy.preparation.disk_images.iso-image" />
    </para>
    </step>
    <step>
     <para>
      Start the VM.
     </para>
    </step>
    <step>
     <para>
      Configure the new &admin_node; as in step <xref linkend="sec.deploy.nodes.admin_configuration"/>.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.deploy.preparation.disk_images.worker">
   <title>Bringing Up a &Worker_Node;</title>
   <para>
    Then, repeat the following steps for each &worker_node;:
   </para>
   <procedure>
    <step>
     <para>
      Create a new VM for the &worker_node;.
     </para>
    </step>
    <step>
     <para>
      Attach a copy of the downloaded disk image as its main hard disk.
     </para>
    </step>
    <step>
     <para>
      Attach the <filename>cc-worker.iso</filename> disk image as a
      virtual DVD unit. For details about preparing the image, see <xref
      linkend="sec.deploy.preparation.disk_images.iso-image" />. The
      ISO image can be reused for multiple &worker_node;s.
     </para>
    </step>
    <step>
     <para>
      Start the VM.
     </para>
    </step>
   </procedure>
   <para>
    Once you have brought up as many &worker_node;s as you need, proceed to
    bootstap the cluster using the &dashboard; dashboard.
   </para>
  </sect2>
 </sect1>

 <sect1 xml:id="sec.deploy.preparation.pxe">
  <title>Installing from Network Source</title>
  <para>
   This procedure provides an overview of the steps for the
   cluster deployment from an network installation server. A PXE
   environment sis used to provide the nodes with the data required for
   installation.
  </para>
  <procedure>
   <step>
    <para>
     Install an installation server that provides a DHCP, PXE and TFTP
     service. Additionally, you can provide the installation data on
     an HTTP or FTP server. For details, refer to the &sle; 12 Deployment Guide:
     <link xlink:href="https://www.suse.com/documentation/sles-12/singlehtml/book_sle_deployment/book_sle_deployment.html#cha.deployment.prep_boot"/>.
    </para>
    <para>
     You can directly use the <filename>initrd</filename> and
     <filename>linux</filename> files from your installation media, or
     install the package
     <package>tftpboot-installation-CAASP-3.0</package> onto your TFTP
     server. The package provides the required
     <filename>initrd</filename> and <filename>linux</filename> files
     in the <filename>/srv/tftpboot/</filename> directory. You will
     need to modify the paths used in the &sle; 12 Deployment Guide to
     correctly point to the files provided by the package.
    </para>
   </step>
   <step>
    <para>
     PXE boot the machine designated for the &admin_node;. Then follow
     the installation and configuration instructions detailed in <xref
     linkend="sec.deploy.nodes.admin_install" /> and <xref
     linkend="sec.deploy.nodes.admin_configuration" />.
    </para>
   </step>
   <step>
    <para>
     Pxe boot the machines designated to be the Master and Worker
     Nodes. Then follow the installation and configuration instructions
     detailed in <xref linkend="sec.deploy.nodes.worker_install" />. To
     automate the installation, you can use an &ay; file provided by
     the &admin_node;. For details, see <xref
     linkend="sec.deploy.nodes.worker_install.manual.autoyast" />.
    </para>
   </step>
   <step>
    <para>
     Finish the installation by bootstrapping the cluster. For details,
     see <xref linkend="sec.deploy.install.bootstrap" />.
    </para>
   </step>
  </procedure>
 </sect1>

 <sect1 xml:id="sec.deploy.preparation.openstack">
  <title>Installing in &soc;</title>
  <para>
   You can deploy a &productname; on &soc; using &ostack;.
   You will need a &productname; machine image and &ostack; Heat templates.
   Once you have created a stack, you will continue with the &productname; setup.
  </para>
  <note>
   <title>&productname; machine image for &soc;</title>
   <para>
    Download the latest &productname; for &ostack; image from
    <link xlink:href="https://download.suse.com">https://download.suse.com</link>
    (for example,
    <filename>SUSE-CaaS-Platform-3.0-OpenStack-Cloud.x86_64-1.0.0-GM.qcow2</filename>).
   </para>
  </note>
   <note>
    <title>
     &ostack; Heat Templates Repository
    </title>
    <para>
     &productname; Heat templates are available from <link xlink:href="https://github.com/SUSE/caasp-openstack-heat-templates">GitHub</link>.
    </para>
  </note>

  <sect2 xml:id="sec.deploy.preparation.openstack.horizon">
   <title>Using the Horizon Dashboard</title>
   <procedure>
   <step>
    <para>
     Go to <guimenu>Project &rarr; Compute &rarr; Images</guimenu> and click on
     <guimenu>Create Image</guimenu>.
    </para>
    <para>
     Give your image a name (for example: <literal>CaaSP-3</literal>); you will
     need to use this later to find the image.
    </para>
   </step>
    <step>
     <para>
      Go to <guimenu>Project &rarr; Orchestration</guimenu> and click on
      <guimenu>Stacks</guimenu>.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="horizon_stacks.png" width="100%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="horizon_stacks.png" width="100%"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
     <para>
      Click on <guimenu>Launch Stack</guimenu> and provide the stack
      templates. Either upload the files, provide the URL to the raw files
      directly (only applies to stack template), or copy and paste the
      contents into the <guimenu>Direct Input</guimenu> fields.
     </para>
     <warning>
      <title>Replace the default <literal>root_password</literal></title>
       <para>
        Do not use the <filename>caasp-environment.yaml</filename> directly
        from the GitHub repository.
       </para>
        <para>
         You must make sure to replace the value for
         <literal>root_password</literal> with a secure password.
         This will become the password for the &rootuser; account on all
         nodes in the stack.
        </para>
    </warning>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="horizon_launch_stack.png" width="100%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="horizon_launch_stack.png" width="100%"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
   <step>
    <para>
     Click <guimenu>Next</guimenu>.
    </para>
   </step>
    <step>
     <para>
      Now you need to define more information about the stack.
     </para>
     <variablelist>
      <varlistentry>
       <term><literal>Stack Name</literal></term>
       <listitem>
        <para>
         Give your stack a name
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><literal>Password</literal></term>
       <listitem>
        <para>
         Your &soc; password
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><literal>Image</literal></term>
       <listitem>
        <para>
         Select the image your machines will be created from
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><literal>root_password</literal></term>
       <listitem>
        <para>
         Set the root password for your cluster machines
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><literal>admin</literal>/master/worker_flavor</term>
       <listitem>
        <para>
         Select the machine flavor for your nodes
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><literal>worker_count</literal></term>
       <listitem>
        <para>
         Number of worker nodes to be launched
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><literal>external_net</literal></term>
       <listitem>
        <para>
         Select an external network that your cluster will be reachable from
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><literal>internal_net_cidr</literal></term>
       <listitem>
        <para>
         The internal network range to be used inside the cluster
        </para>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><literal>dns_nameserver</literal></term>
       <listitem>
        <para>
         Internal name server for the cluster
        </para>
       </listitem>
      </varlistentry>
     </variablelist>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="horizon_stack_options.png" width="100%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="horizon_stack_options.png" width="100%"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
     <para>
     Click <guimenu>Launch</guimenu>.
     </para>
    </step>
    <step>
     <para>
      After the cluster has been started and the cluster overview shows
      <guimenu>Create Complete</guimenu>, you need to find the external IP
      address for the admin node of your cluster (here:
      <literal>10.86.1.72</literal>). Now visit that IP address in your browser.
      You should see the &dashboard; login page and can continue with
      <xref linkend="sec.deploy.nodes.admin_configuration"/>.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="horizon_stack_resources.png" width="100%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="horizon_stack_resources.png" width="100%"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.deploy.preparation.openstack.cli">
   <title>Using the &ostack; CLI</title>
   <note>
    <para>
     You need to have access to the &ostack; command-line tools. You can
     either access those via <literal>ssh</literal> on your &soc; admin
     server or
     <link xlink:href="https://docs.openstack.org/newton/user-guide/common/cli-install-openstack-command-line-clients.html">
     install a local <command>openstack</command> client</link>.
    </para>
    <para>
     To use the local client, you need to access <guimenu>Project &rarr; Compute &rarr;
     Access &amp; Security</guimenu> in the Horizon Dashboard and click on the
     <guimenu>Download &ostack; RC File v3</guimenu>.
    </para>
    <para>
     The downloaded file is a script that you then need to load using the
     <command>source</command> command. The script will ask you for your
     &soc; password.
    </para>
     <screen>&prompt.user;<command>source container-openrc.sh</command></screen>
   </note>
   <procedure>
     <step>
      <para>
       Upload the container image to &ostack; Glance (Image service). This
       example uses the name <literal>CaaSP-3</literal> as the name of the
       image that is created in &soc;.
      </para>
     <screen>&prompt.user;<command>openstack image create --public --disk-format qcow2 \
--container-format bare \
--file SUSE-CaaS-Platform-3.0-OpenStack-Cloud.x86_64-1.0.0-GM.qcow2 \
<replaceable>CaaSP-3</replaceable></command></screen>
     </step>
     <step>
      <warning>
       <title>Replace the default <literal>root_password</literal></title>
        <para>
         Do not use the <filename>caasp-environment.yaml</filename> directly
         from the GitHub repository.
        </para>
        <para>
         You must make sure to replace the value for
         <literal>root_password</literal> with a secure password.
         This will become the password for the &rootuser; account on all
         nodes in the stack.
        </para>
     </warning>
      <para>
       Download the <filename>caasp-stack.yaml</filename> and
       <filename>caasp-environment.yaml</filename> Heat templates to your
       workstation and then run the <command>openstack stack create</command>
       command.
      </para>
      <screen>&prompt.user;<command>openstack stack create \
-t caasp-stack.yaml \
-e caasp-environment.yaml \
--parameter image=<replaceable>CaaSP-3</replaceable> <replaceable>caasp3-stack</replaceable></command></screen>
     </step>

    <step>
     <para>
      Find out which (external) IP address was assigned to the admin node
      of your &productname; cluster (here: <literal>10.81.1.51</literal>).
     </para>
     <screen>&prompt.user;<command>openstack server list --name "admin" | awk 'FNR > 3 {print $4 $5 $9}'</command>
caasp3-stack-admin|10.81.1.51</screen>
     </step>
     <step>
      <para>
       Visit the external IP address in your browser. You should see the &dashboard;
       login page and can continue with <xref linkend="sec.deploy.nodes.admin_configuration"/>.
      </para>
    </step>
   </procedure>
  </sect2>
 </sect1>

 <sect1 xml:id="sec.deploy.preparation.public_cloud">
  <title>Installing in Public Cloud</title>
  <sect2 xml:id="sec.deploy.preparation.public_cloud.overview">
   <title>Overview</title>
   <para>
    The &productname; images published by &suse; in selected Public Cloud
    environments are provided as <literal>Bring Your Own Subscription
    (BYOS)</literal> images. &productname; instances need to be registered with
    the &scc; in order to receive bugfix and security
    updates. Images labeled with the <literal>cluster</literal> designation in
    the name are not intended to be started directly; they are deployed by the
    Administrative node. Administrative node images contain the
    <literal>admin</literal> designation in the image name.
   </para>
   <para>
    The following procedure outlines the deployment process:
   </para>
   <procedure>
    <step>
     <para>
      Read the special system requirements for public cloud
      installations in <xref
      linkend="sec.deploy.requirements.public_cloud" />.
     </para>
    </step>
    <step>
     <para>
      Provision the cluster nodes. For details, see <xref linkend=
      "sec.deploy.preparation.public_cloud.provisioning" />.
     </para>
    </step>
    <step>
     <para>
      Deploy the admin node with <command>caasp-admin-setup</command>.
      For details, see <xref
      linkend="sec.deploy.nodes.admin_install_cli" />.
     </para>
    </step>
    <step>
     <para>
      Finish bootstrapping your cluster. The provisioned worker nodes
      are ready to be consumed into the cluster. For details, see <xref
      linkend= "sec.deploy.install.bootstrap" />.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.deploy.preparation.public_cloud.provisioning">
   <title>Provisioning Cluster Nodes</title>
   <sect3 xml:id="sec.deploy.preparation.public_cloud.provisioning.aws">
    <title>Amazon Web Services EC2</title>
    <para>
     You may select from one of the predefined instance types, hand selected
     for general container workloads, or choose <guimenu>Other
     types...</guimenu> and enter any <guimenu>instance type</guimenu>, as
     defined at <link xlink:href="https://aws.amazon.com/ec2/instance-types/">
     https://aws.amazon.com/ec2/instance-types/</link>
    </para>
    <para>
     Two configuration options are required in EC2:
    </para>
    <variablelist>
     <varlistentry>
      <term>Subnet ID</term>
      <listitem>
       <para>
        The <literal>subnet</literal> within which cluster nodes will be
        attached to the network, in the form
        <literal>subnet-xxxxxxxx</literal>.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Security Group ID</term>
      <listitem>
       <para>
        The <literal>security group</literal> defining network access rules for
        the cluster nodes, in the form <literal>sg-xxxxxxxx</literal>.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     The defaults used for those two options are preset to the subnet ID of the
     administration host and the security group ID that was automatically
     created by <command>caasp-admin-setup</command>. You may choose to place
     the cluster nodes in a different subnet and you can also use a custom
     security group, but please bear in mind that traffic must be allowed
     between the individual cluster nodes and also between the admininstration
     node and the cluster nodes.
    </para>
    <para>
     See the <link xlink:href="https://aws.amazon.com/documentation/vpc/">
     Amazon Virtual Private Cloud Documentation</link>for more information.
    </para>
   </sect3>
   <sect3 xml:id="sec.deploy.preparation.public_cloud.provisioning.azure">
    <title>Microsoft Azure</title>
    <para>
     You need to configure credentials for access to the Azure framework so
     instances can be created, as well as parameters for the cluster node
     instances themselves. The credentials refer to authentication via a
     service principal. See
     <link xlink:href="https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-create-service-principal-portal">
     https://docs.microsoft.com/en-us/azure/azure-resource-manager/resource-group-create-service-principal-portal</link>
     for more information on how you can create a service principal.
    </para>
    <variablelist>
     <varlistentry>
      <term>Subscription ID</term>
      <listitem>
       <para>
        The subscription ID of your Azure account.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Tenant ID</term>
      <listitem>
       <para>
        The tenant ID of your service principal, also known as the Active
        Directory ID.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Application ID</term>
      <listitem>
       <para>
        The application ID of your service principal.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Client Secret</term>
      <listitem>
       <para>
        The key value or password of your service principal.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     Below the <guimenu>Service Principal Authentication</guimenu> box you will
     find the <guimenu>Instance Type</guimenu> configuration. You may select
     from one of the predefined instance types, hand selected for general
     container workloads, or choose <guimenu>Other types...</guimenu> and enter
     any <literal>size</literal>, as defined at
     <link xlink:href="https://docs.microsoft.com/en-us/azure/virtual-machines/linux/sizes/">
     https://docs.microsoft.com/en-us/azure/virtual-machines/linux/sizes/</link>Set
     the <guimenu>Cluster size</guimenu> using the slider.
    </para>
    <para>
     The parameters in <guimenu>Resource Scopes</guimenu> define attributes of
     the cluster instances, as required for Azure Resource Manager:
    </para>
    <variablelist>
     <varlistentry>
      <term>Resource Group</term>
      <listitem>
       <para>
        The Resource Group in which all cluster nodes will be created.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Storage Account</term>
      <listitem>
       <para>
        The Storage Account that will be used for storing the cluster node OS
        disks. See
        <link xlink:href="https://docs.microsoft.com/en-us/azure/storage/common/storage-create-storage-account">
        https://docs.microsoft.com/en-us/azure/storage/common/storage-create-storage-account</link>for
        more information about Azure Storage Accounts.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Network</term>
      <listitem>
       <para>
        The virtual network the cluster nodes will be connected to.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Subnet</term>
      <listitem>
       <para>
        A subnet in the previously defined virtual network. See
        <link xlink:href="https://docs.microsoft.com/en-us/azure/virtual-network/">
        https://docs.microsoft.com/en-us/azure/virtual-network/</link> for more
        information about Azure Virtual Networks.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
   </sect3>
   <sect3 xml:id="sec.deploy.preparation.public_cloud.provisioning.google">
    <title>Google Compute Engine</title>
    <para>
     You may select from one of the predefined instance types, hand selected
     for general container workloads, or choose <guimenu>Other
     types...</guimenu> and enter any <literal>machine type</literal>, as
     defined at
     <link xlink:href="https://cloud.google.com/compute/docs/machine-types">
     https://cloud.google.com/compute/docs/machine-types</link>
    </para>
    <para>
     Two configuration options are required in GCE:
    </para>
    <variablelist>
     <varlistentry>
      <term>Network</term>
      <listitem>
       <para>
        The name of the virtual network the cluster nodes will run within.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Subnet</term>
      <listitem>
       <para>
        If you created a custom network, you must specify the name of the
        subnet within which the cluster nodes will run.
       </para>
      </listitem>
     </varlistentry>
    </variablelist>
    <para>
     See the <link xlink:href="https://cloud.google.com/vpc/docs/vpc"> GCE
     Network Documentation</link> for more information.
    </para>
   </sect3>
  </sect2>
 </sect1>
</chapter>
