<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha.deployment.scenarios"
 xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Deployment Scenarios</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker/>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <sect1 xml:id="sec.deploy.scenarios.default">
  <title>Default Scenario</title>

  <para>
   In the default scenario &productname; is deployed in such a way that its
   components have access (either direct or via proxy) to resources on the
   internet.
  </para>

  <informalfigure>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="direct_connection.png" width="100%"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="direct_connection.png" width="100%"/>
    </imageobject>
   </mediaobject>
  </informalfigure>
 </sect1>
 <sect1 xml:id="sec.deploy.scenarios.airgap">
  <title>Air gapped Deployment</title>
  <important>
   <title>Scope Of This Document</title>
   <para>
    This document focuses on providing mirrors for the resources provided by
    &suse; and required for basic &productname; functionality. If you require
    additional functionality, you can use these instructions as an example on
    how to provide additional mirrors.
   </para>
   <para>
    Providing a full set of mirroring instructions, for all usage scenarios, is
    beyond the scope of this document.
   </para>
  </important>
  <para>
   An air gapped deployment can not have any direct connection to the Internet
   or external networks.
  </para>
  <para>
   All data flowing into or out of the air gapped network must be transferred in
   a secure fashion.
  </para>

  <sect2>
   <title>Process Checklist</title>
   <procedure>
    <step>
     <para>
      Read the concepts section.
     </para>
     <para>
      <xref linkend="sec.deploy.scenarios.airgap.concepts"/>
     </para>
    </step>
    <step>
     <para>
      Deploy mirror servers on external and internal networks.
     </para>
     <para>
      <xref linkend="sec.deploy.scenarios.airgap.requirements.machines"/>
     </para>
    </step>
    <step>
     <para>
      Install &rmtool; on servers.
     </para>
     <para>
      <xref linkend="sec.deploy.scenarios.airgap.rpm-repository"/>
     </para>
    </step>
    <step>
     <para>
      Configure container image registry on servers.
     </para>
     <para>
      <xref linkend="sec.deploy.scenarios.airgap.container-registry"/>
     </para>
    </step>
    <step>
     <para>
      Configure Helm Chart repository on internal mirror.
     </para>
     <para>
      <xref linkend="sec.deploy.scenarios.airgap.helm-charts"/>
     </para>
    </step>
    <step>
     <para>
      Perform the &rmtool; update procedure to populate the RPM repository.
     </para>
     <para>
      <xref linkend="sec.deploy.scenarios.airgap.rpm-repository.update"/>
     </para>
    </step>
    <step>
     <para>
      Perform the shared update procedure to populate the Helm chart repository
      and registry services.
     </para>
     <para>
      <xref linkend="sec.deploy.scenarios.airgap.rpm-repository.update"/>
     </para>
    </step>
    <step>
     <para>
      Deploy &productname; and configure the nodes to use the respective
      services on the internal network.
     </para>
     <para>
      <xref linkend="sec.deploy.scenarios.airgap.caasp-deployment"/>
     </para>
     <para>
      RPM Packages: <xref linkend="sec.deploy.scenarios.airgap.rpm-repository.client"/>
     </para>
     <para>
      Helm Charts: <xref linkend="sec.deploy.scenarios.airgap.helm-charts.client"/>
     </para>
     <para>
      Container Images: <xref linkend="sec.deploy.scenarios.airgap.container-registry.client"/>
     </para>
    </step>
   </procedure>

  </sect2>

  <sect2 xml:id="sec.deploy.scenarios.airgap.concepts">
   <title>Concepts</title>
   <sect3>
    <title>Network Separation</title>
    <para>
     For an air gapped scenario we assume a network separation into three logical
     parts.
    </para>
   <informalfigure>
    <mediaobject>
     <imageobject role="fo">
      <imagedata fileref="airgap.png" width="100%"/>
     </imageobject>
     <imageobject role="html">
      <imagedata fileref="airgap.png" width="100%"/>
     </imageobject>
    </mediaobject>
   </informalfigure>
   <variablelist>
    <varlistentry>
     <term>Upstream</term>
     <listitem>
      <para>
       Outside the controlled network.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>External</term>
     <listitem>
      <para>
       Inside the controlled network, outside the air gapped network.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Internal</term>
     <listitem>
      <para>
       Inside the air gapped network.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    The following instructions will use these three terms to refer to parts of
    the infrastructure. For example: "internal mirror" refers to the mirroring
    server on the air gapped network. The terms <literal>air gapped</literal> and
    <literal>internal</literal> will be used interchangeably.
   </para>
  </sect3>
  <sect3>
   <title>Mirrored Resources</title>
   <para>
    In order to disconnect &productname; from the external network, we provide
    ways for the components to retrieve data from alternative sources
    inside the internal (air gapped) network.
   </para>
   <para>
    You will need to create a mirror server inside the internal network; which
    acts as a replacement for the default sources.
   </para>
   <para>
    The three main sources that must be replaced are:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      &suse; &mos; RPM packages
     </para>
     <para>
      Provided by the &suse; package repositories
     </para>
    </listitem>
    <listitem>
     <para>
      Helm installation charts
     </para>
     <para>
      Provided by the &suse; helm chart repository
      (https://kubernetes-charts.suse.com/)
     </para>
    </listitem>
    <listitem>
     <para>
      Container images
     </para>
     <para>
      Provided by the &suse; container registry (https://registry.suse.com)
     </para>
    </listitem>
   </itemizedlist>
   <para>
    You will provide replacements for these resources on a dedicated server
    inside your internal (air gapped) network.
   </para>
   <para>
    The internal mirror must be updated with data retrieved from the original
    upstream sources; in a trusted and secure fashion. To achieve this, you will
    need an additional mirroring server outside of the air gapped network which
    acts as a first stage mirror and allows retrieving data from the internet.
   </para>
   <para>
    Updating of mirrors happens in three stages.
   </para>
   <orderedlist>
    <listitem>
     <para>
      Update the external mirror from upstream.
     </para>
    </listitem>
    <listitem>
     <para>
      Transfer the updated data onto a trusted storage device.
     </para>
    </listitem>
    <listitem>
     <para>
      Update the internal mirror from the trusted storage device.
     </para>
    </listitem>
   </orderedlist>
   <para>
    Once the replacement sources are in place, the key components are
    reconfigured to use the mirrors as their main sources.
   </para>
  </sect3>

  <sect3>
   <title>RPM Package Repository Mirroring</title>
   <para>
    Mirroring of the RPM repositories is handled by the
    <link xlink:href="https://www.suse.com/documentation/sles-15/book_rmt/data/book_rmt.html">&rmtool;</link>
    for &sls; 15. The tool provides functionality that mirrors the upstream
    &suse; package repositories on the local network. This is intended to
    minimize reliance on &suse; infrastructure for updating large volumes of
    machines. The air gapped deployment uses the same technology to provide the
    packages locally for the air gapped environment.
   </para>
   <para>
    &sls; bundles software packages into so called modules. You must enable the
    <literal>Containers</literal> module in addition to the modules enabled by
    default. All enabled modules need to be mirrored inside the air gapped
    network in order to provide the necessary software for other parts of this
    scenario.
   </para>
   <para>
    &rmtool; will provide a repository server that holds the packages
    and related metadata for &mos;; to install them like from the upstream
    repository. Data is synchronized once a day to the external mirror
    automatically or can be forced via the CLI.
   </para>
   <para>
    You can copy this data to your trusted storage at any point
    and update the internal mirror.
   </para>
  </sect3>

   <sect3>
    <title>Helm Chart and Container Image Mirroring</title>
    <para>
     &productname; uses <link xlink:href="https://www.helm.sh/">Helm</link> as
     one method to install additional software on the cluster. The logic behind
     this relies on <literal>Charts</literal>, which are configuration files
     that tell &kube; how to deploy software and its dependencies. The actual
     software installed using this method is delivered as
     <literal>container images</literal>. The download location of the container
     image is stored inside the Helm chart.
    </para>
    <para>
     Container images are provided by &suse; and others on
     so called registries. The &suse; container registry is used to update the
     &productname; components.
    </para>
    <para>
     To mirror container images inside the air gapped environment, you will run
     two container image registry services that are used to pull and in turn serve these
     images. The registry service is shipped as a container image itself.
    </para>
    <para>
     Helm charts are provided independently from container images and can be
     developed by any number of sources. Please make sure that you trust
     the origin of container images referenced in the helm charts.
    </para>
    <para>
     We provide <link xlink:href="https://github.com/openSUSE/helm-mirror">helm-mirror</link>
     to allow downloading all charts present in a chart repository in bulk and
     moreover to extract all container image URLs from the charts.
     <link xlink:href="https://github.com/containers/skopeo">skopeo</link> is
     used to download all the images referred to in the Helm charts from their
     respective registry.
    </para>
    <para>
     Helm charts will be provided to the internal network by a webserver and
     refer to the container images hosted on the internal registry mirror.
    </para>
    <para>
     Once mirroring is configured, you will not have to modify Dockerfile(s) or
     &kube; manifests to use the mirrors. The requests are passed through
     the container engine which forwards them to the configured mirrors. For
     example: All images with a prefix <literal>registry.suse.com/</literal>
     will be automatically pulled from the configured (internal) mirror instead.
    </para>
    <para>
     For further information on registry mirror configuration, refer to
     <xref linkend="sec.admin.velum.registry" /> and
     <xref linkend="sec.admin.velum.mirror" />.
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="sec.deploy.scenarios.airgap.requirements">
   <title>Requirements</title>
   <sect3 xml:id="sec.deploy.scenarios.airgap.requirements.machines">
    <title>Mirror Servers</title>
    <note>
     <title>Shared Mirror Server</title>
     <para>
      If you have multiple &productname; clusters or a very large amount of
      nodes accessing the mirrors, you should increase the sizing of CPU/RAM.
     </para>
     <para>
      Storage sizing depends on your intended update frequency and data
      retention model. If you want to keep snapshots or images of repository
      states at various points, you must increase storage size accordingly.
     </para>
    </note>
    <para>
     You will need to provide and maintain at least two machines in addition to
     your &productname; cluster. These mirror servers will reside on the
     external part of your network and the internal (air gapped) network
     respectively.
    </para>
    <para>
     For more information on the requirements of a &sle; 15 server, refer to:
     <link xlink:href="https://www.suse.com/documentation/sles-15/singlehtml/book_sle_deployment/book_sle_deployment.html#part.prep">Installation Preparation</link>.
    </para>
    <variablelist>
     <varlistentry>
      <term>External</term>
      <listitem>
       <para>
        This machine will host the <literal>&rmtool;</literal> for RPM packages
        and the <literal>container image registry</literal> for container
        images.
       </para>
       <itemizedlist>
        <listitem>
         <para>
          <literal>1</literal> Host machines for the mirror servers.
         </para>
         <itemizedlist>
          <listitem>
           <para>
            SLES 15
           </para>
          </listitem>
          <listitem>
           <para>
            2 (v)CPU
           </para>
          </listitem>
          <listitem>
           <para>
            4 GB RAM
           </para>
          </listitem>
          <listitem>
           <para>
            250 GB Storage
           </para>
          </listitem>
         </itemizedlist>
        </listitem>
       </itemizedlist>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>Internal (Air gapped)</term>
      <listitem>
       <para>
        This machine will host the <literal>&rmtool;</literal> for RPM packages,
        and <literal>container image registry</literal> for container images
        as well as a webserver hosting the <literal>Helm chart repository</literal>
        files.
       </para>
       <itemizedlist>
        <listitem>
         <para>
          <literal>1</literal> Host machines for the mirror servers.
         </para>
         <itemizedlist>
          <listitem>
           <para>
            SLES 15
           </para>
          </listitem>
          <listitem>
           <para>
            2 (v)CPU
           </para>
          </listitem>
          <listitem>
           <para>
            8 GB RAM
           </para>
          </listitem>
          <listitem>
           <para>
            500 GB Storage
           </para>
          </listitem>
         </itemizedlist>
        </listitem>
       </itemizedlist>
      </listitem>
     </varlistentry>
    </variablelist>
    <important>
     <title>Adjust Number Of Mirror Servers</title>
     <para>
      This scenario description does not contain any fallback contingencies for
      the mirror servers. Add additional mirror servers (behind a loadbalancer)
      if you require additional reliability/availability.
     </para>
    </important>
    <procedure>
     <title>Provision Mirror Servers</title>
     <step>
      <para>
       <link xlink:href="https://www.suse.com/documentation/sles-15/book_quickstarts/data/art_sle_installquick.html">Set
       up two &sls; 15 machines</link> one on the internal network and one on
       the air gapped network.
      </para>
     </step>
     <step>
      <para>
       Make sure you have
       <link xlink:href="https://www.suse.com/documentation/sles-15/book_sles_docker/data/preparation.html">enabled
       the <filename>Containers</filename> module</link> on each server.
      </para>
     </step>
     <step>
      <para>
       You will need SSL/TLS certificates to secure services on each server.
       Retrieve appropriate certificates from your network administrator.
       Make sure the certificates are available system wide so they can be used
       by the deployed components.
      </para>
     </step>
    </procedure>
   </sect3>

   <sect3 xml:id="sec.deploy.scenarios.airgap.requirements.network">
    <title>Networking</title>
    <para>
     All members of the &productname; cluster must be able to communicate with
     the internal mirror server(s) within the air gapped network. You must
     configure at least these ports in all firewalls between the cluster and
     the internal mirror:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       80 HTTP - &rmtool; Server and Helm chart repository mirror
      </para>
     </listitem>
     <listitem>
      <para>
       5000 HTTPS - Container image registry
      </para>
     </listitem>
    </itemizedlist>
    <para>
     The external mirror server must be able to exchange outgoing traffic with
     upstream sources on ports <literal>80</literal> and <literal>443</literal>.
    </para>
    <note>
     <title>Additional Port Configuration</title>
     <para>
      If you choose to add more container image registries to your internal
      network, these must run on different ports than the standard registry
      running on <literal>5000</literal>. Configure your network to allow for
      this communication accordingly.
     </para>
    </note>
   </sect3>

   <sect3 xml:id="sec.deploy.scenarios.airgap.requirements.storage">
    <title>Trusted Storage</title>
    <para>
     Transferring data from the external network mirror to the internal mirror
     can be performed in many ways. The most common way is portable storage (USB
     keys or external hard drives).
    </para>
    <para>
     Sizing of the storage is dependent on the number of data sources that need
     to be stored. Container images can easily measure several Gigabytes per
     item; although they are generally smaller for &kube; related
     applications. The overall size of any given RPM repository is at least tens
     of Gigabytes. For example: At the time of writing, the package repository
     for &sls; contains approximately <literal>36 GB</literal> of data.
    </para>
    <para>
     The storage must be formatted to a filesystem type supporting files larger
     than <literal>4 GB</literal>.
    </para>
    <para>
     We recommend external storage with at least <literal>128 GB</literal>.
    </para>
    <note>
     <title>Mount Point For Storage In Examples</title>
     <para>
      In the following procedures, we will assume the storage (when connected)
      is mounted on <filename>/mnt/storage</filename>. Please make sure to
      adjust the mountpoint in the respective command to where the device is
      actually available.
     </para>
    </note>
    <note>
     <title>Handling Of Trusted Storage</title>
     <para>
      Data integrity checks, duplication, backup, and secure handling procedures
      of trusted storage are beyond the scope of this document.
     </para>
    </note>
   </sect3>
  </sect2>

  <sect2 xml:id="sec.deploy.scenarios.airgap.rpm-repository">
   <title>RPM Repository Mirror</title>
   <sect3 xml:id="sec.deploy.scenarios.airgap.rpm-repository.mirror">
    <title>Mirror Configuration</title>
    <note>
     <title>Deploy The Mirror Before &productname; Cluster Deployment</title>
     <para>
      The mirror on the air gapped network must be running and populated before
      <!-- deploying &productname;. Starting with &productname; v4 many components
      will no longer be shipped with the install media and must be fetched
      from the repository. For v3 this significantly simplifies deployment since
      you don't have to reconfigure all existing nodes manually. -->
     </para>
    </note>
    <procedure>
     <title>Install &rmtool; On The Mirror Servers</title>
     <step>
      <para>
       Install the &rmtool; on both mirror servers as described in
       <link xlink:href="https://www.suse.com/documentation/sles-15/book_rmt/data/cha_rmt_installation.html">these instructions</link>.
      </para>
      <important>
       <title>Mirror Registration</title>
       <para>
        During the installation of &rmtool; you will be asked for login
        credentials. On the external mirror, you need to enter your &scc; login
        credentials to register. On the internal mirror, you can enter any
        random characters since the registration will not be possible without
        an internet connection to &scc;.
       </para>
      </important>
     </step>
     </procedure>
    <procedure>
     <title>Configure The External Mirror</title>
     <step>
     <para>
      Connect the external mirror to &scc; as described in
      <link xlink:href="https://www.suse.com/documentation/sles-15/book_rmt/data/sec_rmt_mirroring_credentials.html">these instructions</link>.
     </para>
    </step>
    </procedure>
    <procedure>
     <title>Configure The Internal Mirror</title>
     <step>
      <para>
       You need to disable the automatic repository sync on the internal server.
       Otherwise it will attempt to download information from &scc; which can
       not be reached from inside the air gapped network.
      </para>
      <screen>&prompt.root;systemctl disable rmt-server-sync.timer
      </screen>
     </step>
    </procedure>
    <para>
     Now you need to perform the update procedure to do an initial sync of data
     between the upstream sources and the external mirror and the external and
     internal mirrors. Refer to: <xref linkend="sec.deploy.scenarios.airgap.rpm-repository.update"/>.
    </para>
   </sect3>

   <sect3 xml:id="sec.deploy.scenarios.airgap.rpm-repository.client">
    <title>Client Configuration</title>
    <para>
     <link xlink:href="https://www.suse.com/documentation/sles-15/book_rmt/data/cha_rmt_client.html">Follow these instructions</link>
     to configure all &productname; nodes to use the package repository mirror server in
     the air gapped network.
    </para>
    <para>

    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="sec.deploy.scenarios.airgap.rpm-repository.update">
   <title>Updating RPM Repository Mirror</title>
   <para>
    <link xlink:href="https://www.suse.com/documentation/sles-15/book_rmt/data/sec_rmt_mirroring_export_import.html">Follow these instructions</link>
    to update the external server, transfer the data to a storage device, and
    use that device to update the air gapped server.
   </para>
  </sect2>

  <sect2 xml:id="sec.deploy.scenarios.airgap.container-registry">
   <title>Container Registry Mirror</title>
   <note>
    <title>Mirroring Multiple Image Registries / Chart Repositories</title>
    <para>
     You can mirror images and charts from multiple registries in one shared
     internal registry. We do not recommend mirroring multiple registries in a
     shared registry due to the potential conflicts.
    </para>
    <para>
     We highly recommend running separate helm chart and container registry
     mirrors for each source registry.
    </para>
    <para>
     Additional mirror registries must be run on separate mirror servers for
     technical reasons.
    </para>
   </note>

   <sect3 xml:id="sec.deploy.scenarios.airgap.container-registry.mirror">
    <title>Mirror Configuration</title>
    <para>
     The container image registry is provided as a container image itself.
     You must download the registry container from &suse; and run it on the
     respective server.
    </para>
    <note>
     <title>Internal Registry Mirror Is Read Only</title>
     <para>
      For security reasons, the internal registry mirror is configured in
      <literal>read-only</literal> mode. Therefore, pushing container images to
      this mirror will not be possible. It can only serve images that were
      previously pulled and cached by the external mirror and then uploaded to
      the internal mirror.
     </para>
     <para>
      You can modify and store your own container images on the external
      registry and transfer them with the other container images using the same
      process. If you need to be able to modify and store container images on
      the internal network, we recommend to create a new registry that will hold
      these images. The steps needed to run your own full container image
      registry are not part of this document.
     </para>
      <para>
      For more information you can refer to:
      <link xlink:href="https://www.suse.com/documentation/sles-15/book_sles_docker/data/sec_docker_registry_definition.html">SLES15 - Docker Open Source Engine Guide: What is Docker Registry?</link>.
     </para>
    </note>

    <procedure>
     <title>Set Up The External Mirror</title>
     <step>
      <para>
       SSH into the external mirror server.
      </para>
     </step>
     <step>
      <para>
       Install <filename>docker</filename>, <filename>helm-mirror</filename>
       and <filename>skopeo</filename>.
      </para>
<screen>&prompt.root;zypper in docker helm-mirror skopeo
      </screen>
     </step>
     <step>
      <para>
       Start the docker service and enable it at boot time:
      </para>
<screen>&prompt.root;<command>systemctl enable --now docker.service</command>
         </screen>
     </step>
     <step>
      <para>
       Pull the registry container image from &suse;.
      </para>
<screen>&prompt.root;<command>docker pull registry.suse.com/sles12/registry:2.6.2</command>
   </screen>
     </step>
     <step>
      <para>
       Save the pulled image to a <literal>.tar</literal> file.
      </para>
<screen>&prompt.root;<command>docker save -o /tmp/registry.tar registry.suse.com/sles12/registry:2.6.2</command>
      </screen>
     </step>
     <step>
      <para>
       Connect the trusted storage to the external mirror. Copy the registry
       image onto the storage.
      </para>
<screen>&prompt.user;<command>mv <replaceable>/tmp/registry.tar</replaceable> <replaceable>/mnt/storage/registry.tar</replaceable></command>
   </screen>
     </step>
     <step>
      <para>
       Create basic authentication credentials for the container image registry.
      </para>
      <para>
       Replace <literal>USERNAME</literal> and <literal>PASSWORD</literal> with
       proper credentials of your choosing.
      </para>
<screen>&prompt.root;<command>mkdir -p /etc/docker/registry/{auth,certs}</command>
&prompt.root;<command>docker run --entrypoint htpasswd registry.suse.com/sles12/registry:2.6.2 -Bbn <replaceable>USERNAME PASSWORD</replaceable> \
> /etc/docker/registry/auth/htpasswd</command>
 </screen>
     </step>
     <step>
      <para>
       Create the <filename>/etc/docker/registry/config.yml</filename>
       configuration file.
      </para>
      <para>
       Replace <filename>/etc/docker/registry/certs/domain.crt</filename> and
       <filename>/etc/docker/registry/certs/domain.key</filename> with the appropriate certificate
       files for your organization.
      </para>
<screen>
version: 0.1
log:
  fields:
    service: registry
auth:
  htpasswd:
    realm: basic-realm
    path: /etc/docker/registry/auth/htpasswd
storage:
  cache:
    blobdescriptor: inmemory
  filesystem:
    rootdirectory: /var/lib/registry
http:
  addr: 0.0.0.0:5000
  headers:
    X-Content-Type-Options: [nosniff]
  tls:
    certificate: /etc/docker/registry/certs/domain.crt
    key: /etc/docker/registry/certs/domain.key
health:
  storagedriver:
    enabled: true
    interval: 10s
threshold: 3
</screen>
      <para>
       For more details on the configuration, refer to:
       <link xlink:href="https://docs.docker.com/registry/configuration/">Docker
       Registry: Configuration</link>
      </para>
     </step>
     <step>
      <para>
       Start the registry container.
      </para>
<screen>&prompt.root;<command>docker run -d -p 5000:5000 --restart=always --name registry \
-v /etc/docker/registry:/etc/docker/registry:ro \
-v <replaceable>/var/lib/registry</replaceable>:/var/lib/registry registry.suse.com/sles12/registry:2.6.2</command>
       </screen>
     </step>
    </procedure>

    <para>
     Now we will move on to the internal server in the air gapped network and
     perform a nearly identical procedure there.
    </para>

    <procedure>
     <title>Set Up Internal Mirror</title>
     <step>
      <para>
       SSH into the internal mirror server.
      </para>
     </step>
     <step>
      <para>
       Install <filename>docker</filename>.
      </para>
<screen>&prompt.root;zypper in docker
      </screen>
     </step>
     <step>
      <para>
       Start the docker service and enable it at boot time:
      </para>
<screen>&prompt.root;<command>systemctl enable --now docker.service</command>
         </screen>
     </step>
     <step>
      <para>
       Connect the trusted storage to the internal mirror and load the registry
       container image to the local filesystem.
      </para>
<screen>&prompt.root;<command>docker load -i /mnt/storage/registry.tar</command>
       </screen>
     </step>
     <step>
      <para>
       Create the <filename>/etc/docker/registry/config.yml</filename>
       configuration file.
      </para>
      <para>
       Replace <filename>/etc/docker/registry/certs/domain.crt</filename> and
       <filename>/etc/docker/registry/certs/domain.key</filename> with the appropriate certificate
       files for your organization.
      </para>
<screen>&prompt.root;<command>mkdir -p /etc/docker/registry/</command></screen>
<screen>
version: 0.1
log:
  fields:
    service: registry
storage:
  cache:
    blobdescriptor: inmemory
  filesystem:
    rootdirectory: /var/lib/registry
  maintenance:
    readonly:
      enabled: true
http:
  addr: 0.0.0.0:5000
  headers:
    X-Content-Type-Options: [nosniff]
  tls:
    certificate: /etc/docker/registry/certs/domain.crt
    key: /etc/docker/registry/certs/domain.key
health:
  storagedriver:
    enabled: true
    interval: 10s
threshold: 3
         </screen>
      <para>
       For more details on the configuration, refer to:
       <link xlink:href="https://docs.docker.com/registry/configuration/">Docker
       Registry: Configuration</link>
      </para>
     </step>
     <step>
      <para>
       Start the registry container.
      </para>
<screen>&prompt.root;<command>docker run -d -p 5000:5000 --restart=always --name registry \
-v /etc/docker/registry:/etc/docker/registry:ro \
-v <replaceable>/var/lib/registry</replaceable>:/var/lib/registry registry.suse.com/sles12/registry:2.6.2</command>
       </screen>
     </step>
    </procedure>

    <para>
     Now, you should have the registries set up and listening on port
     <literal>5000</literal> on their respective servers.
    </para>
   </sect3>

   <sect3 xml:id="sec.deploy.scenarios.airgap.container-registry.client">
    <title>Client Configuration</title>
    <para>
     Using the air gapped mirror works exactly like using a traditional
     on-premise mirror. You must configure the internal registry mirror in
     &dashboard;.
    </para>
    <procedure>
     <title>Configuring Registry Mirror</title>
     <step>
      <para>
       Log in to &dashboard; on the &productname; admin node.
      </para>
     </step>
     <step>
      <para>
       Navigate to <guimenu>Settings &rarr; Mirrors</guimenu> and create a
       definition for your internal registry mirror as described in
       <xref linkend="sec.admin.velum.mirror"/>.
      </para>
     </step>
    </procedure>
   </sect3>
  </sect2>

  <sect2 xml:id="sec.deploy.scenarios.airgap.helm-charts">
   <title>Helm Chart Repository Mirror</title>
   <important>
    <para>
     To make use of the helm charts, you must complete
     <xref linkend="sec.deploy.scenarios.airgap.container-registry"/>.
    </para>
   </important>
   <para>
    The helm charts will require images available from a registry mirror. The
    charts themselves are served on a simple webserver and do not require any
    particular configuration apart from basic networking availability and a
    hostname.
   </para>
   <sect3>
    <title>Mirror Configuration</title>
    <procedure>
     <title>Setting Up Chart Repository Mirror</title>
     <step>
      <para>
       We will re-use the webserver that is shipped with &rmtool; to create
       a virtual host for the chart repository. The repository will be available
       at <literal>http://mymirror.local:80/charts</literal>.
      </para>
     </step>
    </procedure>
    <para>
     Update the Helm chart repository by following the shared update procedure
     <xref linkend="sec.deploy.scenarios.airgap.update"/>.
    </para>
   </sect3>

   <sect3 xml:id="sec.deploy.scenarios.airgap.helm-charts.client">
    <title>Client Configuration</title>
    <para>
     Add the webserver as a repo to <command>helm</command>.
     </para>
     <para>
      This step needs to be performed on a machine where Helm is installed and
      configured to talk to the Tiller server in the &productname; cluster.
      Refer to, <xref linkend="sec.admin.software.helm"/>.
     </para>
     <para>
      <literal>SUSE-MIRROR</literal> will be the name for this repository listed
      by Helm and can be freely defined.
     </para>
<screen>&prompt.user;<command>helm repo add SUSE-MIRROR <replaceable>&lt;mymirror.local&gt;/charts</replaceable></command>
   </screen>
   </sect3>
  </sect2>

  <sect2 xml:id="sec.deploy.scenarios.airgap.update">
   <title>Updating Registry Mirror And Helm Charts</title>
   <note>
    <title>Live Update Of Registry</title>
    <para>
     There is no need to stop the container image registry services while doing
     the update procedures. All changed images will be re-indexed automatically.
    </para>
   </note>
   <para>
    Helm charts and container images must be refreshed in the same procedure,
    otherwise charts might refer to image versions that are not mirrored or you
    are mirroring outdated image versions that cause the chart deployment to
    fail.
   </para>

   <procedure>
    <title>Pull Data From Upstream Sources</title>
    <step>
     <para>
      SSH into the mirror server on the external network.
     </para>
    </step>
    <step>
     <para>
     Download all charts from the repository to the filesystem
     (e.g. <filename>/tmp/charts</filename>).
     </para>
     <para>
      This action will download all charts and overwrite the existing Helm
      chart repository URL. Replace <literal>http://mymirror.local/charts</literal>
      with the hostname of the webserver providing the Helm chart repository on
      the internal network.
     </para>
<screen>&prompt.user;<command>mkdir <replaceable>/tmp/charts</replaceable></command></screen>
<screen>&prompt.user;<command>cd <replaceable>/tmp/charts</replaceable></command></screen>
<screen>&prompt.user;<command>helm-mirror --new-root-url <replaceable>http://mymirror.local/charts</replaceable> https://kubernetes-charts.suse.com <replaceable>/tmp/charts</replaceable></command></screen>
   </step>
   <step>
    <para>
     Translate the chart information into the <literal>skopeo</literal> format.
    </para>
<screen>&prompt.user;<command>mkdir <replaceable>/tmp/skopeodata</replaceable></command></screen>
<screen>&prompt.user;<command>helm-mirror inspect-images <replaceable>/tmp/charts</replaceable> -o skopeo=<replaceable>sync.yaml</replaceable></command></screen>
   </step>
   <step>
    <para>
     Download all the referenced images using <command>skopeo</command>.
    </para>
<screen>&prompt.user;<command>skopeo sync --source-yaml <replaceable>sync.yaml</replaceable> dir:<replaceable>/tmp/skopeodata</replaceable></command></screen>
    <para>
     <command>skopeo</command> will automatically create a directory named
     after the hostname of the registry from which you are downloading the
     images. The final path will be something like <filename>/tmp/skopeodata/registry.suse.com/</filename>.
    </para>
   </step>
   <step>
    <para>
     Populate the local registry with the downloaded data.
    </para>
    <para>
     For <literal>--dest-creds</literal> you must use the credentials you
     created during <xref linkend="sec.deploy.scenarios.airgap.container-registry.mirror" />.
    </para>
<screen>&prompt.user;<command>skopeo sync --dest-creds <replaceable>USERNAME:PASSWORD</replaceable> \
dir:<replaceable>/tmp/skopeodata/registry.suse.com/</replaceable> <replaceable>docker://mymirror.local:5000</replaceable></command>
     </screen>
   </step>
   <step>
    <para>
     After the synchronization is done, you can remove the <filename>skopeodata</filename>
     directory.
    </para>
<screen>&prompt.user;<command>rm -rf <replaceable>/tmp/skopeodata</replaceable></command></screen>
   </step>
  </procedure>

   <procedure>
    <title>Transfer Data To Secure Storage</title>
    <step>
     <para>
      Connect the trusted storage to the external mirror.
     </para>
    </step>
    <step>
     <para>
      Transfer the container image data to the trusted storage. This will remove
      all files and directories that are no longer present on the external host
      from the trusted storage.
     </para>
<screen>&prompt.user;<command>rsync -aP /var/lib/registry/ <replaceable>/mnt/storage/registry/</replaceable> --delete</command>
     </screen>
    </step>
    <step>
     <para>
      Transfer the helm chart data to the trusted storage.
     </para>
<screen>&prompt.user;<command>rsync -aP <replaceable>/tmp/charts/</replaceable> <replaceable>/mnt/storage/charts</replaceable> --delete</command>
     </screen>
    </step>
   </procedure>

   <procedure>
    <title>Update Internal Mirror</title>
    <step>
     <para>
      Connect the trusted storage to the internal mirror.
     </para>
    </step>
    <step>
     <para>
      Transfer the container image data to the internal mirror. This will remove
      all files and directories that are no longer present on the trusted storage
      from the internal mirror.
     </para>
     <para>
      The target directory is <filename>/var/lib/registry</filename>.
     </para>
<screen>&prompt.user;<command>rsync -aP <replaceable>/mnt/storage/registry/</replaceable> <replaceable>/var/lib/registry/</replaceable> --delete</command>
     </screen>
    </step>
    <step>
     <para>
      Transfer the helm chart data to the internal mirror. This will remove
      all charts that do not exist on the trusted storage. If you have added
      any charts to the location manually, please back up these first and
      restore after the sync from the trusted storage is done.
     </para>
<screen>&prompt.user;<command>rsync -aP <replaceable>/mnt/storage/charts/</replaceable> <replaceable>/srv/www/htdocs/charts/</replaceable> --delete</command>
     </screen>
    </step>
   </procedure>

   <procedure>
    <title>Refresh information on the &productname; cluster</title>
    <step>
     <para>
      Update the repository information on the machine on which you are using
      Helm to install software to the cluster.
     </para>
<screen>&prompt.user;<command>helm repo update</command>
      </screen>
    </step>
   </procedure>

   <para>
    You can now deploy additional software on your &productname;. Refer to:
    <xref linkend="sec.admin.software.install" />.
   </para>
  </sect2>

  <sect2 xml:id="sec.deploy.scenarios.airgap.caasp-deployment">
   <title>Deploying &productname;</title>
   <para>
    Use the &productname; <xref linkend="book.caasp.deployment"/> as usual.
    Some of the considerations below apply; depending of the chosen installation
    medium.
    </para>
    <para>
     Make sure to add the CA certificate of your &rmtool; server as a systemwide
     certificate in &dashboard; during the &productname; deployment.
    </para>
   <sect3>
    <title>Using the ISO</title>
    <para>
     From &yast; register the node against the &rmtool; server. This will ensure
     the node zypper repositories are pointed against &rmtool;. Moreover,
     all the available updates are going to be installed and there is no need to
     install updates using <command>transactional-update</command> right after the
     installation.
    </para>
   </sect3>
   <sect3>
    <title>Using AutoYast</title>
    <para>
     Ensure the admin node is registered against &rmtool;, that will ensure the
     nodes that are provisioned by AutoYaST are registered against &rmtool; to
     have all the updates applied.
    </para>
   </sect3>
   <sect3>
    <title>Using a prebuilt image (eg: KVM, Xen)</title>
    <para>
     The node has to be registered against &rmtool;. Refer to:
     <xref linkend="sec.deploy.scenarios.airgap.rpm-repository.client"/>.
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="sec.deploy.scenarios.airgap.troubleshooting">
   <title>Troubleshooting</title>
   <sect3>
    <title>Skopeo Fails Because Of Self Signed Certificate</title>
    <para>
     If you are using a self-signed certificate for the registry you can use the
     <literal>--dest-cert-dir /path/to/the/cert</literal>
     parameter to provide the certificate.
    </para>
   </sect3>
   <sect3>
    <title>Registering An Existing Node against &rmtool;</title>
    <para>
     Refer to: <xref linkend="sec.deploy.scenarios.airgap.rpm-repository.client"/>.
    </para>
   </sect3>
  </sect2>
 </sect1>
</chapter>
