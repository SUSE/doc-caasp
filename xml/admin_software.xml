<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.1" xml:id="cha.admin.software"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Software Management</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker/>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>

 <sect1 xml:id="sec.admin.software.transactional-updates">
  <title>Transactional Updates</title>

  <para>
   For security and stability reasons, the operating system and application
   should always be up-to-date. While with a single machine you can keep the
   system up-to-date quite easily by running several commands, in a
   large-scale cluster the update process can become a real burden. Thus
   transactional automatic updates have been introduced. Transactional updates
   can be characterized as follows:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     They are atomic.
    </para>
   </listitem>
   <listitem>
    <para>
     They do not influence the running system.
    </para>
   </listitem>
   <listitem>
    <para>
     They can be rolled back.
    </para>
   </listitem>
   <listitem>
    <para>
     The system needs to be rebooted to activate the changes.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Transactional updates are managed by the <command>transactional-update</command>
   script, which is called once a day. The script checks if any updates are
   available. If there are any updates to be applied, a new snapshot of the root
   file system is created in the background and is updated from the release
   channels. All updates released to this point are applied. The running
   filesystem/machine state is left untouched.
  </para>
  <para>
   The new snapshot, once completely updated, is then marked as active and will
   be used as the new default after the next reboot of the system.
  </para>
  <note>
   <title>Snapshot Activation</title>
   <para>
    For each "transaction" performed by <command>transactional-updates</command>
    a new snapshot is generated and requires a reboot to incorporate the
    changes. If another transaction is run before the reboot, only the latest
    snapshot is used and changes might be lost. If you perform manual updates
    or installation on your nodes, please make sure to reboot after each
    transaction. Refer to:
    <xref linkend="sec.admin.software.transactional-updates.installation.manual"/>.
   </para>
  </note>

  <para>
   &dashboard; will show a list of nodes that have new updates
   available for use. The cluster administrator then uses &dashboard; to reboot
   the nodes to the new snapshots to ensure the health of services and
   configuration. &dashboard; uses <command>salt</command> to safely disable
   services on the nodes, apply new snapshots, rewrite configurations and then
   bring the services and nodes back up.
  </para>

  <sect2 xml:id="sec.admin.software.transactional-updates.command">
   <title>The <command>transactional-update</command> Command</title>
   <important>
    <title>Only use when requested by SUSE Support</title>
    <para>
     This reference for <command>transactional-update</command>
     should only be used when requested by SUSE Support. Updates are handled by
     an automated process and only require user interaction for rebooting of
     nodes.
    </para>
   </important>

   <para>
    The <command>transactional-update</command> enables you to install or
    remove updates of your system in an atomic way. The updates are applied all
    or none of them if any package cannot be installed. Before the update is
    applied, a snapshot of the system is created in order to restore the
    previous state in case of a failure.
   </para>

   <para>
    If the current root file system is identical to the active root file system
    (after applying updates and reboot), run cleanup of all old snapshots:
   </para>
<screen>&prompt.root;<command>transactional-update cleanup</command></screen>
   <para>
    Other options of the command are the following:
   </para>
   <variablelist>
    <varlistentry>
     <term><literal>pkg in/install</literal></term>
     <listitem>
      <para>
       Installs a specific package from the available channels using the
       <command>zypper install</command> command. This command can also be used to
       install a PTF RPM file. Please note that the changes to the base filesystem
       only become permanent after a reboot.
      </para>
<screen>&prompt.root;<command>transactional-update pkg install <replaceable>&lt;packagename&gt;</replaceable></command></screen>
      <para>or</para>
<screen>&prompt.root;<command>transactional-update pkg install <replaceable>&lt;RPM1 RPM2&gt;</replaceable></command></screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>pkg rm/remove</literal></term>
     <listitem>
      <para>
       Removes a specific package from the available channels using the
       <command>zypper remove</command> command. This command can also be used to
       remove a PTF RPM file. Please note that the changes to the base filesystem
       only become permanent after a reboot.
      </para>
<screen>&prompt.root;<command>transactional-update pkg remove <replaceable>&lt;packagename&gt;</replaceable></command></screen>
      <para>or</para>
<screen>&prompt.root;<command>transactional-update pkg install <replaceable>&lt;RPM1 RPM2&gt;</replaceable></command></screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>up/update</literal>
     </term>
     <listitem>
      <para>
       If there are new updates available, a new snapshot is created and
       <command>zypper up/update</command> is used to update the snapshot. The
       snapshot is activated afterwards and is used as the new root file system
       after reboot.
      </para>
<screen>&prompt.root;<command>transactional-update up</command></screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>dup</literal>
     </term>
     <listitem>
      <para>
       If there are new updates available, a new snapshot is created and
       <command>zypper dup â€“no-allow-vendor-change</command> is used to
       update the snapshot. The snapshot is activated afterwards and is used as
       the new root file system after reboot.
      </para>
<screen>&prompt.root;<command>transactional-update dup</command></screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>patch</literal>
     </term>
     <listitem>
      <para>
       If there are new updates available, a new snapshot is created and
       <command>zypper patch</command> is used to update the snapshot. The
       snapshot is activated afterwards and is used as the new root file system
       after reboot.
      </para>
<screen>&prompt.root;<command>transactional-update patch</command></screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>rollback</literal>
     </term>
     <listitem>
      <para>
       The command sets the default sub volume. On systems with read-write file
       system <command>snapper rollback</command> is called. On a read-only
       file system and without any argument, the current system is set to a new
       default root file system. If you specify a number, that snapshot is used
       as the default root file system. On a read-only file system, no
       additional snapshots are created.
      </para>
<screen>&prompt.root;<command>transactional-update rollback <replaceable>SNAPSHOT_NUMBER</replaceable></command></screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>grub.cfg</literal>
     </term>
     <listitem>
      <para>
       The command creates a new grub2 config. Sometimes it is necessary to
       adjust the boot configuration, e.g. by adding additional kernel parameters.
       This can be done by editing <replaceable>/etc/default/grub</replaceable>,
       calling <command>transactional-update grub.cfg</command> and then rebooting
       the machine to activate the change. Please note that without rebooting
       the machine, the new grub config will be overwritten with the default by
       any transactional-update that takes place.
      </para>
<screen>&prompt.root;<command>transactional-update grub.cfg</command></screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>reboot</literal>
     </term>
     <listitem>
      <para>
       This parameter triggers a reboot after the action is completed.
       </para>
       <para>
        How the reboot is done depends on how <command>transactional-update</command>
        is configured. For cluster nodes this will set a Salt grain to show the
        updated node in &dashboard; as requiring reboot.
       </para>
<screen>&prompt.root;<command>transactional-update <replaceable>dup</replaceable> reboot</command></screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>--help</literal>
     </term>
     <listitem>
      <para>
       The option outputs possible options and subcommands.
      </para>
<screen>&prompt.root;<command>transactional-update --help</command></screen>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="sec.admin.software.transactional-updates.disabling">
   <title>Disabling Transactional Updates</title>
   <para>
    Even though it is not recommended, you can disable transactional updates by
    issuing the command:
   </para>
<screen>&prompt.root;<command>systemctl --now disable transactional-update.timer</command></screen>
   <note>
    <title>Disabling transaction update timer is required during upgrade</title>
    <para>
     You must disable transactional updates during the upgrade procedure from
     one version of &productname; to the next.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="sec.admin.software.transactional-updates.installation">
   <title>Applying Updates</title>
   <para>
    It is paramount that you never "hard reboot" nodes in the cluster after
    transactional updates. This will omit reconfiguring services and applications
    and will leave nodes in unhealthy, if not unsusable, states.
   </para>
   <para>
    Updates are typically applied to nodes automatically and will be flagged in
    &dashboard; for reboot. If you have nodes with pending transactional updates
    follow the steps below.
   </para>
   <note>
    <title>General Notes to the Updates Installation</title>
    <para>
     Only packages that are part of the snapshot of the root file system can be
     updated. If packages contain files that are not part of the snapshot, the
     update could fail or break the system.
    </para>
    <para>
     RPMs that require a license to be accepted cannot be updated.
    </para>
   </note>
   <para>
    After the <command>transactional-update</command> script has run on all
    nodes, &dashboard; displays any nodes in your cluster running outdated
    software. The updates are only applied after a reboot. For this purpose,
    &dashboard; enables you to update your cluster directly.
    Follow the next procedure to update your cluster.
   </para>

   <procedure>
    <title>Updating the Cluster with &dashboard;</title>
    <step>
     <para>
      Login to &dashboard;.
     </para>
    </step>
    <step>
     <para>
      If required, click <guimenu>UPDATE ADMIN NODE</guimenu> to start the
      update.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="velum_updating.png" width="100%" format="png"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="velum_updating.png" width="100%" format="png"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
     <para>
      Confirm the update by clicking <guimenu>Reboot to update</guimenu>.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="velum_reboot_and_update.png" width="100%"
         format="png"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="velum_reboot_and_update.png" width="100%"
         format="png"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
     <para>
      Now you have to wait until the &admin_node; reboots and &dashboard; is
      available again.
     </para>
    </step>
    <step>
     <para>
      Click <guimenu>update all nodes</guimenu> to update &master_node; and
      &worker_node;s.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="velum_update_nodes.png" width="100%" format="png"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="velum_update_nodes.png" width="100%" format="png"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
   </procedure>

   <sect3 xml:id="sec.admin.software.transactional-updates.installation.manual">
    <title>Applying Updates Manually</title>
    <para>
     You can use <command>transactional-update</command> to apply updates
     or install PTF files manually.
    </para>
<screen>&prompt.root;<command>transactional-update <replaceable>pkg install &lt;packagename&gt;</replaceable> reboot</command></screen>
    <para>
     If your node is accepted to the cluster, it will have been configured to
     use Salt orchestration to reboot. The updated node will show in
     &dashboard; requiring a reboot.
    </para>
    <para>
     If your node is not (yet) accepted into the cluster it will reboot
     after the transactional-update has finished.
    </para>
   </sect3>
  </sect2>

  <sect2 xml:id="sec.admin.software.transactional-updates.recovering">
   <title>Recovering From Failed Updates</title>
   <para>
    Velum notifies you about failed updates. If the update failed,
    there are several things that can be the cause. The following list
    provides an overview of things to check. For general information
    about troubleshooting, read <xref
    linkend="sec.admin.troubleshooting.overview" />.
   </para>
   <warning>
    <title>Do Not Interfere with Transactional Updates</title>
    <para>
     Do not manually interfere with transactional updates. Do so only
     if you are requested to do so by &suse; support.
    </para>
    <para>
     For details, see <xref
     linkend="sec.admin.software.transactional-updates.command" />.
    </para>
   </warning>
   <variablelist>
    <varlistentry>
     <term>Stopping Services and Reboot</term>
     <listitem>
      <para>
       Velum uses &salt; to stop all services and reboot the node. Salt
       also takes care of adjusting configuration. Check the logs of
       the &salt; master and minions for error messages. For details,
       see <xref linkend="sec.admin.logging.salt.master" /> and <xref
       linkend="sec.admin.logging.salt.minion" />.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Installing Updates</term>
     <listitem>
      <para>
       Updates are installed once a day but only applied after a reboot
       is manually triggered. If the installation of updates fails,
       Velum shows the message <literal>Update Failed</literal> as the
       node's status. In this case, log in on the node and check
       <filename>/var/log/transactional-update.log</filename> for
       problems.
      </para>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term>Starting Services</term>
     <listitem>
      <para>
       Finally, all services of the node are being restarted. Look
       which services have failed by executing <command>systemctl
       list-units --failed</command>. Then check the logs of failed
       services.
      </para>
     </listitem>
    </varlistentry>
   </variablelist>
   <para>
    The following procedure can help in some situations.
   </para>
   <procedure>
    <step>
     <para>
      Reboot all nodes.
     </para>
<screen>&prompt.root.admin;<command>docker exec -it $(docker ps -q -f name="salt-master") \
salt -P "roles:(admin|kube-(master|minion))" system.reboot</command></screen>
    </step>
    <step>
     <para>
      On the &admin_node; run
     </para>
<screen>&prompt.root.admin;<command>docker exec -it $(docker ps -q -f name="salt-master") \
 salt -P "roles:(admin|kube-(master|minion))" cmd.run "transactional-update cleanup reboot dup"</command></screen>
    </step>
    <step>
     <para>
      Reboot all nodes again.
     </para>
<screen>&prompt.root.admin;<command>docker exec -it $(docker ps -q -f name="salt-master") \
salt -P "roles:(admin|kube-(master|minion))" system.reboot</command></screen>
    </step>
    <step>
     <para>
      Start the update with debug output.
     </para>
<screen>&prompt.root.admin;<command>docker exec -it $(docker ps -q -f name="salt-master") \
salt-run -l debug state.orchestrate orch.update</command></screen>
    </step>
    <step>
     <para>
      If there is any ongoing problem, look at all the &salt; grains of
      all nodes in <filename>/etc/salt/grains</filename>. This file
      contains the status if the update is ongoing, and is therefore
      providing the "Update Retry" in Velum.
     </para>
    </step>
   </procedure>
  </sect2>
 </sect1>

 <sect1 xml:id="sec.admin.software.patch">
  <title>Program Temporary Fixes</title>

  <para>
   Program temporary fixes (PTFs) are available in the &productname;
   environment. You install them by using the
   <command>transactional-update</command> script. Typically you invoke the
   installation of PTFs by running:
  </para>

<screen>&prompt.root;<command>transactional-update reboot ptf install <replaceable>RPM1 RPM2 &hellip;</replaceable></command></screen>

  <para>
   The command installs PTF RPMs. The <literal>reboot</literal> option then
   schedules a reboot after the installation. PTFs are activate only after
   rebooting of your system.
  </para>

  <note>
   <title>Reboot Required</title>
   <para>
    If you install or remove PTFs and you call the
    <command>transactional-update</command> to update the system before reboot,
    the applied changes by PTFs are lost and need to be done again after
    reboot.
   </para>
  </note>

  <para>
   In case you need to remove the installed PTFs, use the following command:
  </para>

 <screen>&prompt.root;<command>transactional-update reboot ptf remove <replaceable>RPM1 RPM2 &hellip;</replaceable></command></screen>
 </sect1>

 <sect1 xml:id="sec.admin.software.upgrade_caasp2">
  <title>Upgrading From &productname; 2</title>
  <warning>
   <title>Read This Section Carefully</title>
   <para>
    Before executing the single steps of the upgrade procedure,
    carefully read all information in this overview section.
   </para>
  </warning>
  <para>
   As &productname; is constantly developed and improved, new versions get
   released. You are strongly advised to upgrade to a supported release. These
   upgrades may involve manual intervention.
  </para>
  <procedure xml:id="pro.admin.upgrade.procedure">
   <title>Overview of Upgrade Procedure</title>
   <step>
    <para>
      Plan a maintenance window. Upgrades may take some time, during
      which services may be degraded in performance or completely
      unavailable.
    </para>
   </step>
   <step>
    <para>
     If you are using <emphasis>&rmtool;</emphasis> or
     <emphasis>&smtool;</emphasis>, enable the &productname; 3
     repositories and mirror the packages.
    </para>
   </step>
   <step>
    <para>
     Install all updates for &productname; 2. For details, see <xref
     linkend= "sec.admin.software.upgrade_caasp2.prereq" />
    </para>
   </step>
   <step>
    <para>
     Disable automatic updates during the upgrade procedure. For
     details, see <xref linkend=
     "sec.admin.software.upgrade_caasp2.timer" />.
    </para>
   </step>
   <step>
    <para>
     Upgrade the nodes. For details, refer to <xref linkend=
     "sec.admin.software.upgrade_caasp2.upgrade" />.
    </para>
   </step>
   <step>
    <para>
     Reboot all nodes. For details, refer to <xref linkend=
     "sec.admin.software.upgrade_caasp2.reboot" />.
    </para>
   </step>
  </procedure>
  <sect2 xml:id="sec.admin.software.upgrade_caasp2.prereq">
   <title>Install &productname; 2 Updates</title>
   <para>
    Before you start the upgrade procedure to &productname; v3, you must ensure
    that all your nodes are running on the latest v2 updates. You can check the
    <filename>SUSEConnect</filename> package version to see if you are up to date.
    To do so you will run a <command>salt</command> command to display the
    package version installed on each node.
   </para>

   <screen>
&prompt.user;<command>docker exec -i  $(docker ps -q -f name="salt-master") \
salt --batch 10 -P "roles:(admin|kube-(master|minion))" \
cmd.run "rpm -q SUSEConnect"</command>

Executing run on ['12cda3c374144d74804298bdee4d686c',
                  '9b6d8d28393045c0914c959d0a5c0e33',
                  '73b92dd7816147058c3d0fbb67fb18f9',
                  'admin']
admin:
    SUSEConnect-0.3.11-3.15.1.x86_64
jid:
    20180809103558881056
retcode:
    0
73b92dd7816147058c3d0fbb67fb18f9:
    SUSEConnect-0.3.11-3.15.1.x86_64
jid:
    20180809103558881056
retcode:
    0
9b6d8d28393045c0914c959d0a5c0e33:
    SUSEConnect-0.3.11-3.15.1.x86_64
jid:
    20180809103558881056
retcode:
    0
12cda3c374144d74804298bdee4d686c:
    SUSEConnect-0.3.11-3.15.1.x86_64
jid:
    20180809103558881056
retcode:
    0
   </screen>

   <para>
    If the package version is <literal>0.3.11-3.15.1</literal> (or higher) you
    have the latest updates from the v2 channel installed.
   </para>
  </sect2>

   <sect2 xml:id="sec.admin.software.upgrade_caasp2.timer">
    <title>Disable Automatic Updates</title>
    <para>
    To begin with the upgrade procedure, you first must disable the automatic
    transactional update mechanism to avoid conflicts. To do so you must run a
    <command>salt</command> command across the nodes to disable the
    <literal>transactional-update.timer</literal>.
    </para>
    <para>
     The automatic update timer will be re-enabled automatically after the
     migration procedure.
    </para>
    <screen>
&prompt.user;<command>docker exec -i $(docker ps -q -f name="salt-master") \
salt --batch 10 -P "roles:(admin|kube-(master|minion))" \
cmd.run "systemctl disable --now transactional-update.timer"</command>

Executing run on ['5f6688bbeac94d2ab5c4330dc7043fb2',
                  'c3afd049edbe43afb4e2e5913a88291b',
                  '5bf346291a18406290886c2e2f7c3e3f',
                  'admin']

5bf346291a18406290886c2e2f7c3e3f:
    Removed symlink /etc/systemd/system/timers.target.wants/transactional-update.timer.
jid:
    20180807122220543037
retcode:
    0
admin:
    Removed symlink /etc/systemd/system/timers.target.wants/transactional-update.timer.
jid:
    20180807122220543037
retcode:
    0
c3afd049edbe43afb4e2e5913a88291b:
    Removed symlink /etc/systemd/system/timers.target.wants/transactional-update.timer.
jid:
    20180807122220543037
retcode:
    0
5f6688bbeac94d2ab5c4330dc7043fb2:
    Removed symlink /etc/systemd/system/timers.target.wants/transactional-update.timer.
jid:
    20180807122220543037
retcode:
    0
    </screen>
  </sect2>

   <sect2 xml:id="sec.admin.software.upgrade_caasp2.upgrade">
    <title>Upgrading to &productname; 3</title>
    <para>
     Run the update command across your nodes.
    </para>
    <note>
     <title>Batch size for upgrade</title>
     <para>
      In this example we have limited the number of nodes this step will be
      performed on to <literal>10 nodes</literal> at a time.
     </para>

      <para>
      This is a precaution to avoid problems on slower network connections.
      If you are performing this step on a high bandwidth connection (for
      example from within the same datacenter as the cluster), you can raise the
      number of nodes by replacing the value for the (<literal>--batch</literal>)
      parameter. It is highly recommended not to change this setting.
     </para>
    </note>
    <screen>
&prompt.user;<command>docker exec -i $(docker ps -q -f name="salt-master") \
salt --batch <replaceable>10</replaceable> -P "roles:(admin|kube-(master|minion))" \
cmd.run "transactional-update salt migration -n" \
| tee transactional-update-migration.log</command>

Executing run on ['5f6688bbeac94d2ab5c4330dc7043fb2',
                  'c3afd049edbe43afb4e2e5913a88291b',
                  '5bf346291a18406290886c2e2f7c3e3f',
                  'admin']

5bf346291a18406290886c2e2f7c3e3f:


    Executing 'zypper --root /tmp/tmp.vbaqUwrLIh --non-interactive refresh'

    Retrieving repository 'SUSE-CAASP-ALL-Pool' metadata [...done]
    Building repository 'SUSE-CAASP-ALL-Pool' cache [....done]
    Retrieving repository 'SUSE-CAASP-ALL-Updates' metadata [....done]
    Building repository 'SUSE-CAASP-ALL-Updates' cache [....done]
    All repositories have been refreshed.
    Upgrading product SUSE CaaS Platform 3.0 x86_64.

[ SNIP ... ]

    done
jid:
    20180807122253512832
retcode:
    0
    </screen>
    <para>
     During the procedure the nodes will be switched to the new release channel
     for v3, available updates are downloaded and installed, services and
     applications are reconfigured and brought up in a orderly fashion.
    </para>
    <para>
     This operation will produce a lot of output for each node. The entire output
     is mirrored to a log file <filename>transactional-update-migration.log</filename>
     to the current working directory. This log file can be very helpful should
     any of the update operations fail.
    </para>
   </sect2>

   <sect2 xml:id="sec.admin.software.upgrade_caasp2.reboot">
    <title>Reboot Cluster Nodes</title>
    <para>
     To complete the procedure, you must reboot the cluster nodes. To do this
     properly, use &dashboard; to restart the nodes.
    </para>
    <procedure>
     <step>
      <para>
       Log in to &dashboard;.
      </para>
     </step>
     <step>
      <para>
       Update the Admin node as described in
       <xref linkend="sec.admin.software.transactional-updates.installation" />.
      </para>
     </step>
     <step>
      <para>
       Update the remaining nodes as described in
       <xref linkend="sec.admin.software.transactional-updates.installation" />.
      </para>
     </step>
    </procedure>
   </sect2>

  <sect2 xml:id="sec.admin.software.upgrade_caasp2.troubleshooting">
   <title>Troubleshooting</title>
   <para>
    In case the upgrade fails, please perform the support data collection by
    running <command>supportconfig</command> on the affected nodes. Provide the
    resulting files including the <filename>transactional-update-migration.log</filename>
    to SUSE Support.
   </para>
  </sect2>
 </sect1>

 <sect1 xml:id="sec.admin.software.install">
  <title>Additional Software Installation</title>
  <para>
   Once your cluster is ready, you may want to deploy additional software
   that is not installed on &productname; by default. This chapter provides
   instructions on how to install and configure &helm;, the &kube; package
   manager.
  </para>

  <sect2 xml:id="sec.admin.software.toolchain">
   <title>Building Kernel Modules</title>
   <para>
    Some vendors will only provide certain kernel drivers or modules as source.
    In order to use these modules you must build them on the machine they are
    required on. We provide a <filename>caasp-toolchain</filename> module that
    includes all necessary tools to *build* kernel modules.
   </para>
   <para>
    A full list of tools and packages available through the module can be found
    in the <link xlink:href="https://scc.suse.com/packages?name=SUSE%20CaaS%20Platform&amp;version=3.0&amp;arch=x86_64&amp;query=&amp;module=1752">&scc;</link>.
   </para>
   <important>
    <title>Reboot Required For Toolchain</title>
     <para>
      The toolchain module must be enabled through <command>transactional-update</command>.
      Due to the nature of transactional updates, the machine must reboot at
      least twice. First to activate the module and a second time to start the
      machine from the new snapshot that incorporates the installed tools,
      packages, and libraries.
     </para>
     <para>
      Please plan for maintenance windows when setting up toolchain usage.
     </para>
    </important>
   <procedure>
    <title>Enabling <literal>caasp-toolchain</literal> Module</title>
    <step>
     <para>
      Log in to the machine where you wish to use the toolchain
     </para>
    </step>
    <step>
     <para>
      Register the <literal>caasp-toolchain</literal> module
     </para>
<screen>&prompt.root;<command>transactional-update reboot register -p caasp-toolchain/3.0/x86_64</command>
    </screen>
    <para>
     The machine will reboot to incorporate the module into the read-only
     filesystem and start from the new snapshot.
    </para>
    </step>
    <step>
     <important>
      <title>Avoid Reboots By Installing Multiple Packages</title>
      <para>
       If you wish to install multiple packages, you should install them all in
       a single operation. Each time <command>transactional-update</command> is
       run, it creates a new snapshot and discards all previous changes. The
       changes can only be persisted by starting from the new snapshot through
       reboot.
      </para>
     </important>
     <para>
      Use <command>transactional-update</command> to install the desired
      packages from the toolchain module
    </para>
<screen>&prompt.root;<command>transactional-update reboot pkg in <replaceable>binutils kernel-devel kernel-default-devel kernel-syms kernel-macros</replaceable></command>
     </screen>
     <para>
      After the operation is finished the machine will reboot and start from
      the new snapshots with the packages installed.
     </para>
    </step>
   </procedure>

   <procedure>
    <title>Disabling <literal>caasp-toolchain</literal> Module</title>
    <para>
     After you are done using the toolchain module, you can free up space by
     uninstalling the tools you no longer need and disable the toolchain module.
    </para>
    <step>
     <para>
      Uninstall the packages you no longer need
     </para>
<screen>&prompt.root;<command>transactional-update reboot pkg rm <replaceable>binutils kernel-devel kernel-default-devel kernel-syms kernel-macros</replaceable></command>
          </screen>
     <para>
      The machine will reboot and start from the new snapshot without these
      packages.
     </para>
    </step>
    <step>
     <para>
     Disable the toolchain module
    </para>
<screen>&prompt.root;<command>transactional-update reboot register -d -p caasp-toolchain/3.0/x86_64</command>
   </screen>
   <para>
    The machine will reboot and start from the new snapshot without the
    module registered.
   </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.admin.software.helm">
   <title>Deploying &helm; and &tiller;</title>
   <para>
    &helm; has two parts: &helm; is the client and &tiller; is the server component.
    &helm; runs on your remote workstation that has access to your cluster, and
    &tiller; is installed as a container on &productname; when you run &dashboard;
    for the first time. (See <xref linkend="sec.deploy.nodes.admin_configuration"/>.)
   </para>

   <para>
    You should match the &helm; version with the version of &tiller;
    that is running on your cluster. The &tiller; binary cannot report its
    version, and you need the version that is packaged inside the &tiller;
    container. Run the following command from your workstation to query the
    logs:
   </para>
   <screen>
&prompt.root;<command>kubectl logs -l name=tiller --namespace=kube-system | grep "Starting &tiller;"</command>
[main] 2018/04/04 16:48:27 Starting &tiller; v2.6.1 (tls=false)
   </screen>
   <para>
    If the log gets overwritten and loses this information, the following command
    queries the <command>rpm</command> package manager inside the container. This
    works only on &productname;/&scf; installations:
   </para>
<screen>&prompt.root;<command>kubectl exec -it $(kubectl get pods -n kube-system | awk '/tiller/{print$1}') \
-n kube-system -- rpm -q helm</command>
helm-2.6.1-1.6.x86_64
   </screen>

   <para>
    If the Linux distribution on your workstation doesn't provide the correct
    &helm; version, or you are using some other platform, see the
    <link xlink:href="https://docs.helm.sh/using_helm/#quickstart"> &helm; Quickstart Guide</link>
    for installation instructions and basic usage examples. Download the matching
    &helm; binary into any directory that is in your PATH on your workstation,
    such as your <filename>~/bin</filename> directory. Then initialize just the
    client part:
   </para>
   <screen>&prompt.user;<command>helm init --client-only</command></screen>

   <para>
    The &tiller; version that ships with &productname; is supported by &suse;.
    While &suse; does not provide support for third-party &helm; charts, you can
    easily use them if necessary.
   </para>
  </sect2>

 <sect2 xml:id="sec.admin.software.helm.installing-heapster">
  <title>Example: Installing heapster</title>
  <important>
   <para>
    By default, <literal>tiller</literal> will be installed and you only need to
     initialize data for it. Use the <literal>--client-only</literal> parameter.
   </para>
  </important>
    <procedure>
     <title>Installation of heapster</title>
     <para>
      By default, the chart repository for helm will not be known to the system.
      You must perform <command>helm init</command> to initialize the necessary
      repository files and then refresh the information using <command>helm repo
      update</command>.

      After that, you can install <literal>heapster</literal> from the &kube;
      helm charts repository.
     </para>
     <step>
     <para>
      (On CaaSP Admin Node) Initialize helm repo data.
     </para>
     <screen>&prompt.root;<command>helm init --client-only</command>
Creating /root/.helm/repository
Creating /root/.helm/repository/cache
Creating /root/.helm/repository/local
Creating /root/.helm/plugins
Creating /root/.helm/starters
Creating /root/.helm/repository/repositories.yaml
Adding stable repo with URL: https://kubernetes-charts.storage.googleapis.com
Adding local repo with URL: http://127.0.0.1:8879/charts
$HELM_HOME has been configured at /root/.helm.
Not installing Tiller due to 'client-only' flag having been set
Happy Helming!</screen>
   </step>
     <step>
      <para>Install <literal>heapster</literal> from stable/heapster &kube;
      charts repository</para>
<screen>&prompt.root;<command>helm install --name heapster-default --namespace=kube-system stable/heapster \
--version=0.2.7 --set rbac.create=true</command></screen>
     </step>
     <step>
      <para>
       Verify that <literal>heapster</literal> was deployed successfully.
      </para>
<screen>&prompt.root;<command>helm list | grep heapster</command>
heapster-default  1  Fri Jun 29 10:48:45 2018  DEPLOYED  heapster-0.2.7  kube-system</screen>
     </step>
    </procedure>
  </sect2>
 </sect1>

 <sect1 xml:id="installing.kube.dashboard">
  <title>Installing &kube; Dashboard</title>

  <important>
   <title>Technology Preview</title>
   <para>
    Even though you can install and use the community &kube; dashboard,
    &productname; currently fully supports only &dashboard;.
   </para>
  </important>

  <itemizedlist>
  <title>Requirements</title>
  <listitem>
  <para>
  Heapster version 1.3.0 or later needs to be installed on the cluster
  </para>
  </listitem>
  <listitem>
  <para>
  Helm version 2.7.2+ and kubectl version 1.8.0+ recommended
  </para>
  </listitem>
  </itemizedlist>

  <procedure>
   <title>Installation of &kube; Dashboard</title>
   <step>
    <para>
     If <literal>heapster</literal> is not installed, refer to
     <xref linkend="sec.admin.software.helm.installing-heapster"/>.
    </para>
   </step>
   <step>
<screen><command>helm install --namespace=kube-system \
--name=kubernetes-dashboard stable/kubernetes-dashboard \
--version=0.6.1</command>
     </screen>
   </step>
   <step>
    <para>
      Run <command>kubectl proxy</command> to expose the cluster on your local
      workstation.
    </para>
   </step>
   <step>
    <para>
      Visit <literal>http://127.0.0.1:8081/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/</literal>
      in your browser. You will be greeted with by a welcome page containing a
      dialog to configure authentication.
    </para>
   </step>
   <step>
    <para>
      Select <guimenu>token</guimenu> authentication. To retrieve your token refer to the value in your kubeconfig file by running the command:
    </para>
<screen><command>grep "id-token" /path/to/kubeconfig  | awk '{print $2}'</command>
    </screen>
   </step>
   <step>
    <para>
      On login cluster resources and basic metrics are populated.
    </para>
   </step>
  </procedure>

  <procedure>
   <title>Exposing the Dashboard</title>
   <step>
<screen><command>helm upgrade kubernetes-dashboard stable/kubernetes-dashboard --set service.type=NodePort</command>
</screen>
   </step>
   <step>
    <para>
      Now you may visit the dashboard at
      <literal>https://<replaceable>WORKER_NODE_ADDRESS</replaceable>:<replaceable>NODE_PORT</replaceable></literal>
      in your browser from outside of your cluster.
    </para>
   </step>
  </procedure>

 </sect1>

</chapter>
