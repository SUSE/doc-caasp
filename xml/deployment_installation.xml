<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha.deploy.install"
 xmlns="http://docbook.org/ns/docbook"
 xmlns:xi="http://www.w3.org/2001/XInclude"
 xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>&productname; Cluster Deployment</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker/>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <para>
  This chapter details the various procedures you can use to build a new &productname; 
  &productnumber; cluster.
 </para>
 
 <para>
  There are two ways to build a cluster, one of which has two variants.
 </para>
 
 <orderedlist>
  <listitem>
   <para>
    Installing nodes from installation media (for virtual machines, directly
    from ISO images, or physical media for bare-metal installations).
   </para>
   <warning>
    <title>&Admin_Node;</title>
    <para>
     You must install the &admin_node; first and configure it using
     &dashboard; before installing the other nodes in the cluster.
    </para>
   </warning>
   <para>
    With this method, there are two ways to install the worker nodes:
   </para>
   <itemizedlist>
    <listitem>
     <para>
      Individually, choosing the node type during installation. This is only
      feasible for a small number of workers.
     </para>
    </listitem>
    <listitem>
     <para>
      Installing the worker nodes using &ay; so that they receive their
      configuration automatically from the &admin_node;. This is the
      recommended method for larger clusters, but it is equally suitable for
      small clusters.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    With both methods, the procedure to install the &admin_node; is identical.
    Only installation process for the &worker_node;s differs.
   </para>
  </listitem>
  <listitem>
   <para>
     Using preinstalled virtual disk images. &suse; offers
     <literal>QCOW2</literal> images for clusters hosted on KVM or &xen;
     hypervisors.
    </para>
    <tip>
     <title>Use Disk Images</title>
     <para>
      When building a cluster from virtual machines, &suse; recommends building
      nodes from pre-installed disk images, rather than installing new instances
      from an ISO image.
     </para>
     <para>
      Virtual-disk images are available for Hyper-V, KVM, OpenStack, VMware,
      and Xen.
     </para>
     <para>
      These VM images include the relevant hypervisor's guest additions or tools
      (where this is applicable).
     </para>
    </tip>
   </listitem>
  </orderedlist>
  
  <note>
   <para>
    It is possible to start setup using PXE. For the full procedure, refer to
    the &sle; 12 Deployment Guide:
    <link xlink:href="https://www.suse.com/documentation/sles-12/singlehtml/book_sle_deployment/book_sle_deployment.html#cha.deployment.prep_boot"/>.
   </para>
   <para>
    You can directly use the <filename>initrd</filename> and
    <filename>linux</filename> files from your install media, or install the
    package <package>tftpboot-installation-CAASP-3.0</package> onto your TFTP
    server. The package provides the required <filename>initrd</filename> and
    <filename>linux</filename> files in the <filename>/srv/tftpboot/</filename>
    directory. You will need to modify the paths used in the &sle; 12
    Deployment Guide to correctly point to the files provided by the package.
   </para>
  </note>

  <sect1 xml:id="sec.deploy.install.iso">
   <title>Installation from ISO images or media</title>
   <sect2 xml:id="sec.deploy.install.iso.admin">
    <title>Installing the &Admin_Node;</title>
    <para>
     The procedure for installing the &admin_node; is identical whether or not
     you use &ay; for the rest of the cluster.
    </para>
    <procedure xml:id="pro.deploy.install.iso.admin">
     <step>
      <para>
       Connect or insert the &productname; installation media, then reboot the
       computer to start the installation program. On machines with a
       traditional BIOS, you will see the graphical boot screen shown below.
       The boot screen on machines equipped with UEFI is slightly different.
      </para>
      <para>
       SecureBoot on UEFI machines <emphasis>is</emphasis> supported.
      </para>
      <para>
       Use <keycap>F2</keycap> to change the language for the installer. A
       corresponding keyboard layout is chosen automatically. See
       <link
        xlink:href="https://www.suse.com/documentation/sles-12/book_sle_deployment/data/sec_i_yast2_startup.html"
       />
       for more information about changing boot options.
      </para>
      <informalfigure>
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="install_boot.png" width="100%"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="install_boot.png" width="100%"/>
        </imageobject>
       </mediaobject>
      </informalfigure>
     </step>
     <step>
      <para>
       Select <guimenu>Installation</guimenu> on the boot screen, then press
       <keycap
        function="enter"
       />. This boots the system and
       loads the &productname; installer.
      </para>
     </step>
     <step>
<!-- <para>
        Read the License Agreement. It is presented in the language you have
        chosen on the boot screen. <guimenu>License Translations</guimenu> are
        available. You need to accept the agreement by checking <guimenu>I Agree
        to the License Terms</guimenu> to install &productname;. Proceed with
        <guimenu>Next</guimenu>.
        </para>  -->
      <para>
       Configure the following mandatory settings on the <guimenu>Installation
       Overview</guimenu> screen.
      </para>
      <note>
       <title>Help and Release Notes</title>
       <para>
        From this point on, a brief help document and the Release Notes can be
        viewed from any screen during the installation process by selecting
        <guimenu>Help</guimenu> or <guimenu>Release Notes</guimenu>
        respectively.
       </para>
      </note>
      <variablelist>
       <varlistentry>
        <term>Keyboard Layout</term>
        <listitem>
         <para>
          The <guimenu>Keyboard Layout</guimenu> is initialized with the
          language settings you have chosen on the boot screen. Change it here,
          if necessary.
         </para>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term>Password for &rootuser;</term>
        <listitem>
         <para>
          Type a password for the system administrator account (called the
          &rootuser; user) and confirm it.
         </para>
         <warning>
          <title>Do not forget the &rootuser; Password</title>
          <para>
           You must not lose the &rootuser; password! After you enter it here,
           the password cannot be retrieved. For more information, see
           <link
            xlink:href="https://www.suse.com/documentation/sles-12/book_sle_deployment/data/sec_i_yast2_user_root.html"
           />.
          </para>
         </warning>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term>Registration Code or SMT Server URL</term>
        <listitem>
         <para>
          Enter the <guimenu>Registration Code or SMT Server URL</guimenu>. SMT
          Server URLs should use <literal>https</literal> or
          <literal>http</literal>; other protocols are not supported.
         </para>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term>System Role</term>
        <listitem>
         <para>
          From the <guimenu>System Role</guimenu> menu, choose <guimenu>
          Administration Node (Dashboard)</guimenu>.
         </para>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term>NTP Servers</term>
        <listitem>
         <para>
          Enter the host names or IP addresses of one or more <guimenu>NTP
          Servers</guimenu> for the node, separated by colons or white space. A
          single time server is sufficient, but for optimal precision and
          reliability, nodes should use at least three.
         </para>
         <para>
          For more information about <literal>NTP</literal>, refer to
          <link
           xlink:href="https://www.suse.com/documentation/sles-12/book_sle_admin/data/cha_netz_xntp.html"
          />
         </para>
        </listitem>
       </varlistentry>
      </variablelist>
      <informalfigure>
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="install_overview_admin.png" width="100%"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="install_overview_admin.png" width="100%"/>
        </imageobject>
       </mediaobject>
      </informalfigure>
      <para>
       Optionally, you can customize the following settings. If you do not make
       any changes, defaults are used. A brief summary of the settings is
       displayed below the respective settings option.
      </para>
      <variablelist>
       <varlistentry>
        <term>Partitioning</term>
        <listitem>
         <para>
          Review the partition setup proposed by the system and change it if
          necessary. You have the following options:
         </para>
         <variablelist>
          <varlistentry>
           <term>Select a hard disk</term>
           <listitem>
            <para>
             Select a disk on to which to install &productname; with the
             recommended partitioning scheme.
            </para>
           </listitem>
          </varlistentry>
          <varlistentry>
           <term><guimenu>Custom Partitioning (for Experts)</guimenu></term>
           <listitem>
            <para>
             Opens the <guimenu>Expert Partitioner</guimenu> described in
             <link
              xlink:href="https://www.suse.com/documentation/sles-12/book_sle_deployment/data/sec_yast2_i_y2_part_expert.html"
             />.
            </para>
            <warning>
             <title>For Experts only</title>
             <para>
              As the name suggests, the <guimenu>Expert Partitioner</guimenu>
              is for experts only. Custom partitioning schemes that do not meet
              the requirements of &productname; are not supported.
             </para>
             <itemizedlist>
              <title>Requirements for custom partitioning schemes</title>
              <listitem>
               <para>
                &productname; only supports the &btrfs; file system with
                OverlayFS. A read-only &btrfs; file system is used for the root
                file system, which enables transactional updates.
               </para>
              </listitem>
              <listitem>
               <para>
                For snapshots, partitions should have a capacity of at least 11
                GB.
               </para>
              </listitem>
              <listitem>
               <para>
                Depending on the number and size of your containers, you will
                need sufficient space under the <filename>/var</filename> mount
                point.
               </para>
              </listitem>
             </itemizedlist>
            </warning>
           </listitem>
          </varlistentry>
         </variablelist>
         <para>
          To accept the proposed setup without any changes, choose
          <guimenu>Next</guimenu> to proceed.
         </para>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term>Booting</term>
        <listitem>
         <para>
          This section shows the boot loader configuration. Changing the
          defaults is only recommended if really needed. For details, refer to
          <link
           xlink:href="https://www.suse.com/documentation/sles-12/book_sle_admin/data/cha_grub2.html"
          />.
         </para>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term>Network Configuration</term>
        <listitem>
         <para>
          If the network could not be configured automatically while starting
          the installation system, you should manually configure the
          <guimenu>Network Settings</guimenu>. Please make sure at least one
          network interface is connected to the Internet in order to register
          your product.
         </para>
         <para>
          By default, the installer requests a host name from the DHCP server.
          If you set a custom name in the <guimenu>Hostname/DNS</guimenu> tab,
          make sure that it is unique.
         </para>
         <para>
          For more information on configuring network connections, refer to
          <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_admin/data/sec_basicnet_yast.html"/>.
         </para>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term>&kdump;</term>
        <listitem>
         <para>
          &kdump; saves the memory image (<quote>core dump</quote>) to the file
          system in case the kernel crashes. This enables you to find the cause
          of the crash by debugging the dump file. For more information, see
          <link xlink:href="https://www.suse.com/documentation/sles-12/book_sle_tuning/data/cha_tuning_kdump_basic.html"/> .
         </para>
         <warning>
          <title>&kdump; with large amounts of RAM</title>
          <para>
           If you have a system with large amounts of RAM or a small hard
           drive, core dumps may not be able to fit on the disk. If the
           installer warns you about this, there are two options:
          </para>
          <orderedlist>
           <listitem>
            <para>
             Enter the <guimenu>Expert Partitioner</guimenu> and increase the
             size of the root partition so that it can accommodate the size of
             the core dump. In this case, you will need to decrease the size of
             the data partition accordingly. Remember to keep all other
             parameters of the partitioning (e.g. the root file system, mount
             point of data partition) when doing these changes.
            </para>
           </listitem>
           <listitem>
            <para>
             Disable &kdump; completely.
            </para>
           </listitem>
          </orderedlist>
         </warning>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term>System Information</term>
        <listitem>
         <para>
          View detailed hardware information by clicking <guimenu>System
          Information</guimenu>. In this screen you can also change
          <guimenu>Kernel Settings</guimenu>. See
          <link
           xlink:href="https://www.suse.com/documentation/sles-12/book_sle_tuning/data/cha_tuning_io.html"
          />
          for more information.
         </para>
        </listitem>
       </varlistentry>
      </variablelist>
      <para>
       Proceed with <guimenu>Next</guimenu>.
      </para>
      <tip>
       <title>Installing Product Patches at Installation Time</title>
       <para>
        If &productname; has been successfully registered at the &scc;, you are
        asked whether to install the latest available online updates during the
        installation. If you choose <guimenu>Yes</guimenu>, the system will be
        installed with the most current packages without having to apply the
        updates after installation. Activating this option is recommended.
       </para>
      </tip>
     </step>
     <step>
      <para>
       After you have finalized the system configuration on the
       <guimenu>Installation Overview</guimenu> screen, click
       <guimenu>Install</guimenu>. Up to this point no changes have been made
       to your system.
      </para>
      <para>
       Click <guimenu>Install</guimenu> a second time to start the installation
       process.
      </para>
      <informalfigure>
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="install_confirm.png" width="100%"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="install_confirm.png" width="100%"/>
        </imageobject>
       </mediaobject>
      </informalfigure>
     </step>
     <step>
      <para>
       During the installation, the progress is shown in detail on the
       <guimenu>Details</guimenu> tab.
      </para>
      <informalfigure>
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="install_perform.png" width="100%"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="install_perform.png" width="100%"/>
        </imageobject>
       </mediaobject>
      </informalfigure>
     </step>
     <step>
      <para>
       After the installation routine has finished, the computer will reboot
       into the installed system.
      </para>
     </step>
    </procedure>
   </sect2>
   
   <sect2 xml:id="sec.deploy.install.iso.config">
    <title>&Admin_Node; Configuration</title>
    <para>
     Before installing the other nodes, it is necessary to configure the
     &admin_node;.
    </para>
    <procedure xml:id="pro.deploy.install.iso.config">
     <step>
      <para>
       After the &admin_node; has finished booting and you see the login
       prompt, point a web browser to:
      </para>
      <para>
       <uri>https://<replaceable>caasp-admin.&exampledomain;</replaceable>
       </uri>
      </para>
      <para>
       ... where <literal>caasp-admin.&exampledomain;</literal> is the host
       name or IP address of the &admin_node;. The host name and IP address are
       both shown on the &admin_node; console, above the login prompt.
      </para>
<!-- cwickert 2017-07-23 TODO: We don't really need this image, do we? -->
<!-- <informalfigure>
       <mediaobject>
       <imageobject role="fo">
       <imagedata fileref="velum_login.png" width="100%"/>
       </imageobject>
       <imageobject role="html">
       <imagedata fileref="velum_login.png" width="100%"/>
       </imageobject>
       </mediaobject>
       </informalfigure> -->
     </step>
     <step>
      <para>
       To create an Administrator account, click <guimenu>Create an
       account</guimenu> and provide an e-mail address and a password. Confirm
       the password and click <guimenu>Create Admin</guimenu>. You will be
       logged into the dashboard automatically.
      </para>
      <informalfigure>
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="velum_register.png" width="100%"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="velum_register.png" width="100%"/>
        </imageobject>
       </mediaobject>
      </informalfigure>
     </step>
     <step>
      <para>
       Fill in the values <guimenu>Internal Dashboard Location</guimenu>. 
       If necessary, configure the other settings.
      </para>
      <note>
       <title>
        Host Name, FQDN or IP Address
       </title>
       <para>
        Generally, FQDNs are preferable to host names.
       </para>
       <para>For test deployments, you can use IP addresses instead of names for
        both the dashboard and API server, but this is not recommended for
        use in production.
       </para>
      </note>
      <variablelist>
       <varlistentry>
        <term>Internal Dashboard Location</term>
        <listitem>
         <para>
          FQDN or IP of the node running the &dashboard; dashboard (reachable
          from inside the cluster).
         </para>
        </listitem>
       </varlistentry>
       <varlistentry>
        <term>External Kubernetes API server FQDN</term>
        <listitem>
         <para>
          Name used to reach the node running the &kube; API server.
         </para>
         <note>
          <para>
           In a simple, single-master deployment this will be the name of the
           node that you will select as the &master_node; when bootstrapping the
           cluster.
          </para>
          <para>
            If you are planning a larger cluster with multiple &master_node;s, 
            they must all be accessible from a single host name. If not, the
            functionality of &dashboard; will degrade if the original
            &master_node; is removed. Therefore you should ensure that there is
            some form of load-balancing or reverse proxy on the value that
            you enter in this field.
          </para>
         </note>
       </listitem>
       </varlistentry>
       
<!-- cwickert 2017-07-23 TODO: Another varlist for the individual settings? -->
       
       <varlistentry>
        <term>Install Tiller (Helm's Server Component)</term>
        <listitem>
         
         <informalfigure>
          <mediaobject>
           <imageobject role="fo">
            <imagedata fileref="velum_install_tiller.png" width="100%"/>
           </imageobject>
           <imageobject role="html">
            <imagedata fileref="velum_install_tiller.png" width="100%"/>
           </imageobject>
          </mediaobject>
         </informalfigure>
         
         <para>
          If you intend to deploy &scf; on &productname;, or any other software
          that is installed with &helm; (the &kube; package manager), check the
          box to install &tiller;.
<!-- (See <xref linkend="installing.helm"/>.) -->
         </para>
        </listitem>
       </varlistentry>
       
       <varlistentry>
        <term>Overlay network settings</term>
        <listitem>
         <warning>
          <title>Adjust overlay network to avoid collision with existing services</title>
          <para>
           The overlay network settings have to be verified and adjusted so that
           they do not collide with any services / addresses in the infrastructure 
           that potentially need to be reached from any node or 
           service running within the &productname; cluster.
          </para>
          
          <para>
           For example, an oracle database is running on <literal>172.16.4.5</literal> 
           in the existing infrastructure and a pod in the cluster needs to 
           contact that database. Then, the defaults be adjusted to provide a 
           different overlay network. Another example would be an NFS server or 
           a SES/Ceph cluster running anywhere in the network 
           <literal>172.16.0.0/13</literal> and where persistent storage access 
           of the CaaSP cluster should be hosted on.
          </para>
         </warning>
         <para>
          Describes the settings used by <literal>flannel</literal> to create 
          the overlay network used by all the &kube; pods and services. With 
          this change, the default settings are exposed to the user for fine 
          tuning. The most common reason to change them is to avoid clashes 
          between the default subnetwork we picked up and an already 
          existing one.
         </para>
        </listitem>
       </varlistentry>
       
       <varlistentry>
        <term>Proxy Settings</term>
        <listitem>
         <para>
          If enabled, you can set proxy servers for <literal>HTTP</literal> and
          <literal>HTTPS</literal>. You may also configure exceptions and
          choose whether to apply the settings only to the container engine or
          to all processes running on the cluster nodes.
         </para>
        </listitem>
       </varlistentry>
       
       <varlistentry>
        <term>SUSE registry mirror</term>
        <listitem>
         <para>
          Options for SUSE registry mirror.
         </para>
        </listitem>
       </varlistentry>
     
       <varlistentry>
        <term>Cloud provider integration</term>
        <listitem>
         <para>
          Cloud provider integration enables you to deploy &productname; on 
          &ostack;/&soc;. 
         </para>
         <informalfigure>
          <mediaobject>
           <imageobject role="fo">
            <imagedata fileref="velum_cpi.png" width="100%"/>
           </imageobject>
           <imageobject role="html">
            <imagedata fileref="velum_cpi.png" width="100%"/>
           </imageobject>
          </mediaobject>
         </informalfigure>
         <variablelist>
          
          <varlistentry>
           <term>Keystone API URL</term>
           <listitem>
            <para>
             Specifies the URL of the Keystone API used to authenticate the user. 
             This value can be found in Horizon (the &ostack; control panel) under 
             <guimenu>Project &rarr; Access and Security &rarr; API Access &rarr; 
              Credentials</guimenu>.
            </para>
           </listitem>
          </varlistentry>
          
          <varlistentry>
           <term>Domain name</term>
           <listitem>
            <para>
             (Optional) Used to specify the name of the domain your user belongs
             to.
            </para>
           </listitem>
          </varlistentry>
          
          <varlistentry>
           <term>Domain ID</term>
           <listitem>
            <para>
             (Optional) Used to specify the name of the domain your user belongs
             to.
            </para>
           </listitem>
          </varlistentry>
          
          <varlistentry>
           <term>Project name</term>
           <listitem>
            <para>
             (Optional) Used to specify the name of the project where you want 
             to create your resources.
            </para>
           </listitem>
          </varlistentry>
          
          <varlistentry>
           <term>Project ID</term>
           <listitem>
            <para>
             (Optional) Used to specify the name of the project where you want 
             to create your resources.
            </para>
           </listitem>
          </varlistentry>
          
          <varlistentry>
           <term>Region name</term>
           <listitem>
            <para>
             Used to specify the identifier of the region to use when running on
             a multi-region &ostack; cloud. A region is a general division of 
             an &ostack; deployment.
            </para>
           </listitem>
          </varlistentry>
          
          <varlistentry>
           <term>Username</term>
           <listitem>
            <para>
             Refers to the username of a valid user set in Keystone.
            </para>
           </listitem>
          </varlistentry>
          
          <varlistentry>
           <term>Password</term>
           <listitem>
            <para>
             Refers to the password of a valid user set in Keystone.
            </para>
           </listitem>
          </varlistentry>
          
          <varlistentry>
           <term>Subnet UUID for CaaS Platform private network</term>
           <listitem>
            <para>
             Used to specify the identifier of the subnet you want to create
             your load balancer on. This value can be found on the &ostack;
             control panels, under <guimenu>Project &rarr; Network &rarr;
             Networks</guimenu>. Click on the respective network to see its 
             subnets.
            </para>
           </listitem>
          </varlistentry>
          
          <varlistentry>
           <term>Floating network UUID</term>
           <listitem>
            <para>
             (Optional) When specified, will lead to the creation of a floating 
             IP for the load balancer.
            </para>
           </listitem>
          </varlistentry>
          
          <varlistentry>
           <term>Load balancer monitor max retries</term>
           <listitem>
            <para>
             Number of permissible ping failures before changing the load 
             balancer memberÂ’s status to <literal>INACTIVE</literal>. Must be a 
             number between 1 and 10. (Default: <replaceable>3</replaceable>)
            </para>
           </listitem>
          </varlistentry>
          
          <varlistentry>
           <term>Cinder Block Storage API version</term>
           <listitem>
            <para>
             Specifies the API version to be used when talking to Cinder.
             Currently: <literal>v2</literal>
            </para>
           </listitem>
          </varlistentry>
          
          <varlistentry>
           <term>Ignore Cinder availability zone</term>
           <listitem>
            <para>
             Influence availability zone use when attaching Cinder volumes. When
             Nova and Cinder have different availability zones, this should be 
             set to <literal>True</literal>.
            </para>
           </listitem>
          </varlistentry>
         
         </variablelist>
        </listitem>
       </varlistentry>
     
       <varlistentry>
        <term>Container runtime</term>
        <listitem>
         <warning>
          <para>
           Please note CRI-O is currently only a tech preview. It will work but
           is not officially supported.
          </para>
         </warning>
         
         <informalfigure>
          <mediaobject>
           <imageobject role="fo">
            <imagedata fileref="velum_install_options.png" width="100%"/>
           </imageobject>
           <imageobject role="html">
            <imagedata fileref="velum_install_options.png" width="100%"/>
           </imageobject>
          </mediaobject>
         </informalfigure>
         
         <para>
          Allows choice between Docker and CRI-O as the main container runtime.
         </para>
        </listitem>
       </varlistentry>
     
       <varlistentry>
        <term>System wide certificate</term>
        <listitem>
         <para>
          Specify a system wide trusted certificate.
         </para>
        </listitem>
       </varlistentry>
      </variablelist>
     </step>
      
      <step>
       <para>
        Click <guimenu>Next</guimenu> to conifigure the FQDNs for dashboard and &kube; API.
       </para>
       <para>
        Fill in the values for <guimenu>External Dashboard Location</guimenu> and
        <guimenu>External Kubernetes API server FQDN</guimenu>. If necessary,
        configure the <guimenu>Proxy Settings</guimenu>.
       </para>
      
      <para>
       Click <guimenu>Next</guimenu> to proceed and install some cluster nodes
       as described in <xref
        linkend="sec.deploy.install.iso.node"/>.
      </para>
     </step>
    </procedure>
   </sect2>
   
   <sect2 xml:id="sec.deploy.install.iso.node">
    <title>Installing &Worker_Node;s</title>
    
    <warning>
     <para>
      Before you can install the &worker_node;s of your new cluster, you need to
      install and configure the &admin_node;. Ensure that you have completed
      the steps in <xref linkend="sec.deploy.install.iso.admin"/> and
      <xref linkend="sec.deploy.install.iso.config"/>.
     </para>
    </warning>
    
    <sect3 xml:id="sec.deploy.install.node.manual">
     <title>Manual Installation</title>
     <procedure xml:id="pro.deploy.install.node.manual">
      <step>
       <para>
        Follow the same procedure as for installing the &admin_node; in
        <xref linkend="sec.deploy.install.iso.admin"/>, up
        until selection of the <guimenu>System Role</guimenu>.
       </para>
      </step>
      <step>
       <para>
        Select <literal>Cluster Node</literal> as <guimenu>System
        Role</guimenu> and enter the host name or IP address of the
        &admin_node;.
       </para>
       <note>
        <title>Plain System</title>
        <para>
         It is also possible to select a third node type, "plain node". These
         can be used for testing and debugging purposes, but are not usually
         needed.
        </para>
       </note>
      </step>
      <step>
       <para>
        After you have finalized the system configuration on the
        <guimenu>Installation Overview</guimenu> screen, click
        <guimenu>Install</guimenu>. Up to this point no changes have been made
        to your system. After you click <guimenu>Install</guimenu> a second
        time, the installation process starts.
       </para>
       <para>
        After a reboot, the new node should appear in the dashboard and can be
        added to your cluster.
       </para>
       <para>
        Repeat this procedure at least twice more to add a minimum of three
        nodes: one &master_node; and two &worker_node;s. This is the minimum
        supported size for a &productname; cluster.
       </para>
      </step>
     </procedure>
    </sect3>
    
    <sect3 xml:id="sec.deploy.install.node.ay">
     <title>Automatic Installation Using &ay;</title>
     <para>
      Before installing &worker_node;s with &ay;, you need to obtain the URL
      that points to the &ay; file on the &admin_node;. Generally, this will be
      supplied by the &dashboard; dashboard on the &admin_node;.
     </para>
     <procedure xml:id="pro.deploy.install.node.ay">
      <step>
       <para>
        Insert the &productname; DVD into the drive, then reboot the computer
        to start the installation program.
       </para>
      </step>
      <step>
       <informalfigure>
        <mediaobject>
         <imageobject role="fo">
          <imagedata fileref="install_boot_ay.png" width="100%"/>
         </imageobject>
         <imageobject role="html">
          <imagedata fileref="install_boot_ay.png" width="100%"/>
         </imageobject>
        </mediaobject>
       </informalfigure>
       <para>
        Select <guimenu>Installation</guimenu> on the boot screen, but
        <emphasis>do not</emphasis> press <keycap function="enter"/>.
       </para>
       <para>
        Before proceeding to boot the machine, you should enter the necessary
        <guimenu>Boot Options</guimenu> for &ay; and networking.
       </para>
       <para>
        The most important options are:
       </para>
       <variablelist>
        <varlistentry>
         <term>autoyast</term>
         <listitem>
          <para>
           Path to the &ay; file. It is in the form of a URL built from the
           FQDN of the &admin_node;, followed the path to the &ay; file. For
           example, <literal>http://admin.example.com/autoyast</literal>
          </para>
          <para>
           For more information, refer to
           <link xlink:href="https://www.suse.com/documentation/sles-12/book_autoyast/data/invoking_autoinst.html#commandline_ay"/>.
          </para>
         </listitem>
        </varlistentry>
        <varlistentry>
         <term>netsetup</term>
         <listitem>
          <para>
           Network configuration. If you are using dhcp, you can simply enter
           <literal>netsetup=dhcp</literal>. For manual configuration, refer to
           <link xlink:href="https://www.suse.com/documentation/sles-12/book_autoyast/data/ay_adv_network.html"/>.
          </para>
         </listitem>
        </varlistentry>
        <varlistentry>
         <term>hostname</term>
         <listitem>
          <para>
           The host name for the node, if not provided by DHCP. If you manually
           specify a host name, make sure that it is unique.
          </para>
         </listitem>
        </varlistentry>
       </variablelist>
       <para>
        Press <keycap function="enter"/>. This boots the system and loads the
        &productname; installer.
       </para>
      </step>
      <step>
       <para>
        So long as there are no errors, the rest of the installation should
        complete automatically. After a reboot, the new  node should appear in
        the dashboard and can be added to your cluster.
       </para>
      </step>
     </procedure>

      <note>
      <title>
       &rootuser; Password
      </title>
      <para>
       When nodes are installed using &ay;, there is no opportunity to specify
       the password for &rootuser;. However, each node will have <command>ssh</command>
       keys for &rootuser; on the &master_node; pre-installed. Thus it is
       possible to access the &worker_node;s by opening an <command>ssh</command>
       session from the &master_node;.
      </para>
     </note>
      
    </sect3>
   </sect2>
   </sect1>
   
   
   <sect1 xml:id="sec.deploy.install.qcow2">
    <title>Using Pre-installed Disk Images</title>
    <para>
     For building clusters from virtual machines on supported hypervisors, it
     is not necessary to individually install each node. &suse; offers
     pre-installed VM disk images in the following formats: 
    </para>
    <variablelist>
     <varlistentry>
      <term>&kvm; and &xen;</term>
      <listitem>
       <para>
        In QCOW2 format, for &kvm; and for &xen; using full virtualization.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>&xen;</term>
      <listitem>
       <para>
        In QCOW2 format, for &xen; using paravirtualization.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>VMware</term>
      <listitem>
       <para>
        In VMDK format, for VMware ESXi.
       </para>
      </listitem>
     </varlistentry>
     <varlistentry>
      <term>
       VHD
      </term>
      <listitem>
       <para>For Microsoft Hyper-V.</para>
      </listitem>
     </varlistentry>
    </variablelist>
    
    <sect2 xml:id="sec.deploy.install.qcow2.overview">
     <title>Overview</title>
     <para>
      When deploying a cluster node from pre-installed disk images, the setup
      program never runs. Thus, it is not possible to set values such as time
      servers, node names, the &rootuser; password for the &admin_node; and so
      on in the normal way.
     </para>
     <para>
      Instead, you need to set these values using <literal>cloud-init</literal> configuration
      files. You should place these into a specific subdirectory, then make an
      ISO image. Attach this to the VM as a secondary disk. The pre-installed
      operating system on the disk image looks for a medium with the
      volume label <literal>cidata</literal>, searches that volume for the
      configuration files, and applies the settings within them.
     </para>
     <para>
      Full details of the cloud-init metadata files can be found in the
      Configuration chapter of this manual; for more information, see
      <xref linkend="sec.deploy.cloud-init"/>.
     </para>
     <para>
      As with individual node installation, the &admin_node; needs to be
      configured separately and differently from the &worker_node;s.
     </para>
    </sect2>

    <sect2 xml:id="sec.deploy.install.qcow2.procedure.openstack">
     <title>Installation on &soc;</title>
     <para>
      You can deploy a &productname; on &soc; using &ostack;. 
      You will need a &productname; machine image and &ostack; Heat templates.
      Once you have created a stack, you will continue with the &productname; setup.
     </para>
     <note>
      <title>&productname; machine image for &soc;</title>
      <para>
       Download the latest &productname; for &ostack; image from 
       <link xlink:href="https://download.suse.com">https://download.suse.com</link>
       (for example, 
       <filename>SUSE-CaaS-Platform-3.0-OpenStack-Cloud.x86_64-1.0.0-GM.qcow2</filename>).
      </para>
     </note>
      <note>
       <title>
        &ostack; Heat Templates Repository
       </title>
       <para>
        &productname; Heat templates are available from <link xlink:href="https://github.com/SUSE/caasp-openstack-heat-templates">GitHub</link>.
       </para>
     </note>
     
     <sect3>
      <title>Using the Horizon Dashboard</title>
      <procedure>

      <step>
       <para>
        Go to <guimenu>Project &rarr; Compute &rarr; Images</guimenu> and click on 
        <guimenu>Create Image</guimenu>.
       </para>
       <para>
        Give your image a name (for example: <literal>CaaSP-3</literal>); you will
        need to use this later to find the image.
       </para>
      </step>
       <step>
        <para>
         Go to <guimenu>Project &rarr; Orchestration</guimenu> and click on 
         <guimenu>Stacks</guimenu>.
        </para>
        <informalfigure>
         <mediaobject>
          <imageobject role="fo">
           <imagedata fileref="horizon_stacks.png" width="100%"/>
          </imageobject>
          <imageobject role="html">
           <imagedata fileref="horizon_stacks.png" width="100%"/>
          </imageobject>
         </mediaobject>
        </informalfigure>
       </step>
       
       <step>
        <para>
         Click on <guimenu>Launch Stack</guimenu> and provide the stack 
         templates. Either upload the files, provide the URL to the raw files 
         directly (only applies to stack template), or copy and paste the 
         contents into the <guimenu>Direct Input</guimenu> fields.
        </para>
        <warning>
         <title>Replace the default <literal>root_password</literal></title>
          <para>
           Do not use the <filename>caasp-environment.yaml</filename> directly
           from the GitHub repository.
          </para>
           <para>
            You must make sure to replace the value for 
            <literal>root_password</literal> with a secure password.
            This will become the password for the &rootuser; account on all 
            nodes in the stack.
           </para>
       </warning>

        <informalfigure>
         <mediaobject>
          <imageobject role="fo">
           <imagedata fileref="horizon_launch_stack.png" width="100%"/>
          </imageobject>
          <imageobject role="html">
           <imagedata fileref="horizon_launch_stack.png" width="100%"/>
          </imageobject>
         </mediaobject>
        </informalfigure>
       </step>
       
      <step>
       <para>
        Click <guimenu>Next</guimenu>.
       </para>
      </step>

       <step>
        <para>
         Now you need to define more information about the stack.
        </para>
        <variablelist>
         <varlistentry>
          <term><literal>Stack Name</literal></term>
          <listitem>
           <para>
            Give your stack a name
           </para>
          </listitem>
         </varlistentry>
         <varlistentry>
          <term><literal>Password</literal></term>
          <listitem>
           <para>
            Your &soc; password
           </para>
          </listitem>
         </varlistentry>
         <varlistentry>
          <term><literal>Image</literal></term>
          <listitem>
           <para>
            Select the image your machines will be created from
           </para>
          </listitem>
         </varlistentry>
         <varlistentry>
          <term><literal>root_password</literal></term>
          <listitem>
           <para>
            Set the root password for your cluster machines
           </para>
          </listitem>
         </varlistentry>
         <varlistentry>
          <term><literal>admin</literal>/master/worker_flavor</term>
          <listitem>
           <para>
            Select the machine flavor for your nodes
           </para>
          </listitem>
         </varlistentry>
         <varlistentry>
          <term><literal>worker_count</literal></term>
          <listitem>
           <para>
            Number of worker nodes to be launched
           </para>
          </listitem>
         </varlistentry>
         <varlistentry>
          <term><literal>external_net</literal></term>
          <listitem>
           <para>
            Select an external network that your cluster will be reachable from
           </para>
          </listitem>
         </varlistentry>
         <varlistentry>
          <term><literal>internal_net_cidr</literal></term>
          <listitem>
           <para>
            The internal network range to be used inside the cluster
           </para>
          </listitem>
         </varlistentry>
         <varlistentry>
          <term><literal>dns_nameserver</literal></term>
          <listitem>
           <para>
            Internal name server for the cluster
           </para>
          </listitem>
         </varlistentry>
        </variablelist>
        <informalfigure>
         <mediaobject>
          <imageobject role="fo">
           <imagedata fileref="horizon_stack_options.png" width="100%"/>
          </imageobject>
          <imageobject role="html">
           <imagedata fileref="horizon_stack_options.png" width="100%"/>
          </imageobject>
         </mediaobject>
        </informalfigure>
       </step>
       
       <step>
        <para>
        Click <guimenu>Launch</guimenu>.
        </para>
       </step>
       
       <step>
        <para>
         After the cluster has been started and the cluster overview shows 
         <guimenu>Create Complete</guimenu>, you need to find the external IP 
         address for the admin node of your cluster (here: 
         <literal>10.86.1.72</literal>). Now visit that IP address in your browser. 
         You should see the &dashboard; login page and can continue with 
         <xref linkend="sec.deploy.install.iso.config"/>.
        </para>
        
        <informalfigure>
         <mediaobject>
          <imageobject role="fo">
           <imagedata fileref="horizon_stack_resources.png" width="100%"/>
          </imageobject>
          <imageobject role="html">
           <imagedata fileref="horizon_stack_resources.png" width="100%"/>
          </imageobject>
         </mediaobject>
        </informalfigure>
       </step>
       
      </procedure>
    </sect3>
      
      <sect3>
       <title>Using the &ostack; CLI</title>
       <note>
        <para>
         You need to have access to the &ostack; command-line tools. You can 
         either access those via <literal>ssh</literal> on your &soc; admin 
         server or 
         <link xlink:href="https://docs.openstack.org/newton/user-guide/common/cli-install-openstack-command-line-clients.html">
         install a local <command>openstack</command> client</link>.
        </para>
        <para>
         To use the local client, you need to access <guimenu>Project &rarr; Compute &rarr; 
         Access &amp; Security</guimenu> in the Horizon Dashboard and click on the 
         <guimenu>Download &ostack; RC File v3</guimenu>.
        </para>
        <para>
         The downloaded file is a script that you then need to load using the 
         <command>source</command> command. The script will ask you for your 
         &soc; password.
        </para>
         <screen>
&prompt.user;<command>source container-openrc.sh</command>
         </screen>
       </note>
       <procedure>

         <step>
          <para>
           Upload the container image to &ostack; Glance (Image service). This
           example uses the name <literal>CaaSP-3</literal> as the name of the 
           image that is created in &soc;.
          </para>
         <screen>
&prompt.user;<command>openstack image create --public --disk-format qcow2 \
--container-format bare \
--file SUSE-CaaS-Platform-3.0-OpenStack-Cloud.x86_64-1.0.0-GM.qcow2 \
<replaceable>CaaSP-3</replaceable></command>
         </screen>
         </step>
         <step>
          <warning>
           <title>Replace the default <literal>root_password</literal></title>
            <para>
             Do not use the <filename>caasp-environment.yaml</filename> directly
             from the GitHub repository.
            </para>
            <para>
             You must make sure to replace the value for 
             <literal>root_password</literal> with a secure password.
             This will become the password for the &rootuser; account on all 
             nodes in the stack.
            </para>
         </warning>
          <para>
           Download the <filename>caasp-stack.yaml</filename> and 
           <filename>caasp-environment.yaml</filename> Heat templates to your 
           workstation and then run the <command>openstack stack create</command>
           command.
          </para>
          <screen>
&prompt.user;<command>openstack stack create \ 
-t caasp-stack.yaml \
-e caasp-environment.yaml \
--parameter image=<replaceable>CaaSP-3</replaceable> <replaceable>caasp3-stack</replaceable></command>
          </screen>
         </step>

        <step>
         <para>
          Find out which (external) IP address was assigned to the admin node
          of your &productname; cluster (here: <literal>10.81.1.51</literal>).
         </para>
          <screen>
&prompt.user;<command>openstack server list --name "admin" | awk 'FNR > 3 {print $4 $5 $9}'</command>
caasp3-stack-admin|10.81.1.51
         </screen>
        </step>
        <step>
         <para>
          Visit the external IP address in your browser. You should see the &dashboard; 
          login page and can continue with <xref linkend="sec.deploy.install.iso.config"/>.
         </para>
       </step>
      </procedure>
       
     </sect3>
    </sect2>
    
    <sect2 xml:id="sec.deploy.install.qcow2.procedure">
     <title>Procedure for Hypervisors</title>
     <para>
      The process is very similar for all hypervisors. The examples in this
      manual use &kvm; running on &sle;.
     </para>
     <para>
      You need to build at least two ISO images with configuration information:
      one for the &admin_node; and one for the &worker_node;s. If you wish to
      assign specific host names or customize individual &worker_node;s, then
      you should create separate ISO images for each node.
     </para>
     <para>
      Additionally, you need to make a separate copy of the downloaded VM disk
      image for each VM. We suggest keeping the original download elsewhere and
      making a fresh copy for each node, naming it appropriately: for example,
      <literal>caas-admin</literal>, <literal>caas-master</literal>, 
      <literal>caas-worker1</literal>, <literal>caas-worker2</literal> and so
      on.
     </para>
     <sect3 xml:id="sec.deploy.install.qcow2.configuration">
      <title>Configuration Files</title>
      <para>
       There are two separate configuration files: <filename>user-data</filename>
       and <filename>meta-data</filename>. Each node needs both. Thus, you need to
       prepare at a minimum one pair of files for the &admin_node;, and another
       pair of files for all the &worker_node;s.
      </para>
      <para>
       Place the files into subdirectories named <filename>cc-admin</filename>
       for the &admin_node; and <filename>cc-worker</filename> for the
       &worker_node;s.
      </para>
      <para>
       So, for instance, if your working directory is <filename>~/cloud-config</filename>,
       then for the admin node, you need these two files:
      </para>
      <screen>~/cloud-config/cc-admin/user-data
~/cloud-config/cc-admin/meta-data</screen>
      <para>
       For a &worker_node;, you need:
      </para>
      <screen>~/cloud-config/cc-worker/user-data
~/cloud-config/cc-worker/meta-data</screen>
      <para>
       The same <filename>meta-data</filename> file can be used for both node types.
       Here is an sample <filename>meta-data</filename> file:
       </para>
      <screen>#cloud-config
instance-id: <replaceable>iid-CAAS01</replaceable>
network-interfaces: |
  auto <replaceable>eth0</replaceable>
  iface <replaceable>eth0</replaceable> inet dhcp</screen>
      <para>
       The <filename>user-data</filename> file contains settings such as time
       servers, the &rootuser; password, and the node type.
      </para>
      <para>
       Here is an example <filename>cc-admin/user-data</filename> file for an
       &admin_node;:
      </para>
      <screen>#cloud-config
debug: True
disable_root: False
ssh_deletekeys: False
ssh_pwauth: True
chpasswd:
  list: |
    root:<replaceable>MY_PASSWORD</replaceable>
    expire: False
ntp:
  servers:
    - <replaceable>ntp1.example.com</replaceable>
    - <replaceable>ntp2.example.com</replaceable>
runcmd:
  - /usr/bin/systemctl enable --now ntpd
suse_caasp:
  role: <replaceable>admin</replaceable></screen>
      <para>
       Here is an example <filename>cc-worker/user-data</filename>  for a
       &worker_node;. Rather than providing the &rootuser; password in
       clear text, you can use a hash instead; this example is hashed with
       SHA-256. 
      </para>
      <screen>#cloud-config
debug: True
disable_root: False
ssh_deletekeys: False
ssh_pwauth: True
chpasswd:
  list: |
  root:<replaceable>$5$eriogqzq$Dg7PxHsKGzziuEGkZgkLvacjuEFeljJ.rLf.hZqKQLA</replaceable>
    expire: False
suse_caasp:
   role: <replaceable>cluster</replaceable>
   admin_node: <replaceable>caas-admin.example.com</replaceable></screen>
     </sect3>
     <sect3 xml:id="sec.deploy.install.qcow2.iso-image">
      <title>Preparing an ISO image</title>
      <para>
       Once you have edited the configuration files with your desired settings,
       you need to create an ISO image with the volume label <literal>cidata</literal>
       containing only the subdirectory for that node type.
      </para>
      <para>
       On &sle; 12 or &opensuse; 42, use <command>genisoimage</command> to do this.
       On &sle; or &opensuse; 15, use <command>mkisofs</command>. The parameters
       are the same for both commands.
      </para>
      <para>
       For example, to create the ISO image for an admin node on a computer
       running &opensuse; 42:
      </para>
      <screen>&prompt.user;sudo genisoimage -output <replaceable>cc-admin.img</replaceable> -volid <replaceable>cidata</replaceable> -joliet -rock <replaceable>cc-admin</replaceable></screen>
      <para>
       To create the ISO image for a worker node on a computer running &opensuse;
       15, substituting the name of the folder containing the configuration
       files for a &worker_node; and titling the volume <literal>cc-worker</literal>:
      </para>
      <screen>&prompt.user;sudo mkisofs -output <replaceable>cc-worker.img</replaceable> -volid <replaceable>cidata</replaceable> -joliet -rock <replaceable>cc-worker</replaceable></screen>
     </sect3>
     
     <sect3 xml:id="sec.deploy.install.qcow2.admin">
      <title>Procedure to Bring Up an &Admin_Node;</title>
      <procedure>
       <step>
        <para>
         Make a folder called <filename>cc-admin</filename>.
        </para>
       </step>
       <step>
        <para>
         In that folder, prepare <filename>user-data</filename> and <filename>meta-data</filename>
         files to set the &rootuser; password, node name and so on for the
         &admin_node;.
        </para>
       </step>
       <step>
        <para>
         Create an ISO file called <filename>cc-admin.img</filename> containing
         these two files:
        </para>
        <screen>&prompt.user;sudo genisoimage -output cc-admin.img -volid cidata -joliet -rock cc-admin</screen>
       </step>
       <step>
        <para>
         Create a new VM for the &admin_node;.
        </para>
       </step>
       <step>
        <para>
         Attach a copy of the downloaded disk image as its main hard disk.   
        </para>
       </step>
       <step>
        <para>
         Attach the <filename>cc-admin.img</filename> disk image as a secondary disk.
       </para>
       </step>
       <step>
        <para>
         Start the VM.      
        </para>
       </step>
       <step>
        <para>
         Configure the new &admin_node; as in step <xref linkend="sec.deploy.install.iso.config"/>.
        </para>
       </step>
      </procedure>
     </sect3>
     
     <sect3 xml:id="sec.deploy.install.qcow2.worker">
      <title>Procedure to Bring Up a &Worker_Node;</title>
      <para>
       The first procedure is to generate the configuration files. Unless
       you wish to customize the individual &worker_node;s, then you need only
       do this part once.
      </para>
      <procedure>
       <step>
        <para>
         Make a folder called <filename>cc-worker</filename>.
        </para>
       </step>
       <step>
        <para>
         In that folder, prepare a <filename>user-data</filename> file to set the &rootuser;
         password, network settings and so on for the &worker_node;. If no
         changes are necessary, you can copy the <filename>meta-data</filename>
         file from that used to configure the &admin_node;.
        </para>
       </step>
       <step>
        <para>
         Create an ISO file called <filename>cc-worker.img</filename> containing
         these two files:
        </para>
        <screen>&prompt.user; sudo genisoimage -output cc-worker.img -volid cidata -joliet -rock cc-worker</screen>
       </step>
      </procedure>
      <para>
       Once you have a disk image containing the configuration files, it can be
       reused for multiple &worker_node;s.
      </para>
      <para>
       Then, repeat the following steps for each &worker_node;:
      </para>
      <procedure>
       <step>
        <para>
         Create a new VM for the &worker_node;.
        </para>
       </step>
       <step>
        <para>
         Attach a copy of the downloaded disk image as its main hard disk.
        </para>
       </step>
       <step>
        <para>
         Attach the <filename>cc-worker.img</filename> disk image as a
         secondary disk.
        </para>
       </step>
       <step>
        <para>
         Start the VM.      
        </para>
       </step>
      </procedure>
      <para>
       Once you have brought up as many &worker_node;s as you need, proceed to
       bootstap the cluster using the &dashboard; dashboard.
      </para>
     </sect3>

    </sect2>

   </sect1>

  <sect1 xml:id="sec.deploy.install.bootstrap">
   <title>Bootstrapping the Cluster</title>
   <para>
    To complete the installation of your &productname; cluster, it is necessary
    to bootstrap at least three additional nodes; those will be the &kube;
    master and workers.
<!-- cwickert 2017-07-23: FIXME We are already past AutoYaST -->
<!-- This process leverages &ay; and is (almost) fully automated. -->
   </para>
   <procedure xml:id="pro.deploy.install.bootstrap">
    <step>
     <para>
      Point a web browser to
      <uri>https://<replaceable>caasp-admin.&exampledomain;</replaceable>
      </uri>
      to open the dashboard, where
      <literal>caasp-admin.&exampledomain;</literal> is the host name or IP
      address of the &admin_node;.
     </para>
    </step>
    <step>
     <para>
      The dashboard lists all cluster nodes registered at the &admin_node;.
      Newly installed nodes are be listed as <guimenu>Pending Nodes</guimenu>.
      You can accept individual nodes or all by clicking <guimenu>Accept All
      Nodes</guimenu>.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="velum_nodes.png" width="100%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="velum_nodes.png" width="100%"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
     <para>
      Use the check box in the first column to select the nodes you want to add
      to your cluster. In the last column, select the one that should become
      &kube; master. All other nodes will be set to the worker role once you
      click <guimenu>Bootstrap cluster</guimenu>.
<!-- cwickert 2017-07-23: FIXME explain relation of internal and external names -->
<!-- Make sure the <guimenu>Hostname</guimenu> of the master matches the
      name configured as <guimenu>External Kubernetes API server FQDN</guimenu>
      in <xref linkend="sec.deploy.install.iso.config"/>. If necessary, go
      <guimenu>Back</guimenu> to chance the setting accordingly. -->
     </para>
    </step>
    <step>
     <para>
      Please wait while the cluster is bootstrapped. Once finished, the status
      indicator icon of the nodes changes accordingly and you can download the
      <command>kubectl</command> config file <filename>kubeconfig</filename>.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="velum_status.png" width="100%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="velum_status.png" width="100%"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
   </procedure>
  </sect1>
</chapter>
