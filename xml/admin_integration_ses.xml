<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha.admin.integration"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>&ses; Integration</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <para>
  &productname; can use another &suse; product as storage for
  containers&mdash;&ses; (henceforth called SES). This chapter gives details on
  several ways to integrate these two products.
 </para>
 <sect1 xml:id="sec.admin.integration.prerequisites">
  <title>Prerequisites</title>

  <para>
   Before you start the integration process, you need to ensure the following:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     The &productname; cluster can communicate with the SES nodes&mdash;master,
     monitoring nodes, OSD nodes and the metadata server, in case you need a
     shared file system. For more details regarding SES refer to the SES 
     documentation, here: <link xlink:href=
     "https://www.suse.com/documentation/suse-enterprise-storage/" />.
    </para>
   </listitem>
   <listitem>
    <para>
     The SES cluster has a pool with RADOS Block Device (RBD) enabled.
    </para>
   </listitem>
  </itemizedlist>
 </sect1>
 <sect1 xml:id="sec.admin.integration.mounting-named-object-storage">
  <title>Using RBD in a Pod</title>

  <para>
   The procedure below describes steps to take when you need to use a
   RADOS Block Device in a Pod.
  </para>

  <procedure>
   <title>Using RBD in a Pod</title>
   <step>
    <para>
     Retrieve the Ceph admin secret. Get the key value from the file
     <filename>/etc/ceph/ceph.client.admin.keyring</filename>.
    </para>
   </step>
   <step>
    <para>
     On the &master_node; apply the configuration that includes the
     Ceph secret by using the <command>kubectl apply</command>. Replace
     <replaceable>CEPH_SECRET</replaceable> your Ceph secret.
    </para>
<screen>&prompt.user;<command>kubectl apply -f - &lt;&lt; *EOF*
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
type: "kubernetes.io/rbd"
data:
  key: "$(echo <replaceable>CEPH_SECRET</replaceable> | base64)"
*EOF*</command></screen>
   </step>
   <step>
    <para>
     Create an image in the SES cluster. On the master node, run the following
     command:
    </para>
<screen>&prompt.root;<command>rbd create -s <replaceable>SIZE</replaceable> <replaceable>YOUR_VOLUME</replaceable></command></screen>
    <para>
     Where <replaceable>SIZE</replaceable> is the size of the image and
     <replaceable>YOUR_VOLUME</replaceable> is the name of the image.
    </para>
   </step>
   <step>
    <para>
     Create a pod that uses the image by executing the following
     command on the &master_node;. The example is a minimal
     configuration for using a RADOS Block Devide. Fill in the IP
     addresses and ports of your monitor nodes. The port number usually
     is <literal>6789</literal>.
    </para>
<screen>&prompt.user;<command>kubectl apply -f - &lt;&lt; *EOF*
apiVersion: v1
kind: Pod
metadata:
  name: <replaceable>POD_NAME</replaceable>
spec:
  containers:
  - name: <replaceable>CONTAINER_NAME</replaceable>
    image: <replaceable>IMAGE_NAME</replaceable>
    volumeMounts:
    - mountPath: /mnt/rbdvol
      name: rbdvol
  volumes:
  - name: rbdvol
    rbd:
      monitors:
      - [<replaceable>MONITOR1_IP</replaceable>:<replaceable>MONITOR1_PORT</replaceable>]
      - [<replaceable>MONITOR2_IP</replaceable>:<replaceable>MONITOR2_PORT</replaceable>]
      - [<replaceable>MONITOR3_IP</replaceable>:<replaceable>MONITOR3_PORT</replaceable>]
      pool: rbd
      image: <replaceable>YOUR_VOLUME</replaceable>
      user: admin
      secretRef:
        name: ceph-secret
      fsType: ext4
      readOnly: false
*EOF*</command></screen>
   </step>
   <step>
    <para>
     Verify that the pod exists and its status:
    </para>
<screen>&prompt.user;<command>kubectl get po</command></screen>
   </step>
   <step>
    <para>
     Once the pod is running, check the mounted volume:
    </para>
<screen>&prompt.user;<command>kubectl exec -it <replaceable>CONTAINER_NAME</replaceable> -- df -k</command>
...
/dev/rbd1               999320      1284    929224   0% /mnt/rbdvol
...</screen>
   </step>
  </procedure>

  <para>
   In case you need to delete the pod, run the following command on &master_node;:
  </para>

<screen>&prompt.user;<command>kubectl delete pod <replaceable>POD_NAME</replaceable></command></screen>
 </sect1>
 <sect1 xml:id="sec.admin.integration.persistent-volumes">
  <title>Using RBD in Persistent Volume</title>

  <para>
   The following procedure describes how to attach a pod to a persistent SES
   volume.
  </para>

  <procedure>
   <title>Creating a Pod with RBD in Persistent Volume</title>
   <step>
    <para>
     Retrieve the Ceph admin secret. Get the key value from the file
     <filename>/etc/ceph/ceph.client.admin.keyring</filename>.
    </para>
   </step>
   <step>
    <para>
     On the &master_node; apply the configuration that includes the
     Ceph secret by using the <command>kubectl apply</command>. Replace
     <replaceable>CEPH_SECRET</replaceable> your Ceph secret.
    </para>
<screen>&prompt.user;<command>kubectl apply -f - &lt;&lt; *EOF*
apiVersion: v1
kind: Secret
metadata:
  name: ceph-secret
type: "kubernetes.io/rbd"
data:
  key: "$(echo <replaceable>CEPH_SECRET</replaceable> | base64)"
*EOF*</command></screen>
   </step>
   <step>
    <para>
     Create a volume on the SES cluster:
    </para>
<screen>&prompt.root;<command>rbd create -s <replaceable>SIZE</replaceable> <replaceable>YOUR_VOLUME</replaceable></command></screen>
    <para>
     Where <replaceable>SIZE</replaceable> is the size of the image and
     <replaceable>YOUR_VOLUME</replaceable> is the name of the image.
    </para>
   </step>
   <step>
    <para>
     Create the persistent volume on the &master_node;:
    </para>
<screen>&prompt.user;<command>kubectl apply -f - &lt;&lt; *EOF*
apiVersion: v1
kind: PersistentVolume
metadata:
  name: <replaceable>PV_NAME</replaceable>
spec:
  capacity:
    storage: <replaceable>SIZE</replaceable>
  accessModes:
    - ReadWriteOnce
  rbd:
    monitors:
    - [<replaceable>MONITOR1_IP</replaceable>:<replaceable>MONITOR1_PORT</replaceable>]
    - [<replaceable>MONITOR2_IP</replaceable>:<replaceable>MONITOR2_PORT</replaceable>]
    - [<replaceable>MONITOR3_IP</replaceable>:<replaceable>MONITOR3_PORT</replaceable>]
    pool: rbd
    image: <replaceable>YOUR_VOLUME</replaceable>
    user: admin
    secretRef:
      name: ceph-secret
    fsType: ext4
    readOnly: false
*EOF*</command></screen>
   </step>
   <step>
    <para>
     Create a persistent volume claim on the &master_node;:
    </para>
<screen>&prompt.user;<command>kubectl apply -f - &lt;&lt; *EOF*
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: <replaceable>PV_NAME</replaceable>
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: <replaceable>SIZE</replaceable>
*EOF*</command></screen>
    <note>
     <title>Listing Volumes</title>
     <para>
      This persistent volume claim does not explicitly list the volume.
      Persistent volume claims work by picking any volume that meets the
      criteria from a pool. In this case we specified any volume with a size of
      1G or larger. When the claim is removed the recycling policy will be
      followed.
     </para>
    </note>
   </step>
   <step>
    <para>
     Create a pod that uses the persistent volume claim. On the &master_node; run
     the following:
    </para>
<screen>&prompt.user;<command>kubectl apply -f - &lt;&lt;*EOF*
apiVersion: v1
kind: Pod
metadata:
  name: <replaceable>POD_NAME</replaceable>
spec:
  containers:
  - name: <replaceable>CONTAINER_NAME</replaceable>
    image: <replaceable>IMAGE_NAME</replaceable>
    volumeMounts:
    - mountPath: /mnt/rbdvol
      name: rbdvol
  volumes:
  - name: rbdvol
    persistentVolumeClaim:
      claimName: <replaceable>PV_NAME</replaceable>
*EOF*</command></screen>
   </step>
   <step>
    <para>
     Verify that the pod exists and its status. On the &master_node; run:
    </para>
<screen>&prompt.user;<command>kubectl get po</command></screen>
   </step>
   <step>
    <para>
     Once pod is running, check the volume by running on the &master_node;:
    </para>
<screen>&prompt.user;<command>kubectl exec -it <replaceable>CONTAINER_NAME</replaceable> -- df -k</command>

...
/dev/rbd3               999320      1284    929224   0% /mnt/rbdvol
...</screen>
   </step>
  </procedure>

  <para>
   In case you need to delete the pod, run the following command on the
   &master_node;:
  </para>

<screen>&prompt.user;<command>kubectl delete pod <replaceable>CONTAINER_NAME</replaceable></command></screen>

  <para>
   And when the command finishes, run
  </para>

<screen>&prompt.user;<command>kubectl delete persistentvolume <replaceable>PV_NAME</replaceable></command></screen>

  <note>
   <title>Deleting a Pod</title>
   <para>
    When you delete the pod, the persistent volume claim is deleted as well.
   </para>
  </note>
 </sect1>
</chapter>
