<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha.deploy.upgrade"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Upgrading &productname;</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <para>
  As &productname; is constantly developed and improved, new versions get
  released. You are strongly advised to upgrade to a supported release. These
  upgrades may involve manual intervention.
 </para>
 <important>
  <title>Service Window Required</title>
  <para>
   Upgrades may take some time, during which services may be degraded in
   performance or completely unavailable. Please make sure to plan a service
   window.
  </para>
 </important>
 <procedure xml:id="pro.deploy.upgrade.procedure">
  <title>General Upgrade Procedure</title>
  <step>
   <para>
    Upgrade the &admin_node;.
   </para>
  </step>
  <step>
   <para>
    Manually perform additional upgrade steps of the &admin_node;. These steps
    are version-specific and described in the following chapters.
   </para>
  </step>
  <step>
   <para>
    Upgrade the cluster nodes through &dashboard;.
   </para>
  </step>
 </procedure>

 <sect1 xml:id="sec.deploy.upgrade.caasp1">
  <title>Upgrading from &productname; 1</title>
  <para>
   The following sections contain the necessary steps to upgrade from
   &productname; 1 to 2 or later. <!-- Note that for later versions of
   &productname;, additional steps may be necesssary. These are described in
   their own version-specific sections. -->
  </para>
  <sect2 xml:id="sec.deploy.upgrade.caasp1.users">
   <title>Migrating Users</title>
   <para>
    &productname; 2 comes with Role-Based Access Control (RBAC), which stores
    user information in <phrase role="productname">OpenLDAP</phrase>. Therefore,
    after upgrading from &productname; 1 to version 2 or higher, you have to
    migrate existing &dashboard; users to <phrase
     role="productname">OpenLDAP</phrase> users.
   </para>
   <para>For more information about RBAC and user management, refer to <xref linkend="auth"/>.</para>
   <procedure xml:id="pro.deploy.upgrade.caasp1.users">
    <title>Migrate users from version 1 to 2</title>
    <step>
     <para>
      Connect to the &admin_node; using SSH.
     </para>
    </step>
    <step>
     <para>
      Open a shell in the &dashboard; container:
     </para>
<screen>&prompt.root;<command>docker exec -it $(docker ps | grep dashboard | awk '{print $1}') bash</command></screen>
    </step>
    <step>
     <para>
      Inside the container, execute the following command:
     </para>
<screen>&prompt.bash;<command>entrypoint.sh bundle exec rake velum:migrate_users</command></screen>
     <para>Once the command successfully finishes, existing user accounts will
      be available for logging into &dashboard; again.
     </para>
    </step>
    <step>
     <para>
      Type <command>exit</command> or press <keycombo>
       <keycap function="control"/><keycap>D</keycap></keycombo> to exit the
      &dashboard; container.
     </para>
    </step>
   </procedure>
  </sect2>
  <sect2 xml:id="sec.deploy.upgrade.caasp1.etcd">
   <title>Upgrading etcd</title>
   <para>
    &productname; 2 comes with &kube; 1.7, which uses <literal>etcd</literal>
    version 3 as default storage backend. Therefore, after upgrading from
    &productname; 1 to version 2 or higher, you have to orchestrate the
    migration between <literal>etcd</literal> 2 and 3.
   </para>

   <important>
    <title>Service Window Required</title>
    <para>
     This migration can take a several minutes, during which
     <systemitem class="service">etcd</systemitem> and <systemitem
      class="service">kube-api</systemitem> services are unavailable. Please
     make sure to plan a service window.
    </para>
   </important>

   <procedure xml:id="pro.deploy.upgrade.caasp1.etcd">
    <title>Migrate from etcd version 2 to 3</title>
    <step>
     <para>
      Connect to the &admin_node; using SSH.
     </para>
    </step>
    <step>
     <para>
      Open a shell in the &smaster; container:
     </para>
<screen>&prompt.root;<command>docker exec -it $(docker ps | grep salt-master | awk '{print $1}') bash</command></screen>
    </step>
    <step>
     <para>
      Inside the container, execute the following command:
     </para>
<screen>&prompt.bash;<command>salt-run state.orchestrate orch.etcd-migrate</command></screen>
     <para>
      The orchestration will shutdown all <systemitem
       class="service">etcd</systemitem> and <systemitem
       class="service">kube-apiserver</systemitem> services, perform the
      <systemitem class="service">etcd</systemitem> migration steps, set the
      <quote>etcd_version = etcd3</quote> pillar value, and restart
      <systemitem class="service">etcd</systemitem> and <systemitem
       class="service">kube-api</systemitem> services.
     </para>
     <para>
      Once the command successfully finishes, all services will be available
      again.
     </para>
    </step>
    <step>
     <para>
      Type <command>exit</command> or press <keycombo>
       <keycap function="control"/><keycap>D</keycap></keycombo> to exit the
      &smaster; container.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.deploy.upgrade.caasp1.new-settings">
   <title>Adding new settings</title>
   <para>
    Run the following commands to ensure that default values are set correctly
    for some new options introduced in &productname; 2 which were not present
    in version 1.
   </para>
   <procedure xml:id="pro.deploy.upgrade.caasp1.new-settings">
    <title>Add new settings introduced in &productname; 2</title>
    <step>
     <para>
      Connect to the &admin_node; using SSH.
     </para>
    </step>
    <step>
     <para>
      Open a shell in the &dashboard; container:
     </para>
<screen>&prompt.root;<command>docker exec -it $(docker ps | grep dashboard | awk'{print $1}') bash</command></screen>
    </step>
    <step>
     <para>
      Set <literal>dashboard_external_fqdn</literal> to the Fully Qualified
      Domain Name (FQDN) of the &admin_node;:
     </para>
<screen>&prompt.bash;<command>entrypoint.sh bundle exec rails runner \
'Pillar.create(pillar: "dashboard_external_fqdn", value: "<replaceable>FQDN</replaceable>")'</command></screen>
     <para>
      Replace <literal>FQDN</literal> with the Fully Qualified Domain Name of
      your &admin_node;.
     </para>
    </step>
    <step>
     <para>
      Create the LDAP related pillars:
     </para>
<screen>&prompt.bash;<command>entrypoint.sh bundle exec rails runner \
'Velum::LDAP.ldap_pillar_settings!({}).each \
{|key, value| Pillar.create(pillar: Pillar.all_pillars[key.to_sym], \
value: value)}'</command></screen>
    </step>
    <step>
     <para>
      If you intend to use Helm on your CaaSP Cluster, you also need to enable
      Tiller (Helm's server component). Execute the following command in the
      open shell:
     </para>
<screen>&prompt.bash;<command>entrypoint.sh bundle exec rails runner \
'Pillar.create(pillar: "addons:tiller", value: "true")'</command></screen>
    </step>
    <step>
     <para>
      Type <command>exit</command> or press <keycombo>
       <keycap function="control"/><keycap>D</keycap></keycombo> to exit the
      &dashboard; container.
     </para>
    </step>
   </procedure>
  </sect2>
  <sect2 xml:id="sec.deploy.upgrade.caasp1.service-account">
   <title>Generating the Service Account Key File on the CA</title>
   <para>
    &kube; distinguishes between user and service accounts. While user accounts
    are for humans, service accounts are for processes, which run in pods.
   </para>
   <para>
    In order to use service acconunts, you have to generate the service account
    key file <filename>sa.key</filename> on the Certificate Authority (CA).
   </para>
   <procedure xml:id="pro.deploy.upgrade.caasp1.service-account">
    <title>Generate the Service Account Key File <filename>sa.key</filename> on the CA.</title>
    <step>
     <para>
      Connect to the &admin_node; using SSH.
     </para>
    </step>
    <step>
     <para>
      Open a shell in the &smaster; container with:
     </para>
<screen>&prompt.root;<command>docker exec -it $(docker ps | grep salt-master | awk '{print $1}') bash</command></screen>
    </step>
    <step>
     <para>
      Inside the container, execute the following command:
     </para>
<screen>&prompt.bash;<command>salt "ca" state.apply kubernetes-common.generate-serviceaccount-key</command></screen>
    </step>
    <step>
     <para>
      Type <command>exit</command> or press <keycombo>
       <keycap function="control"/><keycap>D</keycap></keycombo> to exit the
      &smaster; container.
     </para>
    </step>
   </procedure>
  </sect2>
 </sect1>
 
 <sect1 xml:id="sec.deploy.upgrade.caasp2">
  <title>Upgrading from &productname; 2</title>
  <note>
   <para>
    If you wish to upgrade to &productname; v3 from v1 you must first perform
    all the steps described in <xref linkend="sec.deploy.upgrade.caasp1"/>.
   </para>
  </note>
  
  <sect2 xml:id="pro.deploy.upgrade.caasp2.prereq">
   <title>Ensure All &productname; v2 Updates Are Installed</title>
   <para>
    Before you start the upgrade procedure to &productname; v3, you must ensure
    that all your nodes are running on the latest v2 updates. You can check the
    <filename>SUSEConnect</filename> package version to see if you are up to date.
    To do so you will run a <command>salt</command> command to display the
    packave version installed on each node.
   </para>
   
   <screen>
&prompt.user;<command>docker exec -i  $(docker ps | grep salt-master | awk '{print $1}') salt --batch 10 -P "roles:(admin|kube-(master|minion))" cmd.run "rpm -q SUSEConnect"</command>
Executing run on ['12cda3c374144d74804298bdee4d686c', '9b6d8d28393045c0914c959d0a5c0e33', '73b92dd7816147058c3d0fbb67fb18f9', 'admin']
admin:
    SUSEConnect-0.3.11-3.15.1.x86_64
jid:
    20180809103558881056
retcode:
    0
73b92dd7816147058c3d0fbb67fb18f9:
    SUSEConnect-0.3.11-3.15.1.x86_64
jid:
    20180809103558881056
retcode:
    0
9b6d8d28393045c0914c959d0a5c0e33:
    SUSEConnect-0.3.11-3.15.1.x86_64
jid:
    20180809103558881056
retcode:
    0
12cda3c374144d74804298bdee4d686c:
    SUSEConnect-0.3.11-3.15.1.x86_64
jid:
    20180809103558881056
retcode:
    0
   </screen>
   
   <para>
    If the package version is <literal>0.3.11-3.15.1</literal>
   </para>
   
   <para>
   </para>
  </sect2>
  
  
 </sect1>
 
</chapter>
