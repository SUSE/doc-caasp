<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha.deploy.upgrade"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Upgrading &productname;</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 <para>
  As &productname; is constantly developed and improved, new versions get
  released. You are strongly advised to upgrade to a supported release. These
  upgrades may involve manual intervention.
 </para>
 <important>
  <title>Service Window Required</title>
  <para>
   Upgrades may take some time, during which services may be degraded in
   performance or completely unavailable. Please make sure to plan a service
   window.
  </para>
 </important>
 <procedure xml:id="pro.deploy.upgrade.procedure">
  <title>General Upgrade Procedure</title>
  <step>
   <para>
    Upgrade the &admin_node;.
   </para>
  </step>
  <step>
   <para>
    Manually perform additional upgrade steps of the &admin_node;. These steps
    are version-specific and described in the following chapters.
   </para>
  </step>
  <step>
   <para>
    Upgrade the cluster nodes through &dashboard;.
   </para>
  </step>
 </procedure>

 <sect1 xml:id="sec.deploy.upgrade.caasp1">
  <title>Upgrading from &productname; 1</title>
  <para>
   The following sections contain the necessary steps to upgrade from
   &productname; 1 to 2 or later. <!-- Note that for later versions of
   &productname;, additional steps may be necesssary. These are described in
   their own version-specific sections. -->
  </para>
  <sect2 xml:id="sec.deploy.upgrade.caasp1.users">
   <title>Migrating Users</title>
   <para>
    &productname; 2 comes with Role-Based Access Control (RBAC), which stores
    user information in <phrase role="productname">OpenLDAP</phrase>. Therefore,
    after upgrading from &productname; 1 to version 2 or higher, you have to
    migrate existing &dashboard; users to <phrase
     role="productname">OpenLDAP</phrase> users.
   </para>
   <para>For more information about RBAC and user management, refer to <xref linkend="auth"/>.</para>
   <procedure xml:id="pro.deploy.upgrade.caasp1.users">
    <title>Migrate users from version 1 to 2</title>
    <step>
     <para>
      Connect to the &admin_node; using SSH.
     </para>
    </step>
    <step>
     <para>
      Open a shell in the &dashboard; container:
     </para>
<screen>&prompt.root;<command>docker exec -it $(docker ps | grep dashboard | awk '{print $1}') bash</command></screen>
    </step>
    <step>
     <para>
      Inside the container, execute the following command:
     </para>
<screen>&prompt.bash;<command>entrypoint.sh bundle exec rake velum:migrate_users</command></screen>
     <para>Once the command successfully finishes, existing user accounts will
      be available for logging into &dashboard; again.
     </para>
    </step>
    <step>
     <para>
      Type <command>exit</command> or press <keycombo>
       <keycap function="control"/><keycap>D</keycap></keycombo> to exit the
      &dashboard; container.
     </para>
    </step>
   </procedure>
  </sect2>
  <sect2 xml:id="sec.deploy.upgrade.caasp1.etcd">
   <title>Upgrading etcd</title>
   <para>
    &productname; 2 comes with &kube; 1.7, which uses <literal>etcd</literal>
    version 3 as default storage backend. Therefore, after upgrading from
    &productname; 1 to version 2 or higher, you have to orchestrate the
    migration between <literal>etcd</literal> 2 and 3.
   </para>

   <important>
    <title>Service Window Required</title>
    <para>
     This migration can take a several minutes, during which
     <systemitem class="service">etcd</systemitem> and <systemitem
      class="service">kube-api</systemitem> services are unavailable. Please
     make sure to plan a service window.
    </para>
   </important>

   <procedure xml:id="pro.deploy.upgrade.caasp1.etcd">
    <title>Migrate from etcd version 2 to 3</title>
    <step>
     <para>
      Connect to the &admin_node; using SSH.
     </para>
    </step>
    <step>
     <para>
      Open a shell in the &smaster; container:
     </para>
<screen>&prompt.root;<command>docker exec -it $(docker ps | grep salt-master | awk '{print $1}') bash</command></screen>
    </step>
    <step>
     <para>
      Inside the container, execute the following command:
     </para>
<screen>&prompt.bash;<command>salt-run state.orchestrate orch.etcd-migrate</command></screen>
     <para>
      The orchestration will shutdown all <systemitem
       class="service">etcd</systemitem> and <systemitem
       class="service">kube-apiserver</systemitem> services, perform the
      <systemitem class="service">etcd</systemitem> migration steps, set the
      <quote>etcd_version = etcd3</quote> pillar value, and restart
      <systemitem class="service">etcd</systemitem> and <systemitem
       class="service">kube-api</systemitem> services.
     </para>
     <para>
      Once the command successfully finishes, all services will be available
      again.
     </para>
    </step>
    <step>
     <para>
      Type <command>exit</command> or press <keycombo>
       <keycap function="control"/><keycap>D</keycap></keycombo> to exit the
      &smaster; container.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.deploy.upgrade.caasp1.new-settings">
   <title>Adding new settings</title>
   <para>
    Run the following commands to ensure that default values are set correctly
    for some new options introduced in &productname; 2 which were not present
    in version 1.
   </para>
   <procedure xml:id="pro.deploy.upgrade.caasp1.new-settings">
    <title>Add new settings introduced in &productname; 2</title>
    <step>
     <para>
      Connect to the &admin_node; using SSH.
     </para>
    </step>
    <step>
     <para>
      Open a shell in the &dashboard; container:
     </para>
<screen>&prompt.root;<command>docker exec -it $(docker ps | grep dashboard | awk'{print $1}') bash</command></screen>
    </step>
    <step>
     <para>
      Set <literal>dashboard_external_fqdn</literal> to the Fully Qualified
      Domain Name (FQDN) of the &admin_node;:
     </para>
<screen>&prompt.bash;<command>entrypoint.sh bundle exec rails runner \
'Pillar.create(pillar: "dashboard_external_fqdn", value: "<replaceable>FQDN</replaceable>")'</command></screen>
     <para>
      Replace <literal>FQDN</literal> with the Fully Qualified Domain Name of
      your &admin_node;.
     </para>
    </step>
    <step>
     <para>
      Create the LDAP related pillars:
     </para>
<screen>&prompt.bash;<command>entrypoint.sh bundle exec rails runner \
'Velum::LDAP.ldap_pillar_settings!({}).each \
{|key, value| Pillar.create(pillar: Pillar.all_pillars[key.to_sym], \
value: value)}'</command></screen>
    </step>
    <step>
     <para>
      If you intend to use Helm on your CaaSP Cluster, you also need to enable
      Tiller (Helm's server component). Execute the following command in the
      open shell:
     </para>
<screen>&prompt.bash;<command>entrypoint.sh bundle exec rails runner \
'Pillar.create(pillar: "addons:tiller", value: "true")'</command></screen>
    </step>
    <step>
     <para>
      Type <command>exit</command> or press <keycombo>
       <keycap function="control"/><keycap>D</keycap></keycombo> to exit the
      &dashboard; container.
     </para>
    </step>
   </procedure>
  </sect2>
  <sect2 xml:id="sec.deploy.upgrade.caasp1.service-account">
   <title>Generating the Service Account Key File on the CA</title>
   <para>
    &kube; distinguishes between user and service accounts. While user accounts
    are for humans, service accounts are for processes, which run in pods.
   </para>
   <para>
    In order to use service acconunts, you have to generate the service account
    key file <filename>sa.key</filename> on the Certificate Authority (CA).
   </para>
   <procedure xml:id="pro.deploy.upgrade.caasp1.service-account">
    <title>Generate the Service Account Key File <filename>sa.key</filename> on the CA.</title>
    <step>
     <para>
      Connect to the &admin_node; using SSH.
     </para>
    </step>
    <step>
     <para>
      Open a shell in the &smaster; container with:
     </para>
<screen>&prompt.root;<command>docker exec -it $(docker ps | grep salt-master | awk '{print $1}') bash</command></screen>
    </step>
    <step>
     <para>
      Inside the container, execute the following command:
     </para>
<screen>&prompt.bash;<command>salt "ca" state.apply kubernetes-common.generate-serviceaccount-key</command></screen>
    </step>
    <step>
     <para>
      Type <command>exit</command> or press <keycombo>
       <keycap function="control"/><keycap>D</keycap></keycombo> to exit the
      &smaster; container.
     </para>
    </step>
   </procedure>
  </sect2>
 </sect1>
 
 <sect1 xml:id="sec.deploy.upgrade.caasp2">
  <title>Upgrading from &productname; 2</title>
  <note>
   <para>
    If you wish to upgrade to &productname; v3 from v1 you must first perform
    all the steps described in <xref linkend="sec.deploy.upgrade.caasp1"/>.
   </para>
  </note>
  
  <sect2 xml:id="sec.deploy.upgrade.caasp2.prereq">
   <title>Ensure All &productname; v2 Updates Are Installed</title>
   <para>
    Before you start the upgrade procedure to &productname; v3, you must ensure
    that all your nodes are running on the latest v2 updates. You can check the
    <filename>SUSEConnect</filename> package version to see if you are up to date.
    To do so you will run a <command>salt</command> command to display the
    package version installed on each node.
   </para>
   
   <screen>
&prompt.user;<command>docker exec -i  $(docker ps | grep salt-master | awk '{print $1}') \
salt --batch 10 -P "roles:(admin|kube-(master|minion))" \
cmd.run "rpm -q SUSEConnect"</command>

Executing run on ['12cda3c374144d74804298bdee4d686c',
                  '9b6d8d28393045c0914c959d0a5c0e33',
                  '73b92dd7816147058c3d0fbb67fb18f9',
                  'admin']
admin:
    SUSEConnect-0.3.11-3.15.1.x86_64
jid:
    20180809103558881056
retcode:
    0
73b92dd7816147058c3d0fbb67fb18f9:
    SUSEConnect-0.3.11-3.15.1.x86_64
jid:
    20180809103558881056
retcode:
    0
9b6d8d28393045c0914c959d0a5c0e33:
    SUSEConnect-0.3.11-3.15.1.x86_64
jid:
    20180809103558881056
retcode:
    0
12cda3c374144d74804298bdee4d686c:
    SUSEConnect-0.3.11-3.15.1.x86_64
jid:
    20180809103558881056
retcode:
    0
   </screen>
   
   <para>
    If the package version is <literal>0.3.11-3.15.1</literal> (or higher) you
    have the latest updates from the v2 channel installed.
   </para>
   </sect2>
   
   <sect2 xml:id="sec.deploy.upgrade.caasp2.timer">
    <title>Disable Automatic <literal>transactional-update</literal></title>
    <para>
    To begin with the upgrade procedure, you first must disable the automatic
    transactional update mechanism to avoid conflicts. To do so you must run a
    <command>salt</command> across the nodes to disable the 
    <literal>transactional-update.timer</literal>.
    </para>
    <screen>
&prompt.user;<command>docker exec -i $(docker ps | grep salt-master | awk '{print $1}') \
salt --batch 10 -P "roles:(admin|kube-(master|minion))" \
cmd.run "systemctl disable --now transactional-update.timer"</command>

Executing run on ['5f6688bbeac94d2ab5c4330dc7043fb2',
                  'c3afd049edbe43afb4e2e5913a88291b',
                  '5bf346291a18406290886c2e2f7c3e3f',
                  'admin']

5bf346291a18406290886c2e2f7c3e3f:
    Removed symlink /etc/systemd/system/timers.target.wants/transactional-update.timer.
jid:
    20180807122220543037
retcode:
    0
admin:
    Removed symlink /etc/systemd/system/timers.target.wants/transactional-update.timer.
jid:
    20180807122220543037
retcode:
    0
c3afd049edbe43afb4e2e5913a88291b:
    Removed symlink /etc/systemd/system/timers.target.wants/transactional-update.timer.
jid:
    20180807122220543037
retcode:
    0
5f6688bbeac94d2ab5c4330dc7043fb2:
    Removed symlink /etc/systemd/system/timers.target.wants/transactional-update.timer.
jid:
    20180807122220543037
retcode:
    0
    </screen>
   </sect2>
   
   <sect2 xml:id="sec.deploy.upgrade.caasp2.upgrade">
    <title>Performing The Upgrade Procedure</title>
    <para>
     In the next step you must run the update command across your nodes.
    </para>
    <note>
     <title>Batch size for upgrade</title>
     <para>
      In this example we have limited the number of nodes this step will be 
      performed on to <literal>10 nodes</literal> at a time.
     </para>
      
      <para>
      This is a precaution to avoid problems on slower network connections.
      If you are performing this step on a high bandwidth connection (for 
      example from within the same datacenter as the cluster), you can raise the
      number of nodes by replacing the value for the (<literal>--batch</literal>)
      parameter.
     </para>
    </note>
    <screen>
&prompt.user;<command>docker exec -i $(docker ps | grep salt-master | awk '{print $1}') \
salt --batch <replaceable>10</replaceable> -P "roles:(admin|kube-(master|minion))" \
cmd.run "transactional-update salt migration -n" \
| tee transactional-update-migration.log</command>

Executing run on ['5f6688bbeac94d2ab5c4330dc7043fb2',
                  'c3afd049edbe43afb4e2e5913a88291b',
                  '5bf346291a18406290886c2e2f7c3e3f',
                  'admin']

5bf346291a18406290886c2e2f7c3e3f:
    
    
    Executing 'zypper --root /tmp/tmp.vbaqUwrLIh --non-interactive  refresh'
    
    Retrieving repository 'SUSE-CAASP-ALL-Pool' metadata [...done]
    Building repository 'SUSE-CAASP-ALL-Pool' cache [....done]
    Retrieving repository 'SUSE-CAASP-ALL-Updates' metadata [....done]
    Building repository 'SUSE-CAASP-ALL-Updates' cache [....done]
    All repositories have been refreshed.
    Upgrading product SUSE CaaS Platform 3.0 x86_64.

[ SNIP ... ]

    done
jid:
    20180807122253512832
retcode:
    0
    </screen>
    <para>
     During the procedure the nodes will be switched to the new release channel
     for v3, available updates are downloaded and installed, services and 
     applications are reconfigured and brought up in a orderly fashion.
    </para>
    <para>
     This operation will produce a lot of output for each node. The entire output
     is mirrored to a log file <filename>transactional-update-migration.log</filename>
     to the current working directory. This log file can be very helpful should
     any of the update operations fail.
    </para>
   </sect2>
   
   <sect2 xml:id="sec.deploy.upgrade.caasp2.reboot">
    <title>Reboot cluster nodes from &dashboard;</title>
    <para>
     To complete the procedure, you must reboot the cluster nodes. To do this 
     properly, use &dashboard; to restart the nodes.
    </para>
    <procedure>
     <step>
      <para>
       Log in to &dashboard;.
      </para>
     </step>
     <step>
      <para>
       Reboot the Admin node as described in 
       <xref linkend="transactional.updates.installation" />.
      </para>
     </step>
     <step>
      <para>
       Reboot the remaining nodes as described in 
       <xref linkend="transactional.updates.installation" />.
      </para>
     </step>
    </procedure>
   </sect2>
  
  <sect2 xml:id="sec.deploy.upgrade.caasp2.troubleshooting">
   <title>Troubleshooting</title>
   <para>
    In case the upgrade fails, please perform the support data collection by 
    running <command>supportconfig</command> on the affected nodes. Provide the 
    resulting files including the <filename>transactional-update-migration.log</filename>
    to SUSE Support.
   </para>
  </sect2>
  
 </sect1>
 
</chapter>
