<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="administration"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Cluster Administration</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>

 <sect1 xml:id="auth">
  <title>Authentication and Authorization</title>
  <para>
   Role-based access control (RBAC) adds the ability to perform authentication
   and authorization of activities performed against a &kube; cluster.
   Authentication is concerned with the <quote>who</quote> and authorization is
   concerned with the <quote>what</quote>.
  </para>

  <sect2 xml:id="auth.kubeconfig">
   <title>Authentication</title>
   <para>
    Starting in &productname; 2, &kubectl; needs to authenticate against the
    &kube; &master_node;. The necessary authentication information is included in
    the &kubeconfig; file available from &dashboard;. Click the
    <literal>kubectl config</literal> button and authenticate with your user
    name and password. Download the file from &dashboard; and save it as 
    <filename>$HOME/.kube/config</filename>.
   </para>
   <tip xml:id="auth.login.kubeconfig">
    <title>The KUBECONFIG variable</title>
    <para>
     &kubectl; uses an environment variable named <varname>KUBECONFIG</varname>
     to locate your &kubeconfig; file. If this variable is not specified, it
     defaults to <filename>$HOME/.kube/config</filename>. To use a different
     location, run
    </para>
    <screen>&prompt.user;<command>export</command> <option>KUBECONFIG=<replaceable>/path/to/kube/config/file</replaceable></option></screen>
   </tip>

   <note xml:id="auth.login.rootca">
    <title>Obtaining the root CA certificate</title>
    <para>
     You can obtain the root CA certificate from any node in your cluster via SCP:
    </para>
    <screen>&prompt.user;<command>scp</command> <option><replaceable>NODE</replaceable>:/etc/pki/trust/anchors/SUSE_CaaSP_CA.crt .</option></screen>
    <para>
     To trust this root CA certificate on your machine, place it in
     <filename>/etc/pki/trust/anchors/</filename> and call the
     <command>update-ca-certificates</command> script.
    </para>
   </note>

  </sect2>

  <sect2 xml:id="auth.users">
   <title>Managing Users and Groups</title>
   <para>User information is stored in
    <phrase role="productname">OpenLDAP</phrase> running in a container on your
    &productname; &admin_node;. You can use standard LDAP administration tools
    for managing these users remotely. To do so, install the
    <package>openldap2</package> package on a computer in your network and make
    sure that computer can connect to the &admin_node; on port 389. For further
    information, refer to <xref linkend="sec.caasp.installquick.netreqs"/>.
   </para>
   <sect3 xml:id="auth.user.ldap-rootpw">
    <title>Obtaining the OpenLDAP Password</title>
    <para>
     Before performing any administrative tasks on the <phrase
      role="productname">OpenLDAP</phrase> instance, you will need to retrieve
     your <phrase role="productname">OpenLDAP</phrase> administrator account
     password. To do this, run:
    </para>
    <screen>&prompt.user;<command>ssh</command> <option>root@<replaceable>caasp-admin.example.com</replaceable></option> \
    <command>cat</command> <option>/var/lib/misc/infra-secrets/openldap-password</option></screen>
    <para>Make sure to replace
     <replaceable>caasp-admin.example.com</replaceable> with the FQDN or IP of
     your &admin_node;.
    </para>
   </sect3>

   <sect3 xml:id="auth.users.add">
    <title>Adding New Users</title>
    <para>
     By default, when you create the first user in &dashboard; during bootstrap of
     your cluster, that user is granted <literal>Cluster Administrator</literal>
     privileges within &kube;. You can add additional users with these rights by
     adding new entries into the LDAP directory.
    </para>
    <para>
     To add a new user, create a LDIF file like this:
    </para>
<screen>
dn: uid=<replaceable>userid</replaceable><co xml:id="co.auth.users.add.uid"/>,ou=People,dc=infra,dc=caasp,dc=local
objectClass: person
objectClass: inetOrgPerson
objectClass: top
uid: <replaceable>userid</replaceable> <xref linkend="co.auth.users.add.uid" xrefstyle="select:label nopage"/>
userPassword: <replaceable>password hash</replaceable> <co xml:id="co.auth.users.add.password"/>
givenname: <replaceable>first name</replaceable> <co xml:id="co.auth.users.add.firstname"/>
sn: <replaceable>surname</replaceable> <co xml:id="co.auth.users.add.surname"/>
cn: <replaceable>full name</replaceable> <co xml:id="co.auth.users.add.fullname"/>
mail: <replaceable>email address</replaceable> <co xml:id="co.auth.users.add.email"/>
</screen>
    <para>
     Make sure to replace all the parameters indicated <replaceable>like
      this</replaceable> in the template above as follows:
    </para>
    <calloutlist>
     <callout arearefs="co.auth.users.add.uid">
      <para>
       User ID (UID) of the new user. Needs to be unique.
      </para>
     </callout>
     <callout arearefs="co.auth.users.add.password">
      <para>
       The user's hashed password. Use <command>/usr/sbin/slappasswd</command>
       to generate the hash.
      </para>
     </callout>
     <callout arearefs="co.auth.users.add.firstname">
      <para>
       The user's first name
      </para>
     </callout>
     <callout arearefs="co.auth.users.add.surname">
      <para>
       The user's last name
      </para>
     </callout>
     <callout arearefs="co.auth.users.add.fullname">
      <para>
       The user's full name
      </para>
     </callout>
     <callout arearefs="co.auth.users.add.email">
      <para>The user's e-mail address. It is used as the login name to
       &dashboard; and &kube;.</para>
     </callout>
    </calloutlist>
    <para>
     Populate your OpenLDAP server with this LDIF file:
    </para>
    <screen>&prompt.root;<command>ldapadd</command> -H ldap://<replaceable>ADMIN NODE IP</replaceable>;:389 -ZZ -D cn=admin,dc=infra,dc=caasp,dc=local -w <replaceable>LDAP ADMIN PASSWORD</replaceable> -f <replaceable>LDIF FILE</replaceable></screen>
    <para>
     To add this new user to the existing Administrators group, create a new LDIF
     file like this:
    </para>
    <screen>
dn: cn=Administrators,ou=Groups,dc=infra,dc=caasp,dc=local
changetype: modify
add: uniqueMember
uniqueMember: uid=<replaceable>userid</replaceable><co xml:id="co.auth.users.add.admin"/>,ou=People,dc=infra,dc=caasp,dc=local
    </screen>
    <para>
     Make sure to replace all the parameters indicated <replaceable>like
      this</replaceable> in the template above as follows:
    </para>
    <calloutlist>
     <callout arearefs="co.auth.users.add.admin">
      <para>The user ID (UID) of the user</para>
     </callout>
    </calloutlist>
    <para>
     Populate your <phrase role="productname">OpenLDAP</phrase> server with the
     LDIF file:
    </para>
    <screen>&prompt.root;<command>ldapadd</command> -H ldap://<replaceable>ADMIN NODE IP</replaceable>:389 -ZZ -D cn=admin,dc=infra,dc=caasp,dc=local -w <replaceable>LDAP ADMIN PASSWORD</replaceable> -f <replaceable>LDIF FILE</replaceable></screen>
   </sect3>

   <sect3 xml:id="auth.users.change-pw">
    <title>Changing a User's Password</title>
    <para>
     To change a user's password, create a LDIF file like this:
    </para>
    <screen>
dn: uid=<replaceable>userid</replaceable><co xml:id="co.auth.users.change-pw.uid"/>,ou=People,dc=infra,dc=caasp,dc=local
changetype: modify
modify: userPassword
userPassword: <replaceable>password hash</replaceable><co xml:id="co.auth.users.change-pw.password"/>
    </screen>
    <para>
     Make sure to replace all the parameters indicated <replaceable>like
      this</replaceable> in the template above as follows:
    </para>
    <calloutlist>
     <callout arearefs="co.auth.users.change-pw.uid">
      <para>
       User ID (UID) of the user.
      </para>
     </callout>
     <callout arearefs="co.auth.users.change-pw.password">
      <para>
       The user's new hashed password. Use
       <command>/usr/sbin/slappasswd</command> to generate the hash.
      </para>
     </callout>
    </calloutlist>
    <para>
     Populate your OpenLDAP server with this LDIF file:
    </para>
    <screen>&prompt.root;<command>ldapadd</command> -H ldap://<replaceable>ADMIN NODE IP</replaceable>;:389 -ZZ -D cn=admin,dc=infra,dc=caasp,dc=local -w <replaceable>LDAP ADMIN PASSWORD</replaceable> -f <replaceable>LDIF FILE</replaceable></screen>
   </sect3>

   <sect3 xml:id="auth.group">
    <title>Adding New Groups</title>
    <para>
     Say you have users that you want to grant access to manage a single
     namespace in Kubernetes. To do this, first create your users as mentioned
     in <xref linkend="auth.users.add"/>. Then create a new group:
    </para>
    <screen>
dn: cn=<replaceable>group name</replaceable><co xml:id="co.auth.group.cn"/>,ou=Groups,dc=infra,dc=caasp,dc=local
objectclass: top
objectclass: groupOfUniqueNames
cn: <replaceable>group name</replaceable><xref linkend="co.auth.group.cn" xrefstyle="select:label nopage"/>
uniqueMember: uid=<replaceable>member1</replaceable>,<co xml:id="co.auth.group.member"/>ou=People,dc=infra,dc=caasp,dc=local
uniqueMember: uid=<replaceable>member2</replaceable>,<xref linkend="co.auth.group.member" xrefstyle="select:label nopage"/>ou=People,dc=infra,dc=caasp,dc=local
uniqueMember: uid=<replaceable>member3</replaceable>,<xref linkend="co.auth.group.member" xrefstyle="select:label nopage"/>ou=People,dc=infra,dc=caasp,dc=local
    </screen>
    <para>
     Make sure to replace all the parameters indicated <replaceable>like
      this</replaceable> in the template above as follows:
    </para>
    <calloutlist>
     <callout arearefs="co.auth.group.cn">
      <para>
       The group's name.
      </para>
     </callout>
     <callout arearefs="co.auth.group.member">
      <para>
       Members of the group. Repeat the <literal>uniqueMember</literal>
       attribute for every member of this group.
      </para>
     </callout>
    </calloutlist>
    <para>
     Populate your <phrase role="productname">OpenLDAP</phrase> server with the
     LDIF file:
    </para>
    <screen>&prompt.root;<command>ldapadd</command> -H ldap://<replaceable>ADMIN NODE IP</replaceable>:389 -ZZ -D cn=admin,dc=infra,dc=caasp,dc=local -w <replaceable>LDAP ADMIN PASSWORD</replaceable> -f <replaceable>LDIF FILE</replaceable></screen>
    <para>
     Next, create a role binding to allow this new LDAP group access in &kube;.
     Create a Kubernetes deployment descriptor like this:
    </para>
    <screen>
# Define a Role and its permissions in &kube;
kind: Role
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: <replaceable>role name</replaceable><co xml:id="co.auth.group.role-name"/>
  namespace: <replaceable>applicable namespace</replaceable><co xml:id="co.auth.group.role-namespace"/>
# This set of rules amounts to "allow all"
rules:
- apiGroups: [""]
  resources: [""]
  resourceNames: [""]
  verbs: [""]
---
# Map an LDAP group to this &kube; role
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: <replaceable>role binding name</replaceable><co xml:id="co.auth.group.role-binding-name"/>
  namespace: <replaceable>applicable namespace</replaceable><xref linkend="co.auth.group.role-namespace" xrefstyle="select:label nopage"/>
subjects:
- kind: Group
  name: <replaceable>LDAP group name</replaceable><co xml:id="co.auth.group.ldap-group-name"/>
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role
  name: <replaceable>role name</replaceable><xref linkend="co.auth.group.role-name" xrefstyle="select:label nopage"/>
  apiGroup: rbac.authorization.k8s.io
    </screen>
    <calloutlist>
     <callout arearefs="co.auth.group.role-name">
      <para>
       Name of the new role in &kube;
      </para>
     </callout>
     <callout arearefs="co.auth.group.role-namespace">
      <para>
       Namespace the new group should be allowed to access. Use
        <literal>default</literal> for &kube;' default namespace.
      </para>
     </callout>
     <callout arearefs="co.auth.group.role-binding-name">
      <para>
       Name of the role binding in &kube;
      </para>
     </callout>
     <callout arearefs="co.auth.group.ldap-group-name">
      <para>
       Name of the corresponding group in LDAP
      </para>
     </callout>
    </calloutlist>
    <para>
     Add this role and binding to &kube;:
    </para>
    <screen>&prompt.root;<command>kubectl</command> <option>apply -f <replaceable>DEPLOYMENT DESCRIPTOR FILE</replaceable></option></screen>
   </sect3>
   <sect3>
    <title>Further information</title>
    <para>For more details on authorization in &kube;, refer to <link
     xlink:href="https://kubernetes.io/docs/admin/authorization/rbac/"></link>
    </para>
   </sect3>


 </sect2>

 </sect1>
 <sect1 xml:id="transactional.updates">
  <title>Handling Transactional Updates</title>

  <para>
   For security and stability reasons, the operating system and application
   should always be up-to-date. While with a single machine you can keep the
   system up-to-date quite easily by running several commands, in a
   large-scaled cluster the update process can become a real burden. Thus
   transactional automatic updates have been introduced. Transactional updates
   can be characterized as follows:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     They are atomic.
    </para>
   </listitem>
   <listitem>
    <para>
     They do not influence the running system.
    </para>
   </listitem>
   <listitem>
    <para>
     They can be rolled back.
    </para>
   </listitem>
   <listitem>
    <para>
     The system needs to be rebooted to activate the changes.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   Transactional updates are managed by the
   <command>transactional-update</command> script, which is called once a day.
   The script checks if any update is available. If there is an update to be
   applied, a new snapshot of the root file system is created and the system is
   updated by using <command>zypper dup</command>. All updates released to this
   point are applied. The snapshot is then marked as active and will be used
   after the next reboot of the system. Ensure that the cluster is rebooted as
   soon as possible after the update installation is complete, otherwise all
   changes will be lost.
  </para>

  <note>
   <title>General Notes to the Updates Installation</title>
   <para>
    Only packages that are part of the snapshot of the root file system can be
    updated. If packages contain files that are not part of the snapshot, the
    update could fail or break the system.
   </para>
   <para>
    RPMs that require a license to be accepted cannot be updated.
   </para>
  </note>

  <sect2 xml:id="transactional.updates.installation">
   <title>Manual Installation of Updates</title>
   <para>
    After the <command>transactional-update</command> script has run on all
    nodes, &dashboard; displays any nodes in your cluster running outdated
    software. &dashboard; then enables you to update your cluster directly.
    Follow the next procedure to update your cluster.
   </para>
   <procedure>
    <title>Updating the Cluster with &dashboard;</title>
    <step>
     <para>
      Login to &dashboard;.
     </para>
    </step>
    <step>
     <para>
      If required, click <guimenu>UPDATE ADMIN NODE</guimenu> to start the
      update.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject>
        <imagedata fileref="velum_updating.png" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
     <para>
      Confirm the update by clicking <guimenu>Reboot to update</guimenu>.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject>
        <imagedata fileref="velum_reboot_and_update.png" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
     <para>
      Now you have to wait until the &admin_node; reboots and &dashboard; is
      available again.
     </para>
    </step>
    <step>
     <para>
      Click <guimenu>update all nodes</guimenu> to update &master_node; and
      &worker_node;s.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject>
        <imagedata fileref="velum_update_nodes.png" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="transactional.updates.disabling">
   <title>Disabling Transactional Updates</title>
   <para>
    Even though it is not recommended, you can disable transactional updates
    by issuing the command:
   </para>
<screen><command>systemctl</command> <option>--now disable transactional-update.timer</option></screen>
   <para>
    Automatic reboot can also be disabled; use either of the
    commands:
   </para>
<screen><command>systemctl --now disable rebootmgr</command></screen>
   <para>
    or
   </para>
<screen><command>rebootmgrctl</command> <option>set-strategy off</option></screen>
  </sect2>
 </sect1>
 <sect1 xml:id="ptf.handling">
  <title>Handling Program Temporary Fixes</title>

  <para>
   Program temporary fixes (PTFs) are available in the &productname;
   environment. You install them by using the
   <command>transactional-update</command> script. Typically you invoke the
   installation of PTFs by running:
  </para>

<screen><command>transactional-update</command> <option>reboot ptf install <replaceable>rpm &hellip; rpm</replaceable></option></screen>

  <para>
   The command installs PTF RPMs. The <literal>reboot</literal> option then
   schedules a reboot after the installation. PTFs are activate only after
   rebooting of your system.
  </para>

  <note>
   <title>Reboot Required</title>
   <para>
    If you install or remove PTFs and you call the
    <command>transactional-update</command> to update the system before reboot,
    the applied changes by PTFs are lost and need to be done again after
    reboot.
   </para>
  </note>

  <para>
   In case you need to remove the installed PTFs, use the following command:
  </para>

<screen><command>transactional-update</command> <option>reboot ptf remove <replaceable>rpm &hellip; rpm</replaceable></option></screen>
 </sect1>

 <sect1 xml:id="commands.node.managment">
  <title>Commands for Single Node Management</title>

  <para>
   &productname; comes with several built-in commands that enable you to manage
   your &cluster_node;s.
  </para>

  <sect2 xml:id="commands.node.managment.issue-generator">
   <title>The <command>issue-generator</command> Command</title>
   <para>
    The <command>issue-generator</command> creates a volatile temporary
    <filename>/run/issue</filename> file. The file
    <filename>/etc/issue</filename> should be a symbolic link to the temporary
    <filename>/run/issue</filename>.
   </para>
   <para>
    You can use the command to prefix all directories and files with a
    specified prefix (path in this case):
   </para>
<screen>issue-generator --prefix <replaceable>path</replaceable></screen>
   <para>
    By using the command you can also create or delete files in the network
    configuration, for example:
   </para>
<screen>issue-generator network <replaceable>remove</replaceable> <replaceable>interface</replaceable></screen>
   <para>
    The command removes file
    <filename>/run/issue.d/70-<replaceable>interface</replaceable>.conf</filename>.
    The file contains the name of the <replaceable>interface</replaceable> and
    escape codes for <command>agentty</command>.
   </para>
   <para>
    You can use the command to add or delete
    <filename>/run/issue.d/60-ssh_host_keys.conf</filename> that contains
    fingerprints of the public SSH keys of the host:
   </para>
<screen>issue-generator ssh <replaceable>add|remove</replaceable></screen>
   <note>
    <title>The Command without Arguments</title>
    <para>
     If you run the command without any argument, all input files will be
     applied.
    </para>
   </note>
  </sect2>

  <sect2 xml:id="commands.node.managment.transactional-update">
   <title>The <command>transactional-update</command> Command</title>
   <para>
    The <command>transactional-update</command> enables you to install or
    remove updates of your system in an atomic way. The updates are applied all
    or none of them if any package cannot be installed. Before the update is
    applied, a snapshot of the system is created in order to restore the
    previous state in case of a failure.
   </para>
   <para>
    If the current root file system is identical to the active root file system
    (after applying updates and reboot), run cleanup of all old snapshots:
   </para>
<screen>transactional-update cleanup</screen>
   <para>
    Other options of the command are the following:
   </para>
   <variablelist>
    <varlistentry>
     <term><literal>up</literal>
     </term>
     <listitem>
      <para>
       If there are new updates available, a new snapshot is created and
       <command>zypper dup</command> is used to update the snapshot. The
       snapshot is activated afterwards and is used as the new root file system
       after reboot.
      </para>
<screen>transactional-update up</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>dup</literal>
     </term>
     <listitem>
      <para>
       If there are new updates available, a new snapshot is created and
       <command>zypper dup â€“no-allow-vendor-change</command> is used to
       update the snapshot. The snapshot is activated afterwards and is used as
       the new root file system after reboot.
      </para>
<screen>transactional-update dup</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>patch</literal>
     </term>
     <listitem>
      <para>
       If there are new updates available, a new snapshot is created and
       <command>zypper patch</command> is used to update the snapshot. The
       snapshot is activated afterwards and is used as the new root file system
       after reboot.
      </para>
<screen>transactional-update patch</screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>ptf install</literal>
     </term>
     <listitem>
      <para>
       The command installs the specified RPMs:
      </para>
<screen>transactional-update ptf install <replaceable>rpm ... rpm</replaceable></screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>ptf remove</literal>
     </term>
     <listitem>
      <para>
       The command removes the specified RPMs from the system:
      </para>
<screen>transactional-update ptf remove <replaceable>rpm ... rpm</replaceable></screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>rollback</literal>
     </term>
     <listitem>
      <para>
       The command sets the default sub volume. On systems with read-write
       file system <command>snapper rollback</command> is called. On a read-only
       file system and without any argument, the current system is set to a new
       default root file system. If you specify a number, that snapshot is used
       as the default root file system. On a read-only file system, no additional
       snapshots are created.
      </para>
<screen>transactional-update rollback <replaceable>snapshot_number</replaceable></screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>--help</literal>
     </term>
     <listitem>
      <para>
       The option outputs possible options and subcommands.
      </para>
<screen>transactional-update --help</screen>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>

  <sect2 xml:id="commands.node.managment.create_autoyast_profile">
   <title>The <command>create_autoyast_profile</command> Command</title>
   <para>
    The <command>create_autoyast_profile</command> command creates an autoyast
    profile for fully automatic installation of &productname;. You can use the
    following options when invoking the command:
   </para>
   <variablelist>
    <varlistentry>
     <term><literal>-o|--output</literal>
     </term>
     <listitem>
      <para>
       Specify to which file the command should save the created profile.
      </para>
<screen>create_autoyast_profile -o <replaceable>filename</replaceable></screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>--salt-master</literal>
     </term>
     <listitem>
      <para>
       Specify the host name of the &smaster;.
      </para>
<screen>create_autoyast_profile --salt-master <replaceable>saltmaster</replaceable></screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>--smt-url</literal>
     </term>
     <listitem>
      <para>
       Specify the URL of the SMT server.
      </para>
<screen>create_autoyast_profile --smt-url <replaceable>saltmaster</replaceable></screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>--regcode</literal>
     </term>
     <listitem>
      <para>
       Specify the registration code for &productname;.
      </para>
<screen>create_autoyast_profile --regcode <replaceable>405XAbs593</replaceable></screen>
     </listitem>
    </varlistentry>
    <varlistentry>
     <term><literal>--reg-email</literal>
     </term>
     <listitem>
      <para>
       Specify an e-mail address for registration.
      </para>
<screen>create_autoyast_profile --reg-email <replaceable>address@exampl.com</replaceable></screen>
     </listitem>
    </varlistentry>
   </variablelist>
  </sect2>
 </sect1>
 <sect1 xml:id="node.managment">
  <title>Node Management</title>

  <para>
   After you complete the deployment and you bootstrap the cluster, you may
   need to perform additional changes to the cluster. By using &dashboard; you
   can add additional nodes to the cluster. You can also delete some nodes, but
   in that case make sure that you do not break the cluster.
  </para>

  <sect2 xml:id="node.managment.adding">
   <title>Adding Nodes</title>
   <para>
    You may need to add additional &worker_node;s to your cluster. The following
    steps guides you through that procedure:
   </para>
   <procedure>
    <title>Adding Nodes to Existing Cluster</title>
    <step>
     <para>
      Prepare the node as described in
      <xref linkend="sec.caasp.installquick.node"/>
     </para>
    </step>
    <step>
     <para>
      Open &dashboard; in your browser and login.
     </para>
    </step>
    <step>
     <para>
      You should see the newly added node as a node to be accepted in
      <guimenu>Pending Nodes</guimenu>. Accept the node.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject>
        <imagedata fileref="velum_pending_nodes.png" format="PNG"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
     <para>
      In the <guimenu>Summary</guimenu> you can see the <guimenu>New</guimenu>
      that appears next to <guimenu>New nodes</guimenu>. Click the
      <guimenu>New</guimenu> button.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject>
        <imagedata fileref="velum_unassigned_nodes.png" format="PNG" width="60%"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
     <para>
      Select the node to be added and click <guimenu>Add nodes</guimenu>.
     </para>
    </step>
    <step>
     <para>
      The node has been added to your cluster.
     </para>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="node.managment.removing">
   <title>Removing Nodes</title>
   <para>
    As each node in the cluster runs also an instance of
    <literal>etcd</literal>, &productname; has to ensure that removing of
    several nodes does not break the <literal>etcd</literal> cluster. In case
    you have for example three nodes in the <literal>etcd</literal> and you
    delete two of them, &productname; deletes one node, recovers the cluster
    and only if the recovery is successful, the next node can be removed. If a
    node runs just an <literal>etcd-proxy</literal>, there is nothing special
    that has to be done, as deleting any amount of
    <literal>etcd-proxy</literal> can not break the <literal>etcd</literal>
    cluster.
   </para>
  </sect2>
 </sect1>
 <sect1 xml:id="cluster.monitoring">
  <title>Cluster Monitoring</title>

  <para>
   There are three basic ways how you can monitor your cluster:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     by directly accessing the <emphasis>cAdvisor</emphasis> on
     <literal>http://<replaceable>WORKER NODE ADDRESS</replaceable>;:4194/containers/</literal>.
     The <emphasis>cAdvisor</emphasis> runs on worker nodes by default.
    </para>
   </listitem>
   <listitem>
    <para>
     By using <emphasis>Heapster</emphasis>, for details refer to
     <xref linkend="cluster.monitoring.heapster"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     By using <emphasis>Grafana</emphasis>, for details refer to
     <xref linkend="cluster.monitoring.grafana"/>.
    </para>
   </listitem>
  </itemizedlist>

  <sect2 xml:id="cluster.monitoring.heapster">
   <title>Monitoring with Heapster</title>
   <para>
    <emphasis>Heapster</emphasis> is a tool that collects and interprets
    various signals from your cluster. <emphasis>Heapster</emphasis>
    communicates directly with the <emphasis>cAdvisor</emphasis>. The signals
    from the cluster are then exported using REST endpoints.
   </para>
   <para>
    To deploy <emphasis>Heapster</emphasis>, run the following command:
   </para>
<screen>kubectl apply -f https://raw.githubusercontent.com/SUSE/caasp-services/master/contrib/addons/heapster/heapster.yaml</screen>
   <para>
    <emphasis>Heapster</emphasis> can store data in
    <emphasis>InfluxDB</emphasis>, which can be then used by other tools.
   </para>
  </sect2>

  <sect2 xml:id="cluster.monitoring.grafana">
   <title>Monitoring with Grafana</title>
   <para>
    <emphasis>Grafana</emphasis> is an analytics platform that processes data
    stored in <emphasis>InfluxDB</emphasis> and displays the data graphically.
    You can deploy <emphasis>Grafana</emphasis> by running the following
    commands:
   </para>
<screen>kubectl apply -f https://raw.githubusercontent.com/SUSE/caasp-services/master/contrib/addons/heapster/heapster.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes/heapster/master/deploy/kube-config/influxdb/influxdb.yaml
kubectl apply -f https://raw.githubusercontent.com/kubernetes/heapster/release-1.3/deploy/kube-config/influxdb/grafana-deployment.yaml
wget https://raw.githubusercontent.com/kubernetes/heapster/release-1.3/deploy/kube-config/influxdb/grafana-service.yaml</screen>
   <para>
    Then open the file <filename>grafana-service.yaml</filename>:
   </para>
<screen>vi grafana-service.yaml</screen>
   <para>
    In the file uncomment the line with the <literal>NodePort</literal> type.
   </para>
   <para>
    To finish the <emphasis>Grafana</emphasis> installation, apply the
    configuration by running:
   </para>
<screen>kubectl apply -f grafana-service.yaml</screen>
  </sect2>
 </sect1>
</chapter>
