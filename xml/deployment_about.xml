<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter version="5.0" xml:id="deployment.about"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
  <title>About &productname;</title>
 </info>
 <para>
  &suse;&reg; CaaS (Container as a Service) Platform is a platform designed for
  fast deployment of container-based applications. You can deploy &productname;
  onto physical servers or use it on virtual machines. After deployment, it
  is ready to run out of the box and provides a highly-scalable cluster.
 </para>
 <para>
  &productname; was designed to simplify the transformation of applications into
  <emphasis>application containers</emphasis>. Creating clouds with containers
  is much easier than installing those applications directly onto the underlying
  operating system, as it eliminates problems with inter-application 
  compatibility and dependencies. To enable the most rapid application
  deployment possible, a container orchestration framework, e.g. &kube;, helps
  considerably.
 </para>
 <para>
  While &productname; inherits benefits of &sle; and uses tools and
  technologies well-known to system administrators&mdash;such as
  <literal>cloud-init</literal>, Kubernetes, and Salt&mdash;the main
  innovation (compared to &sls;) comes with <emphasis role="bold">transactional
  updates</emphasis>. A transactional update is an update that can be installed
  when the system is running without any down-time. A transaction update can
  be rolled back, so if the update fails or is not compatible with your
  infrastructure, you can restore the previous system state.

 </para>
 <para>
  &productname; uses the &btrfs; file system with the following characteristics:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    The root filesystem and its snapshots are read-only.
   </para>
  </listitem>
  <listitem>
   <para>
    Sub-volumes for data sharing are read-write.
   </para>
  </listitem>
  <listitem>
   <para>
    &productname; introduces overlays of the <literal>/etc</literal> directories
    used by <literal>cloud-init</literal> and &salt;.
   </para>
  </listitem>
 </itemizedlist>
 <sect1 xml:id="caasp.architecture">
  <title>&productname; Architecture</title>

  <para>
   A typical &productname; cluster consists of several types of nodes:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     The <emphasis>&admin_node;</emphasis> is a &smaster; which assigns roles to
     &sminion;s. This node runs the &dash; Web-based dashboard for managing the
     whole cluster. For details, refer to <xref linkend="administration_dashboard"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Each &cluster_node; is a &sminion; which can have one of the following
     roles:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       &kube; master: <emphasis>&master_node;s</emphasis> which manage the worker nodes.
      </para>
     </listitem>
     <listitem>
      <para>
       &kube; worker: <emphasis>&worker_node;s</emphasis> which run the application
       containers which are the main workload of the cluster. 
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
  </itemizedlist>

  <para>
   In large-scale clusters, there are other types of node that can help you to
   manage and run the cluster:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     a local <emphasis>&smt; server</emphasis> that manages subscriptions for
     workers and so decreases the traffic to the &scc;.
    </para>
   </listitem>
   <listitem>
    <para>
     a <emphasis>log server</emphasis> that stores the cluster nodes' logs.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   The following figure illustrates the interactions of the nodes.
  </para>

  <figure xml:id="caasp.architecture.cluster">
   <title>&productname; Nodes Architecture</title>
   <mediaobject>
    <imageobject>
     <imagedata fileref="caasp_generic_scheme.png" width="100%"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   To run the whole cluster, &productname; uses various technologies such as
   &salt;, Flannel networking, the <literal>etcd</literal> distributed key-
   store database, a controller, a scheduler, <literal>kubelet</literal>,
   a choice of two container runtime engines, and a &kube; API Server.
  </para>

  <para>
   &salt; is used to manage deployment and administration of the cluster. The
   <literal>&salt;-api</literal> is used to distribute commands from
   &dashboard; to the <literal>salt-master</literal> daemon. The
   <literal>salt-master</literal> daemon stores events in <emphasis>MariaDB</emphasis>
   (the database is also used to store &dashboard; data). The
   <literal>salt-minion</literal> daemon on the &admin_node; is used to
   generate required certificates, and on the &sminion;s the daemons communicate
   with the &admin_node;.
  </para>

  <para>
   As there can be several containers running on each host machine, each
   container is assigned an IP address that is used for communication with
   other containers on the same host. Containers might need to have a
   unique IP address exposed for network communications, thus Flannel networking
   is used. Flannel gives each host an IP subnet from which the container
   engine can allocate IP addresses to containers. The mapping of IP addresses
   is stored by <literal>etcd</literal>. The <literal>flanneld</literal> daemon
   manages routing of packets and mapping of IP addresses.
  </para>

  <para>
   Within the cluster there are several instances of <literal>etcd</literal>,
   each with a different purpose. The <literal>etcd discovery</literal> daemon
   running on the &admin_node; is used to bootstrap instances of
   <literal>etcd</literal> on other nodes and is not part of the
   <literal>etcd</literal> cluster on the other nodes. The <literal>etcd</literal>
   instance on the &master_node; stores events from the &kube; API Server. The
   <literal>etcd</literal> instance on &worker_node;s runs as a proxy that forwards
   clients to the <literal>etcd</literal> on the &master_node;.
  </para>

  <para>
   &kube; is used to manage containers orchestration. The following services
   and daemons are used by &kube;:
  </para>

  <variablelist>
   <varlistentry>
    <term>
     kubelet
    </term>
    <listitem>
     <para>
      An agent that runs on each node to monitor and control all the containers
      in a pod, ensuring that they are running and healthy.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     kube-apiserver 
    </term>
    <listitem>
     <para>
      This daemon exposes a REST API used to manage pods. The API server
      performs authentication and authorization.
     </para>
    </listitem>      
   </varlistentry>
   <varlistentry>
    <term>
     scheduler
    </term>
    <listitem>
     <para>
      The scheduler assigns pods onto nodes. It does not run them itself; that
      is <literal>kubelet</literal>'s job.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     controllers
    </term>
    <listitem>
     <para>
      These monitor the shared state of the cluster through the <literal>apiserver</literal>
      and handle pod replication, deployment, etc.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     kube-proxy
    </term>
    <listitem>
     <para>
      This runs on each node and is used to distribute loads and reach services.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   Now let's focus on a more detailed view of the cluster that involves also
   services running on each node type.
  </para>

  <figure xml:id="caasp.architecture.services">
   <title>Services on nodes</title>
   <mediaobject>
    <imageobject>
     <imagedata fileref="caasp_nodes_architecture.png" width="90%"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="administration_dashboard">
   <title>The Administration Node</title>
   <para>
    The &admin_node; manages the cluster and runs several applications required
    for proper functioning of the cluster. Because it is integral to the 
    operation of &productname;,the &admin_node; must have a fully-qualified 
    domain name (FQDN) which can be resolved from outside the cluster.  
   </para>
   <para>
    The &admin_node; runs &dashboard;, the administration dashboard; the MariaDB
    database; the <literal>etcd discovery</literal> server,
    <emphasis>salt-api</emphasis>, <literal>salt-master</literal> and <literal>salt-minion</literal>. The
    dashboard, database, and daemons all run in separate containers.
   </para>
   <para>
    &dashboard; is a Web application that enables you to deploy, manage, and
    monitor the cluster. The dashboard manages the cluster using 
    <emphasis>salt-api</emphasis> to interact with the underlying &salt;
    technology.
   </para>
   <para>
    The containers on the &admin_node; are managed by <literal>kubelet</literal>
    as a static pod. Bear in mind that this <literal>kubelet</literal> does not
    manage the cluster nodes. Each cluster node has its own running instance of
    <literal>kubelet</literal>.
   </para>
  </sect2>
  
  <sect2 xml:id="master_node">
   <title>Master Nodes</title>
   <para>
    &productname; &master_node;s monitor and control the &worker_node;s. They
    make global decisions about the cluster, such as starting and scheduling 
    pods of containers on the &worker_node;s. They run <emphasis>kube-apiserver</emphasis>
    but do not host application containers.
   </para>
   <para>
    Each cluster must have at least one &master_node;. For larger clusters, more
    &master_node;s can be added, but there must always be an odd number. 
   </para>
   <para>
    Like the &admin_node;, the &master_node; must have a resolvable FQDN. For
    &dashboard; to function correctly, it must always be able to resolve the 
    IP address of a &master_node;, so if there are multiple &master_node;s, they
    must all share the same FQDN, meaning that some form of DNS load-balancing,
    such as a round-robin DNS server, should be configured.
   </para>
  </sect2>
  <sect2 xml:id="worker_node">
   <title>Worker Nodes</title>
   <para>
    The &worker_node;s are the machines in the cluster which host application
    containers. Each runs its own instance of <emphasis>kubelet</emphasis> which
    controls the pods on that machine. Earlier versions of &kube; referred to
    &worker_node;s as "minions".
   </para>
   <para>
    Each &worker_node; runs a container runtime engine (either <literal>Docker</literal>
    or <literal>cri-o</literal>) and an instance of <literal>kube-proxy</literal>.
   </para>
   <para>
    The &worker_node;s do not require individual FQDNs, although it may help in
    troubleshooting network problems. 
   </para>
  </sect2>
  
 </sect1>
</chapter>
