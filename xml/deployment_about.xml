<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter version="5.0" xml:id="cha.deploy.about"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
  <title>About &productname;</title>
 </info>
 
  <para>
  &suse; &productname; is a Cloud-Native Computing Foundation (CNCF) certified
  &kube; distribution on top of &suse; &mos;.
  &suse; &mos; is a minimalist operating system based on &sle;, dedicated
  to hosting containers. &suse; &mos; inherits the benefits of &sle; in the
  form of a smaller, simpler, and more robust operating system, optimized for
  large, clustered deployments. It also features an atomic, transactional update
  mechanism, making the system more resilient against software-update-related
  problems.
 </para>
  
  <para>
  &productname; automates the orchestration and management of
  containerized applications and services with powerful &kube; capabilities,
  including:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    Workload scheduling optimizes hardware utilization while taking the
    container requirements into account.
   </para>
  </listitem>
  <listitem>
   <para>
    Service proxies provide single IP addresses for services and
    distribute the load between containers.
   </para>
  </listitem>
  <listitem>
   <para>
    Application scaling up and down accommodates changing loads.
   </para>
  </listitem>
  <listitem>
   <para>
    Non-disruptive rollout/rollback of new applications and updates enables
    frequent changes without downtime.
   </para>
  </listitem>
  <listitem>
   <para>
    Health monitoring and management supports application self-healing and
    ensures application availability.
   </para>
  </listitem>
 </itemizedlist>
 <para>
  In addition, &productname; simplifies the platform operatorâ€™s experience, with
  everything you need to get up and running quickly, and to manage the
  environment effectively in production. It provides:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    A complete container execution environment, including a purpose-built
    container host operating system (&suse; &mos;), container runtime, and
    container image registries.
   </para>
  </listitem>
  <listitem>
   <para>
    Enhanced datacenter integration features that enable you to plug &kube;
    into new or existing infrastructure, systems, and processes.    
   </para>
  </listitem>
  <listitem>
   <para>
    Application ecosystem support with &sle; container base images, and access
    to tools and services offered by &suse; Ready for CaaS Platform partners and
    the &kube; community.
   </para>
  </listitem>
  <listitem>
   <para>
    End-to-End security, implemented holistically across the full stack.
   </para>
  </listitem>
  <listitem>
   <para>
    Advanced platform management that simplifies platform installation,
    configuration, re-configuration, monitoring, maintenance, updates,
    and recovery.
   </para>
  </listitem>
  <listitem>
   <para>
    Enterprise hardening including comprehensive interoperability testing,
    support for thousands of platforms, and world-class platform maintenance and
    technical support.
   </para>
  </listitem>
 </itemizedlist>
 
 <para>
  You can deploy &productname; onto physical servers or use it on virtual
  machines. After deployment, it is immediately ready to run and provides a
  highly-scalable cluster.
 </para>
 <para>
  While &productname; inherits benefits of &sle; and uses tools and
  technologies well-known to system administrators such as
  <literal>cloud-init</literal> and Salt, the main innovation (compared to
  &sls;) comes with <emphasis role="bold">transactional updates</emphasis>. A
  transactional update is an update that can be installed when the system is
  running without any down-time. A transactional update can be rolled back, so
  if the update fails or is not compatible with your infrastructure, you can
  restore the previous system state.
 </para>
 <para>
  &productname; uses the &btrfs; file system with the following characteristics:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    The root file system and its snapshots are read-only.
   </para>
  </listitem>
  <listitem>
   <para>
    Sub-volumes for data sharing are read-write.
   </para>
  </listitem>
  <listitem>
   <para>
    &productname; introduces overlays of the <literal>/etc</literal> directories
    used by <literal>cloud-init</literal> and &salt;.
   </para>
  </listitem>
 </itemizedlist>
 
 <para>
  For more information, including a list of the various components which make up
  &productname; please refer to the Release Notes on <link
   xlink:href="https://www.suse.com/releasenotes/"/>.
 </para>
 
 <sect1 xml:id="sec.deploy.architecture">
  <title>Architectural Overview</title>

  <para>
   A typical &productname; cluster consists of several types of nodes:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     The <emphasis>&admin_node;</emphasis> is a &smaster; which assigns roles to
     &sminion;s. This node runs the &dash; Web-based dashboard for managing the
     whole cluster. For details, refer to
     <xref linkend="sec.deploy.architecture.administration-node"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     Each &cluster_node; is a &sminion; which can have one of the following
     roles:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       &kube; master: <emphasis>&master_node;s</emphasis> manage the worker nodes.
      </para>
     </listitem>
     <listitem>
      <para>
       &kube; worker: <emphasis>&worker_node;s</emphasis> run the application
       containers with the main workload of the cluster. 
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
  </itemizedlist>

  <para>
   In large-scale clusters, there are other types of node that can help you to
   manage and run the cluster:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     a local <emphasis>&smt; server</emphasis> that manages subscriptions for
     workers and so decreases the traffic to the &scc;.
    </para>
   </listitem>
   <listitem>
    <para>
     a <emphasis>log server</emphasis> that stores the cluster nodes' logs.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   The following figure illustrates the interactions of the nodes.
  </para>

  <figure xml:id="fig.deploy.architecture.cluster">
   <title>&productname; Nodes Architecture</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="caasp_generic_scheme.png" width="100%"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="caasp_generic_scheme.png" width="100%"/>
    </imageobject>
    <textobject role="description">
     <phrase>&productname; Nodes Architecture</phrase>
    </textobject>
   </mediaobject>
  </figure>
 </sect1>

 <sect1 xml:id="sec.deploy.components">
  <title>Software Components</title>
  <para>
   To run the whole cluster, &productname; uses various technologies such as
   &salt;, Flannel networking, the <literal>etcd</literal> distributed key-
   store database, a controller, a scheduler, <literal>kubelet</literal>, a
   &kube; API Server, and a choice of two container runtime engines, Docker or
   CRI-O.
  </para>
  <itemizedlist>
   <listitem>
    <formalpara>
     <title>Docker</title>
     <para>
      This is the leading open-source format for application containers. It is
      fully supported by &suse;.
     </para>
    </formalpara>
    <para>
     For more information, see <link xlink:href="https://www.docker.com/"/>.
    </para>
   </listitem>
   <listitem>
    <formalpara>
     <title>CRI-O</title>
     <para>
      Designed specifically for &kube;, CRI-O is an implementation of CRI, the
      Container Runtime Interface. A lightweight alternative to Docker or Moby,
      it supports Open Container Initiative (OCI) images.
     </para>
    </formalpara>
    <para>
     For more information, see <link xlink:href="http://cri-o.io/"/>.
    </para>
    
    <note>
     <title>Container Engine Support Status</title>
     <para>
      CRI-O is included as an <emphasis>unsupported technology
       preview</emphasis>, to allow customers to evaluate the new technology.
      It is <emphasis>not</emphasis> supported for use in production deployments.
     </para>
    </note>
   </listitem>
   </itemizedlist>

  <para>
   &salt; is used to manage deployment and administration of the cluster. 
   <literal>&salt;-api</literal> is used to distribute commands from
   &dashboard; to the <literal>salt-master</literal> daemon. The
   <literal>salt-master</literal> daemon stores events in <emphasis>MariaDB</emphasis>,
   which is also used to store &dashboard; data. The <literal>salt-minion</literal>
   daemon on the &admin_node; generates the required certificates, and
   &sminion;s on the &worker_node;s communicate with the &admin_node;.
  </para>

  <para>
   As there can be several containers running on each host machine, each
   container is assigned an IP address that is used for communication with
   other containers on the same host. Containers might need to have a
   unique IP address exposed for network communications, thus Flannel networking
   is used. Flannel gives each host an IP subnet from which the container
   engine can allocate IP addresses to containers. The mapping of IP addresses
   is stored by <literal>etcd</literal>. The <literal>flanneld</literal> daemon
   manages routing of packets and mapping of IP addresses.
  </para>

  <para>
   Within the cluster there are several instances of <literal>etcd</literal>,
   each with a different purpose. The <literal>etcd discovery</literal> daemon
   running on the &admin_node; is used to bootstrap instances of
   <literal>etcd</literal> on other nodes and is not part of the
   <literal>etcd</literal> cluster on the other nodes. The <literal>etcd</literal>
   instance on the &master_node; stores events from the &kube; API Server. The
   <literal>etcd</literal> instance on &worker_node;s runs as a proxy that forwards
   clients to the <literal>etcd</literal> on the &master_node;.
  </para>

  <para>
   &kube; is used to manage container orchestration. The following services
   and daemons are used by &kube;:
  </para>

  <variablelist>
   <varlistentry>
    <term>
     kubelet
    </term>
    <listitem>
     <para>
      An agent that runs on each node to monitor and control all the containers
      in a pod, ensuring that they are running and healthy.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     kube-apiserver 
    </term>
    <listitem>
     <para>
      This daemon exposes a REST API used to manage pods. The API server
      performs authentication and authorization.
     </para>
    </listitem>      
   </varlistentry>
   <varlistentry>
    <term>
     scheduler
    </term>
    <listitem>
     <para>
      The scheduler assigns pods onto nodes. It does not run them itself; that
      is <literal>kubelet</literal>'s job.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     controllers
    </term>
    <listitem>
     <para>
      These monitor the shared state of the cluster through the <literal>apiserver</literal>
      and handle pod replication, deployment, etc.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     kube-proxy
    </term>
    <listitem>
     <para>
      This runs on each node and is used to distribute loads and reach services.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   Now let's focus on a more detailed view of the cluster that involves also
   services running on each node type.
  </para>

  <figure xml:id="fig.deploy.architecture.services">
   <title>Services on nodes</title>
   <mediaobject>
    <imageobject role="fo">
     <imagedata fileref="caasp_nodes_architecture.svg" width="100%"/>
    </imageobject>
    <imageobject role="html">
     <imagedata fileref="caasp_nodes_architecture.svg" width="100%"/>
    </imageobject>
    <textobject role="description">
     <phrase>Services on nodes</phrase>
    </textobject>
   </mediaobject>
  </figure>
 </sect1>

 <sect1 xml:id="sec.deploy.architecture.administration-node">
  <title>The Administration Node</title>
  <para>
   The &admin_node; manages the cluster and runs several applications required
   for proper functioning of the cluster. Because it is integral to the 
   operation of &productname;, the &admin_node; must have a fully-qualified 
   domain name (FQDN) which can be resolved from outside the cluster.  
  </para>
  <para>
   The &admin_node; runs &dashboard;, the administration dashboard; the MariaDB
   database; the <literal>etcd discovery</literal> server,
   <emphasis>salt-api</emphasis>, <literal>salt-master</literal> and <literal>salt-minion</literal>. The
   dashboard, database, and daemons all run in separate containers.
  </para>
  <para>
   &dashboard; is a Web application that enables you to deploy, manage, and
   monitor the cluster. The dashboard manages the cluster using 
   <emphasis>salt-api</emphasis> to interact with the underlying &salt;
   technology.
  </para>
  <para>
   The containers on the &admin_node; are managed by <literal>kubelet</literal>
   as a static pod. Bear in mind that this <literal>kubelet</literal> does not
   manage the cluster nodes. Each cluster node has its own running instance of
   <literal>kubelet</literal>.
  </para>
 </sect1>
 
 <sect1 xml:id="sec.deploy.architecture.master-nodes">
  <title>Master Nodes</title>
  <para>
   &productname; &master_node;s monitor and control the &worker_node;s. They
   make global decisions about the cluster, such as starting and scheduling 
   pods of containers on the &worker_node;s. They run <emphasis>kube-apiserver</emphasis>
   but do not host application containers.
  </para>
  <para>
   Each cluster must have at least one &master_node;. For larger clusters, more
   &master_node;s can be added, but there must always be an odd number. 
  </para>
  <para>
   Like the &admin_node;, the &master_node; must have a resolvable FQDN. For
   &dashboard; to function correctly, it must always be able to resolve the 
   IP address of a &master_node;, so if there are multiple &master_node;s, they
   must all share the same FQDN, meaning that load-balancing should be
   configured.
  </para>
 </sect1>
 <sect1 xml:id="sec.deploy.architecture.worker_node">
  <title>Worker Nodes</title>
  <para>
   The &worker_node;s are the machines in the cluster which host application
   containers. Each runs its own instance of <emphasis>kubelet</emphasis> which
   controls the pods on that machine. Earlier versions of &kube; referred to
   &worker_node;s as "minions".
  </para>
  <para>
   Each &worker_node; runs a container runtime engine (either <literal>Docker</literal>
   or <literal>cri-o</literal>) and an instance of <literal>kube-proxy</literal>.
  </para>
  <para>
   The &worker_node;s do not require individual FQDNs, although it may help in
   troubleshooting network problems. 
  </para>
 </sect1>
</chapter>
