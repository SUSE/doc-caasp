<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<!-- Converted by suse-upgrade version 1.1 -->
<chapter version="5.0" xml:id="deployment.about"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker></dm:bugtracker>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
  <title>About &productname;</title>
 </info>
 <para>
  &suse;&reg; CaaS (Container as a Service) Platform is a platform designed for
  fast deployment of container-based applications. You can deploy &productname;
  on your physical machines or use it on virtual machines. After deployment, it
  is provides a ready-to-run, highly-scalable &kube; cluster.
 </para>
 <para>
  &productname; was designed to simplify transformation of conventional, locally-
  installed applications into container-based applications. Creating clouds with
  container-based applications is much easier, as it eliminates problems with
  compatibility and dependencies. To enable the fastest possible application
  deployment requires a container orchestration framework. &kube; is rapidly
  becoming the industry standard tool for managing containerized applications.
 </para>
 <para>
  While &productname; inherits benefits of &sle; and uses tools and
  technologies well-known to system administrators, such as <literal>cloud-init</literal>,
  &kube; and SaltStack, its main innovation compared to &sls; is that it comes
  with <emphasis role="bold">transactional updates</emphasis>. A transactional
  update is an update that can be installed when the system is running without
  any down-time. The term "transactional" means that an update can be rolled
  back, so if the upgrade fails or the update is not compatible with your
  infrastructure, you can easily restore the previous system state.
 </para>
 <para>
  &productname; uses the &btrfs; file system with the following characteristic:
 </para>
 <itemizedlist>
  <listitem>
   <para>
    The base OS and snapshots are read-only.
   </para>
  </listitem>
  <listitem>
   <para>
    Sub-volumes for data sharing are read-write.
   </para>
  </listitem>
  <listitem>
   <para>
    &productname; introduces overlays for <literal>/etc</literal> directories
    used by <literal>cloud-init</literal> and &salt;
   </para>
  </listitem>
 </itemizedlist>
 <sect1 xml:id="caasp.architecture">
  <title>&productname; Architecture</title>

  <para>
   A typical &productname; cluster consists of several types of nodes:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     &admin_node; - is a &smaster; that assigns roles to &sminion;s. The node
     runs the GUI dashboard that manages the whole cluster. For details refer
     to <xref linkend="administration_dashboard"/>.
    </para>
   </listitem>
   <listitem>
    <para>
     &cluster_node; - is a &sminion; that can have one of the following roles:
    </para>
    <itemizedlist>
     <listitem>
      <para>
       &kube; master - that manages nodes running containers.
      </para>
     </listitem>
     <listitem>
      <para>
       &kube; worker - that runs containers.
      </para>
     </listitem>
    </itemizedlist>
   </listitem>
  </itemizedlist>

  <para>
   In large-scale clusters, there are other types of node that can help you to
   manage and run the cluster:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     a local &smt; server that manages subscriptions for workers and so
     decreases the traffic to &scc;
    </para>
   </listitem>
   <listitem>
    <para>
     a <emphasis>log server</emphasis> that stores logs for the other nodes.
    </para>
   </listitem>
  </itemizedlist>

  <para>
   The following figure illustrates interactions of the nodes.
  </para>

  <figure xml:id="caasp.architecture.cluster">
   <title>&productname; Nodes Architecture</title>
   <mediaobject>
    <imageobject>
     <imagedata fileref="caasp_generic_scheme.png" width="100%"/>
    </imageobject>
   </mediaobject>
  </figure>

  <para>
   To run the whole cluster, &productname; uses various technologies like
   &salt;, flannel network, <literal>etcd</literal> cluster, controller,
   scheduler, <literal>kubelet</literal>, container engines, and an API Server.
  </para>

  <para>
   &salt; is used to manage deployment and administration of the cluster. The
   <literal>&salt;-api</literal> is used to distribute commands from
   &dashboard; to the <literal>salt-master</literal> daemon. The
   <literal>salt-master</literal> daemon stores events in <emphasis>MariaDB</emphasis> (the database is also used to store &dashboard; data). The
   <literal>salt-minion</literal> daemon on the &admin_node; is used to
   generate required certificates, and on the &sminion;s the daemons communicate
   with the &admin_node;.
  </para>

  <para>
   As there can be several containers running on each host machine, each
   container is assigned an IP address that is used for communication with
   other containers on the same host. Containers might need to have a
   unique IP address exposed for network communications, thus Flannel networking
   is used. Flannel gives each host an IP subnet from which the container
   engine can allocate IP addresses to containers. The mapping of IP addresses
   is stored by <literal>etcd</literal>. The <literal>flanneld</literal> daemon
   manages routing of packets and mapping of IP addresses.
  </para>

  <para>
   Within the cluster there are several instances of <literal>etcd</literal>,
   each with a different purpose. The <literal>etcd discovery</literal> daemon
   running on the &admin_node; is used to bootstrap instances of
   <literal>etcd</literal> on other nodes and is not part of the
   <literal>etcd</literal> cluster on the other nodes. The <literal>etcd</literal>
   instance on the &master_node; stores events from the &kube; API Server. The
   <literal>etcd</literal> instance on &worker_node;s runs as a proxy that forwards
   clients to the <literal>etcd</literal> on the &master_node;.
  </para>

  <para>
   &kube; is used to manage container orchestration. The following services
   and daemons are used by &kube;:
  </para>

  <variablelist>
   <varlistentry>
    <term>
     kubelet
    </term>
    <listitem>
    <para>
     An agent that runs on each node to monitor and control all the containers
     in a pod, ensuring that they are running and healthy.
    </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     kube-apiserver 
    </term>
    <listitem>
     <para>
     This daemon exposes a REST API used to manage pods. The API server performs
     authentication and authorization.
     </para>
     </listitem>      
   </varlistentry>
   <varlistentry>
    <term>
     scheduler
    </term>
    <listitem>
     <para>
      The scheduler assigns pods onto nodes. It does not run them itself; that
      is <literal>kubelet</literal>'s job.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     controllers
    </term>
    <listitem>
     <para>
      These monitor the shared state of the cluster through the <literal>apiserver</literal>
      and handle pod replication, deployment, etc.
     </para>
    </listitem>
   </varlistentry>
   <varlistentry>
    <term>
     kube-proxy
    </term>
    <listitem>
     <para>
      This runs on each node and is used to distribute loads and reach services.
     </para>
    </listitem>
   </varlistentry>
  </variablelist>

  <para>
   Now let's focus on a more detailed view of the cluster that involves also
   services running on each node type.
  </para>

  <figure xml:id="caasp.architecture.services">
   <title>Services on nodes</title>
   <mediaobject>
    <imageobject>
     <imagedata fileref="caasp_nodes_architecture.png" width="90%"/>
    </imageobject>
   </mediaobject>
  </figure>

  <sect2 xml:id="administration_dashboard">
   <title>The Administration Node</title>
   <para>
    The &admin_node; is intended to manage deployment of the cluster and run
    applications required for a proper function of the cluster. The
    &admin_node; runs the administration dashboard called &dashboard;, the
    <emphasis>salt-api</emphasis>, the MariaDB database, the <literal>etcd
    discovery</literal> server, the <literal>salt-master</literal> and
    <literal>salt-minion</literal>. The dashboard, database, and daemons all run
    in separate containers.
   </para>
   <para>
    The dashboard is a web application that enables you to manage, monitor, and
    deploy the cluster. The dashboard manages the cluster by using the
    <emphasis>salt-api</emphasis> to interact with the underlying &salt;
    technology.
   </para>
   <para>
    The containers on the &admin_node; are managed by
    <literal>kubelet</literal> as a static pod. Bear in mind that this
    <literal>kubelet</literal> does not manage the cluster nodes. Each cluster
    node has its own running instance of <literal>kubelet</literal>.
   </para>
  </sect2>
 </sect1>
</chapter>
