<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.0" xml:id="cha.admin"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Cluster Management</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker/>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>
 
 <!-- FIXME, mnapp 14/08/18 fill in these sections
 
 <sect1 xml:id="sec.admin.concepts">
  <title>Concepts</title>
 </sect1>
 
 <sect1 xml:id="sec.admin.startup">
  <title>Startup</title>
 </sect1>
 
 <sect1 xml:id="sec.admin.shutdown">
  <title>Shutdown</title>
 </sect1>
 
-->
 
 <sect1 xml:id="sec.admin.kubernetes.install-kubectl">
  <title>Interacting with &kube;</title>
  <para>
   &kube; requires the use of <literal>kubectl</literal> for many tasks.
   You can perform most of these actions while logged in to an SSH session on
   the master node of your &productname; cluster.
  </para>
  <para>
   One key exception is the use of <literal>kubectl proxy</literal>.
   The proxy functionality requires <literal>kubectl</literal> to be installed
   on your local machine to act as a proxy between the local workstation and the
    remote cluster.
  </para>
  <tip>
   <title>Installing kubectl on non-SUSE OS</title>
   <para>
    If you are using an operating system other than &sle; or &opensuse; please
    consult the
    <link xlink:href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">
    installation instructions</link> from the &kube; project.
   </para>
  </tip>
  
  <procedure>
   <title>Installing <literal>kubectl</literal> from the package repository</title>
   <step>
    <para>
     Go to <link xlink:href="https://build.opensuse.org/repositories/Virtualization:containers">the <filename>Virtualization:containers</filename> repository</link> and find the version of &sle;/&opensuse; you are using.
    </para>
   </step>
   <step>
    <para>
      Click the "Go to download repository" underneath your version.
     </para>
      <informalfigure>
       <mediaobject>
        <imageobject role="fo">
         <imagedata fileref="dl_repo_link.png" width="100%" format="png"/>
        </imageobject>
        <imageobject role="html">
         <imagedata fileref="dl_repo_link.png" width="100%" format="png"/>
        </imageobject>
       </mediaobject>
      </informalfigure>
   </step>
   <step>
    <para>
     Copy the URL to the repository from the URL bar of your browser. This is the location you will add as repository information in zypper.
    </para>
   </step>
   <step>
     <para>
      Open a terminal on your local machine add a repository to zypper:
     </para>
     <screen>&prompt.sudo;<command>zypper ar -f \
https://download.opensuse.org/repositories/Virtualization:/containers/SLE_12_SP3/</command></screen>
      <para>
       You will have to trust the key signature for the repository to continue.
      </para>
   </step>
   <step>
    <para>
     Refresh the repository information:
    </para>
    <screen>&prompt.sudo;<command>zypper ref</command></screen>
   </step>
   <step>
    <para>
     Install the <filename>kubernetes-client</filename> package:
    </para>
    <screen>&prompt.sudo;<command>zypper in kubernetes-client</command></screen>
   </step>
   <step>
    <para>
     To use kubectl to connect to a local machine you must perform <xref linkend="sec.admin.security.auth.kubeconfig" /> against the &kube; master node. Download the <filename>.kubeconfig</filename> file from &dashboard; and place it in <filename>Ëœ/.kube/config</filename>.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject role="fo">
       <imagedata fileref="velum_status.png" width="100%"/>
      </imageobject>
      <imageobject role="html">
       <imagedata fileref="velum_status.png" width="100%"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </step>
   <step>
    <para>
     Verify that <literal>kubectl</literal> was installed and is configured correctly:
    </para>
    <screen>&prompt.user;<command>kubectl get nodes</command>
NAME                  STATUS    ROLES     AGE       VERSION
caasp3-master     Ready     master    1d        v1.9.8
caasp3-worker-1   Ready     &lt;none&gt;    1d        v1.9.8
caasp3-worker-2   Ready     &lt;none&gt;    1d        v1.9.8
caasp3-worker-3   Ready     &lt;none&gt;    1d        v1.9.8
caasp3-worker-4   Ready     &lt;none&gt;    1d        v1.9.8</screen>
     <para>
     You should see the list of nodes known to &productname;.
    </para>
   </step>
 </procedure>
 </sect1>

 <sect1 xml:id="sec.admin.nodes">
  <title>Node Management</title>
  <para>
   After you complete the deployment and you bootstrap the cluster, you may
   need to perform additional changes to the cluster. By using &dashboard; you
   can add additional nodes to the cluster. You can also delete some nodes, but
   in that case make sure that you do not break the cluster.
  </para>

  <sect2 xml:id="sec.admin.nodes.add">
   <title>Adding Nodes</title>
   <para>
    You may need to add additional &worker_node;s to your cluster. The
    following steps guides you through that procedure:
   </para>
   <procedure>
    <title>Adding Nodes to Existing Cluster</title>
    <step>
     <para>
      Prepare the node as described in
      <xref linkend="sec.deploy.nodes.worker_install"/>
     </para>
    </step>
    <step>
     <para>
      Open &dashboard; in your browser and login.
     </para>
    </step>
    <step>
     <para>
      You should see the newly added node as a node to be accepted in
      <guimenu>Pending Nodes</guimenu>. Click on <guimenu>Accept Node</guimenu>.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="velum_pending_nodes.png" format="PNG" width="100%"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="velum_pending_nodes.png" width="100%" format="png"
        />
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
     <para>
      In the <guimenu>Summary</guimenu> you can see the <guimenu>New</guimenu>
      that appears next to <guimenu>New nodes</guimenu>. Click the
      <guimenu>New</guimenu> button.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject role="fo">
        <imagedata fileref="velum_unassigned_nodes.png" width="100%"
         format="png"/>
       </imageobject>
       <imageobject role="html">
        <imagedata fileref="velum_unassigned_nodes.png" width="100%"
         format="png"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
     <para>
      Select the node to be added and click <guimenu>Add nodes</guimenu>.
     </para>
    </step>
    <step>
     <para>
      The node has been added to your cluster.
     </para>
    </step>
   </procedure>
   
   <sect3 xml:id="sec.admin.nodes.create_autoyast_profile">
     <title>The <command>create_autoyast_profile</command> Command</title>
     <para>
      The <command>create_autoyast_profile</command> command creates an autoyast
      profile for fully automatic installation of &productname;. You can use the
      following options when invoking the command:
     </para>
     <variablelist>
      <varlistentry>
       <term><literal>-o|--output</literal>
       </term>
       <listitem>
        <para>
         Specify to which file the command should save the created profile.
        </para>
  <screen>&prompt.root;<command>create_autoyast_profile -o <replaceable>FILENAME</replaceable></command></screen>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><literal>--salt-master</literal>
       </term>
       <listitem>
        <para>
         Specify the host name of the &smaster;.
        </para>
  <screen>&prompt.root;<command>create_autoyast_profile --salt-master <replaceable>SALTMASTER</replaceable></command></screen>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><literal>--smt-url</literal>
       </term>
       <listitem>
        <para>
         Specify the URL of the SMT server.
        </para>
  <screen>&prompt.root;<command>create_autoyast_profile --smt-url <replaceable>SALTMASTER</replaceable></command></screen>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><literal>--regcode</literal>
       </term>
       <listitem>
        <para>
         Specify the registration code for &productname;.
        </para>
  <screen>&prompt.root;<command>create_autoyast_profile --regcode <replaceable>RIGISTRATION_CODE</replaceable></command></screen>
       </listitem>
      </varlistentry>
      <varlistentry>
       <term><literal>--reg-email</literal>
       </term>
       <listitem>
        <para>
         Specify an e-mail address for registration.
        </para>
  <screen>&prompt.root;<command>create_autoyast_profile --reg-email <replaceable>E-MAIL_ADRESS</replaceable></command></screen>
       </listitem>
      </varlistentry>
     </variablelist>
    </sect3>
  </sect2>

  <sect2 xml:id="sec.admin.nodes.remove">
   <title>Removing Nodes</title>
   <warning>
    <para>
     If you attempt to remove more nodes than are required for the minimum cluster
     size (3 nodes: 1 master, 2 workers) &dashboard; will display a warning.
     Your cluster will be disfunctional until you add the minimum amount of nodes
     again.
    </para>
   </warning>
   <note>
    <para>
     As each node in the cluster runs also an instance of
     <literal>etcd</literal>, &productname; has to ensure that removing of
     several nodes does not break the <literal>etcd</literal> cluster. In case
     you have, for example, three nodes in the <literal>etcd</literal> and you
     delete two of them, &productname; deletes one node, recovers the cluster
     and only if the recovery is successful, allows the next node to be removed.
     If a node runs just an <literal>etcd-proxy</literal>, there is nothing special
     that has to be done, as deleting any amount of
     <literal>etcd-proxy</literal> can not break the <literal>etcd</literal>
     cluster.
    </para>
   </note>
   <note>
    <para>
     If you have only one master node configured, &dashboard; will not allow you
     to remove it. You must first add a second master node as a replacement.
    </para>
   </note>
   
   <procedure>
    <step>
     <para>
      Log-in to &dashboard; on your &productname; Admin node.
      Then, click <guimenu>Remove</guimenu> next to the node you wish to remove.
      A dialog will ask you to confirm the removal.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject>
        <imagedata fileref="velum_status.png" format="PNG" width="100%"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
    <step>
     <para>
    The cluster will then attempt to remove the node in a controlled manner.
    Progress is indicated by a spinning icon and the words <literal>Pending removal</literal>
    in the location where the <guimenu>Remove</guimenu>-button was displayed before. 
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject>
        <imagedata fileref="velum_pending_removal.png" format="PNG" width="100%"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
     <para>
      This should conclude the regular removal process. If the node is successfully 
      removed, it will disappear from the list after a few moments.
     </para>
    </step>
    <step>
     <para>
      In some cases nodes can not be removed in a controlled manner and must be
      forced out of the cluster. A typical scenario is a machine instance was 
      removed externally or has no connectivity. In such cases, the removal will
      fail. You then get the option to <guimenu>Force remove</guimenu>. A dialog
      will ask you to confirm the removal.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject>
        <imagedata fileref="velum_failed_removal.png" format="PNG" width="100%"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
     <para>
      Additionally, a large warning dialog will ask you to confirm the forced
      removal. Click <guimenu>Proceed with forcible removal</guimenu> if you 
      are sure you wish to force the node out of the cluster.
     </para>
     <informalfigure>
      <mediaobject>
       <imageobject>
        <imagedata fileref="velum_force_removal.png" format="PNG" width="100%"/>
       </imageobject>
      </mediaobject>
     </informalfigure>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.admin.nodes.remove.unassigned">
  <!-- FIXME mnapp 2018-07-03, replace terminology and screenshots once 
  bsc#1100113 has been resolved -->
  
   <title>Removing unassigned nodes</title>
   <para>You might run into the situation where you have (accidentally) added
    new nodes to a cluster but did not wish to bootstrap them. They are now
    registered against the cluster and show up in "Unassigned nodes".
    You might have already configured the machine to register with another cluster
    and want to clean out this entry from the "Unassigned Nodes" view.
    
    You must perform the following steps:
   </para>
   <procedure>
   <step>
    <para>
     Find the "Unassigned nodes" line in the overview and click on <guimenu>(new)</guimenu> 
     next to the count number. You will be shown the "Unassigned Nodes" view 
     where all the unassigned nodes are listed. Make sure that you first assign 
     all roles to nodes that you wish to keep and proceed with bootstrapping. 
     Once the list only show the nodes you are sure to remove copy the ID of the 
     node you wish to drop.
    </para>
    <informalfigure>
     <mediaobject>
      <imageobject>
       <imagedata fileref="velum_unassigned_nodes.png" format="PNG" width="100%"/>
      </imageobject>
     </mediaobject>
    </informalfigure>
   </step>
   <step>
    <para>
   Log into the Admin node of you cluster via SSH.
    </para>
   </step>
   <step>
    <para>
     Run the following command and replace <replaceable>$ID_FROM_UNASSIGNED_QUEUE</replaceable>
     with the node ID that you copied from the "Unassigned nodes" view in &dashboard;.
    </para>
    <warning>
     <para>
      Make absolutely sure that the node ID you have copied is the one of the node
       you wish to drop. This command is <literal>irreversible</literal> and will remove the 
       specified node from the cluster without confirmation.
      </para>
     </warning>
    <screen>&prompt.root;<command>docker exec -it $(docker ps | grep "velum-dashboard" | awk '{print $1}') \
entrypoint.sh bundle exec rails runner 'puts Minion.find_by(minion_id: "<replaceable>$ID_FROM_UNASSIGNED_QUEUE</replaceable>").destroy'</command>
    </screen>
   </step>
  </procedure>
  </sect2>
 </sect1>
 
</chapter>
