<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.1" xml:id="cha.admin.backup"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Disaster Recovery</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker/>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>

 <sect1 xml:id="sec.admin.assumptions">
  <title>Assumptions</title>
  <important>
   <title>Scope</title>
   <para>
    The backup scenario assumes that the underlying &productname; cluster is
    healthy at the time of backup.
   </para>
   <para>
    This document does not describe recovery from a multi node cluster failure.
   </para>
   <para>
    The scenario described in this document does not cover backup for persistent
    data. It is supposed to allow restoring cluster functionality after node
    failure. Please apply best practices to ensure proper retention of
    persistent data.
   </para>
   <para>
    Depending on the type of failure scenario, workloads are likely to be
    recovered in Kubernetes but must be expected to be restarted in many
    scenarios.
   </para>
   <para>
    Always make sure your cluster has sufficient redundant capacity to
    temporarily shift workloads to other workers during upgrades or failures.
   </para>
   <para>
    The described approach is not suited for incremental backups.
   </para>
  </important>
 </sect1>

 <sect1 xml:id="sec.admin.backup">
 <title>Backup Scenarios</title>
 <note>
  <title>Recovering Failed Nodes</title>
  <para>
   In some failure scenarios (network disconnect, simple hardware failure) nodes
   can be recovered without additional action. Simply bringing the machines back
   online will suffice to return them to the cluster.
  </para>
  <para>
   In case of severe hardware failure (loss of hard drive, data corruption),
   generally speaking, a replacement is advised and nodes should be replaced
   with fresh (virtual) machines.
  </para>
 </note>

  <sect2 xml:id="sec.admin.backup.admin">
   <title>Admin Node Failure</title>
   <para>
    We assume:
   </para>

    <itemizedlist>
     <listitem>
      <para>
       The &admin_node; has failed and is unrecoverable
      </para>
     </listitem>
     <listitem>
      <para>
       The rest of the cluster is unaffected
      </para>
     </listitem>
     <listitem>
      <para>
       The IP address / hostname for the replacement node will be identical
      </para>
     </listitem>
     <listitem>
      <para>
       A backup of the &admin_node; has been taken from a healthy cluster state
      </para>
     </listitem>
     <listitem>
      <para>
       The backup procedure is performed regularly
      </para>
     </listitem>
    </itemizedlist>

   <para>
    The <xref linkend="sec.admin.backup.admin.restore"/> will involve deploying
    a new &admin_node; machine and restoring the previous data onto it.
    &productname; will then resume management of the cluster. Existing workloads
    should not be affected.
   </para>
   <procedure>
    <title>Backup procedure</title>
    <step>
     <para>
      SSH into the &admin_node;.
     </para>
    </step>
    <step>
     <para>
      Change to the <filename>/var/tmp</filename> directory and package
      important data into a tar file archive:
     </para>
<screen>&prompt.root;<command>
tar czf <replaceable>backup-admin-node-$(date +'%Y-%m-%d').tgz</replaceable> -C / \
    var/lib/misc/salt-master-credentials \
    usr/share/salt \
    etc/hosts \
    etc/pki \
    usr/share/caasp-container-manifests \
    etc/salt/pki \
    etc/caasp \
    var/lib/misc/salt/cache \
    etc/caasp/haproxy \
    var/lib/misc/infra-secrets \
    var/lib/misc/ldap \
    var/lib/misc/ldap-config \
    var/lib/misc/ssh-public-key \
    etc/salt/pillar \
    var/lib/misc/velum-secrets \
    etc/machine-id</command>
 </screen>
    </step>
    <step>
     <para>
      Then, export the MariaDB database and archive it:
     </para>
<screen>&prompt.root;<command>docker exec $(docker ps -qf name="velum-mariadb") mysqldump \
--password=$(cat /var/lib/misc/infra-secrets/mariadb-root-password) \
velum_production | gzip > <replaceable>/var/tmp/db-backup-$(date +'%Y-%m-%d').sql.gz</replaceable></command>
 </screen>
    </step>
    <step>
     <para>
      Finally, copy the created files <filename>backup-admin-node-[date].tgz</filename>
      and <filename>/var/tmp/db-backup-[date].sql.gz</filename> to
      a location of your choosing; in accordance with your best practices for
      backup files.
     </para>
     <important>
      <title>Sensitive Data</title>
      <para>
       Please note that the data backup files contain senstive information that
       allow full cluster access and must be handled securely.
      </para>
     </important>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.admin.backup.admin.restore">
   <title>Restore Procedure</title>
   <procedure>
    <step>
     <para>
      Set up a new machine for the &admin_node; by following
      <xref linkend="sec.deploy.nodes.admin_install" />.
     </para>
     <para>
      Make sure the &admin_node; is registered against &scc; and has the latest
      packages installed from the update channel.
     </para>
     <para>
      You must be able to access &dashboard; indicating all containers are
      correctly started. There is, however, no need to complete the
      configuration steps in &dashboard;.
     </para>
    </step>
    <step>
     <para>
      Copy the <filename>backup-admin-node-[date].tgz</filename> and
      <filename>db-backup-[date].sql.gz</filename> of the most recent backups
      onto the newly created &admin_node; machine. Make sure to replace
      <literal>[date]</literal> with the actual date subsituted during the
      backup procedure.
     </para>
    </step>
    <step>
     <para>
      SSH into the new &admin_node; machine and switch to the location where
      you have stored the backup files.
     </para>
    </step>
    <step>
     <para>
      Run the following command to unpack the data backup:
     </para>
<screen>&prompt.root;<command>tar xzf <replaceable>backup-admin-node-[date].tgz</replaceable> --exclude='mariadb*' --exclude='55-returner-credentials.conf' -C / 2> /var/tmp/log.txt</command></screen>
     <para>
      Run the following command to load the database backup into the MariaDB
      container. Make sure to replace <literal>[date]</literal> with the actual
      date subsituted during the backup procedure.
     </para>
<screen>&prompt.root;<command>zcat <replaceable>db-backup-[date].sql.gz</replaceable> | \
docker exec -i $(docker ps -qf name="velum-mariadb") \
mysql --password=$(cat /var/lib/misc/infra-secrets/mariadb-root-password) velum_production</command></screen>
     <note>
      <para>
       Note: These instructions assume that all cluster member node hostnames
       are resolvable by one another via DNS.
      </para>
     </note>
    </step>
    <step>
     <para>
      Once the files have been unpacked successfully, reboot the node.
     </para>
    <screen>&prompt.root;<command>reboot now</command></screen>
    </step>
    <step>
     <para>
      Once the new &admin_node; has restarted, ssh back onto it and run the following command.
<screen>&prompt.root;<command> docker exec -it $(docker ps -qf name="salt-master") salt-key -A</command></screen>
     </para>
    </step>
    <step>
     <para>
      The &dashboard; web interface should become available to you once the new
      &admin_node; has finished booting. You can then access it as usual by
      opening the hostname of your &admin_node; in a browser. You should be
      able to log in with the credentials you created during the original
      installation of the cluster.
     </para>
    </step>
   </procedure>
  </sect2>
 </sect1>

 <sect1 xml:id="sec.admin.backup.general">
  <title>General Node Recovery</title>
  <note>
   <title>Force Removal Of Nodes</title>
   <para>
    Failed nodes other than the &admin_node; will need to be "forcefully"
    removed as the standard removal process requires communicating with the
    node.
   </para>
   <para>
    To "force remove" a node, click the <guimenu>Remove</guimenu> button in
    &dashboard;. If the hardware has failed, this removal will also fail because there
    is no communication with the services on the failed node. Once regular
    removal has failed, you will be given the option to force remove the node by
    clicking the <guimenu>Force Remove</guimenu> button that appears in the same
    place.
   </para>
  </note>
  <important>
   <title>Force Removal Is Final</title>
   <para>
    Nodes that have been "forcefully" removed can no longer join the cluster
    and must be reinstalled.
   </para>
  </important>

  <sect2 xml:id="sec.admin.backup.general.master">
   <title>Master Node Failure</title>
   <para>
    We assume:
   </para>

   <itemizedlist>
    <listitem>
     <para>
      One or more Master node has failed and is unrecoverable
     </para>
    </listitem>
    <listitem>
     <para>
      The remaining Master nodes still retain quorum
     </para>
    </listitem>
    <listitem>
     <para>
      The Worker nodes are not affected
     </para>
    </listitem>
    <listitem>
     <para>
      The &admin_node; is not affected
     </para>
    </listitem>
   </itemizedlist>

   <important>
    <title><literal>etcd</literal> Fault Tolerance</title>
    <para>
     Recovering from loss of <literal>etcd</literal> Master nodes beyond
     the fault tolerance is not in scope for this document. If you have lost
     quorum and have surpassed fault tolerance, please contact SUSE Support.
    </para>
    <para>
     <literal>etcd</literal> quorum and fault tolerance limits are described in
     <xref linkend="sec.deploy.requirements.system.cluster.etcd_cluster_size"/>.
    </para>
   </important>

   <para>
    &productname; will consider failed Master nodes as "unreachable" but still
    counting towards quorum. You must remove the failed nodes without losing the
    quorum majority.
   </para>

   <para>
    As long as you are within the fault tolerance. You must "force remove" the
    failed nodes through &dashboard; and add new Master nodes until you reach
    your desired number of master nodes.
   </para>
   <para>
    The <xref linkend="sec.admin.backup.general.restore"/> is the same as for Worker nodes but
    you must consider quorum in any case.
   </para>
  </sect2>

  <sect2 xml:id="sec.admin.backup.general.worker">
   <title>Worker Node Failure</title>
   <para>
    We assume:
   </para>

   <itemizedlist>
    <listitem>
     <para>
      One or more Worker node has failed and is unrecoverable
     </para>
     <itemizedlist>
      <listitem>
       <para>
        Kubernetes will schedule the workloads that were running on it on another node
       </para>
       </listitem>
       <listitem>
        <para>
         There is enough free capacity in the cluster to take existing workloads
        </para>
        </listitem>
      </itemizedlist>
    </listitem>
    <listitem>
     <para>
      Workloads must shift to other available nodes during this process.
     </para>
    </listitem>
   </itemizedlist>
   <para>
    Worker nodes should be treated as disposable. &productname; will schedule
    workloads across the other available nodes.
   </para>
  </sect2>

  <sect2 xml:id="sec.admin.backup.general.restore">
   <title>Restore Procedure</title>
   <para>
    The restoration procedure does not concern itself with data stored on the
    actual Master or Worker node. You will be forcing the broken nodes out of
    the cluster and adding full replacements.
   </para>
   <procedure>
    <step>
     <para>
     Remove the failed node: <xref linkend="sec.admin.nodes.remove" />
     </para>
    </step>
    <step>
     <para>
     Verify the node is removed from the node list in Velum
     </para>
    </step>
    <step>
     <para>
      Add a new master/worker node: <xref linkend="sec.deploy.nodes.worker_install" />
     </para>
     <para>
      Since you already have an Admin node in place you can ignore the warning
      box.
     </para>
    </step>
    <step>
     <para>
      Verify the node is shown in the nodes list
    </para>
   </step>
  </procedure>
  <para>
   You can observe the status of the new nodes in the velum dashboard nodes
   list. Work will be scheduled automatically once the nodes are configured and
   added to the cluster (indicated by a green checkmark icon).
  </para>
  </sect2>
 </sect1>
</chapter>
