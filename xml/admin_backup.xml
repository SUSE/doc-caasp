<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.1" xml:id="cha.admin.backup"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Backup and Restore</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker/>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>

 <sect1 xml:id="sec.admin.backup.about">
  <title>About &productname; Backup</title>
  <para>
   Due to the nature of &kube; and container technology in general (and thus
   &productname;), backup and restore procedures work very differently than with
   "stateful" infrastructure.
  </para>
  <para>
   A &productname; backup consists of a few basic components that are then
   dynamically recombined to produce a cluster that is identical in functionality.
   This is not compatible with a 1:1 backup and restore approach.
  </para>
  <para>
   The backup consists of:
  </para>

  <itemizedlist>
   <listitem>
    <para>
     The &dashboard; database from the &productname; admin node.
    </para>
   </listitem>
   <listitem>
    <para>
     The cluster configuration state from <literal>etcd</literal>.
    </para>
   </listitem>
   <listitem>
    <para>
     External "stateful" data stores (not covered in these instructions).
    </para>
   </listitem>
  </itemizedlist>

   <para>
   It means that the internal IDs for machines, applications and services will
   change. It also means that the components for your deployments are likely
   to be deployed to different nodes than before. If any external services
   or implementations rely on internal IDs (for example: a specific container ID),
   these must be adjusted after restoration of a cluster.
  </para>
 </sect1>

 <sect1 xml:id="sec.admin.backup">
  <title>Backup</title>
  <para>
   The backup procedure consists of taking backups of the &dashboard; database
   and the <literal>etcd</literal> configuration data.
  </para>

  <sect2 xml:id="sec.admin.backup.admin">
   <title>Backup Admin Node And &dashboard; Database</title>
   <para>
    You must backup the configuration and main MariaDB database for the
    Admin node.
    </para>
   <procedure>
    <title>Backup database</title>
    <step>
     <para>
      Log in to the admin node of your cluster via <command>ssh</command>.
     </para>
    </step>

    <step>
     <para>
      Dump the contents of the database and package it as an archive <filename>
      velum-backup.sql.gz</filename>.
     </para>
<screen>&prompt.user;<command>docker exec $(docker ps -qf name="velum-mariadb") \
mysqldump --password=$(cat /var/lib/misc/infra-secrets/mariadb-root-password) velum_production \
| gzip &gt; /var/lib/misc/infra-secrets/velum-backup.sql.gz</command>
     </screen>
    </step>

    <!--  Missing LDAP  -->

    <step>
     <para>
      Package <literal>salt</literal>/<literal>etcd</literal> certificates and
      configurations as well as the container manifests.
     </para>
<screen>&prompt.user;<command>tar czf - -C / var/lib/misc \
etc/salt/pki \
etc/pki \
usr/share/caasp-container-manifests \
usr/share/salt/kubernetes \
&gt; backup-CaaSP-admin-node.tgz</command>
     </screen>
    </step>
   </procedure>
  </sect2>

  <sect2 xml:id="sec.admin.backup.etcd">
   <title>Backup <literal>etcd</literal></title>
   <para>
    You must back up the cluster configuration state stored in <literal>etcd</literal>.
    </para>
    <note>
     <title>Temporary backup directory</title>
    <para>
     In this example all files will be stored in a temporary directory.
     The resulting directory is called something like <filename>/tmp/etcd-1970-01-01</filename>.
    </para>
    <para>
     Since the name of the temporary directory is generated automatically using
     the <command>data</command> function, the name will change depending
     on which actual day you are performing these steps on.
    </para>
   </note>

    <procedure>
     <step>
      <para>
       Load the <command>etcdctl</command> configuration into your current
       work shell.
      </para>
<screen>&prompt.user;<command>set -a</command>

<command>source /etc/sysconfig/etcdctl</command>
     </screen>
    </step>

    <step>
     <para>
      (Optional) Display a list of members in the <literal>etcd</literal> cluster.
     </para>
<screen>
&prompt.user;<command>etcdctl member list</command>
8bef9c9ac28aa50e: name=d5e33fd8fa954a89aa3ddd12937c08e5
                  peerURLs=https://d5e33fd8fa954a89aa3ddd12937c08e5.infra.caasp.local:2380
                  clientURLs=https://d5e33fd8fa954a89aa3ddd12937c08e5.infra.caasp.local:2379
                  isLeader=false
e69de3a81c5588dd: name=627c1595c5e845c992d34613de46cbc6
                  peerURLs=https://627c1595c5e845c992d34613de46cbc6.infra.caasp.local:2380
                  clientURLs=https://627c1595c5e845c992d34613de46cbc6.infra.caasp.local:2379
                  isLeader=true
...
</screen>
   </step>

   <step>
    <para>
     Copy the contents of <filename>/etc/sysconfig/etcd</filename> and
     <filename>/etc/sysconfig/etcdctl</filename> to the temporary directory.
    </para>
<screen>&prompt.user;<command>cp -p /etc/sysconfig/etcd /tmp/etcd-`date +%F`</command>

<command>cp -p /etc/sysconfig/etcdctl /tmp/etcd-`date +%F`</command>
    </screen>
   </step>

   <step>
    <para>
     Copy the generated root certificate and the minion certificate/key
     to the temporary directory.
    </para>
<screen>&prompt.user;<command>cp -p /etc/pki/trust/anchors/SUSE_CaaSP_CA.crt /tmp/etcd-`date +%F`/SUSE_CaaSP_CA.crt</command>

<command>cp -p /etc/pki/minion.crt /tmp/etcd-`date +%F`/minion.crt</command>

<command>cp -p /etc/pki/minion.key /tmp/etcd-`date +%F`/minion.key</command>
    </screen>
   </step>

   <step>
    <para>
     Create the backup of the actual <literal>etcd</literal> configuration
     data in the temporary directory.
    </para>
<screen>&prompt.user;<command>systemctl stop etcd</command>

<command>etcdctl --debug backup --data-dir=/var/lib/etcd/ --backup-dir=/tmp/etcd-`date +%F`</command>

<command>cp -p /var/lib/etcd/member/snap/db /tmp/etcd-`date +%F`/member/snap/</command>

<command>chown -R etcd:etcd  /tmp/etcd-`date +%F`/*</command>
    </screen>
   </step>

   <step>
   <para>
   </para>
    <screen>&prompt.user;<command>cd /tmp/etcd-`date +%F`</command>

<command>tar -cvzf /tmp/etcd-`date +%F`.tar.gz *</command>
    </screen>
   </step>
   </procedure>

  </sect2>
 </sect1>

 <sect1 xml:id="sec.admin.restore.admin">
  <title>Restore</title>
  <para>
   Restoration of a cluster includes launching a new admin node and repopulating
   it with the backed up configurations. And then retoring the <literal>etcd</literal>
   cluster configuration and letting it rebuild.
  </para>

  <sect2 xml:id="sec.admin.restore.admin.rebuild">
   <title>Rebuild Admin Node</title>

  <para>
   Boot a fresh admin node from your installation image. Make sure the node has
   the same FQDN as before. Best practice is to use a CNAME DNS record for the
   node and adjust the CNAME record to the new node address.
   </para>

   <sect3 xml:id="sec.admin.restore.admin.rebuild.database">
    <title>Restore &dashboard; Database</title>
    <para>

    </para>
    <procedure>
     <step>
<screen>&prompt.user;<command>tar xzf - --exclude='mariadb-root-password' \
-C / &lt; backup-CaaSP-admin-node.tgz 2>/dev/null</command>
 </screen>
     </step>

     <step>
<screen>&prompt.user;<command>zcat /var/lib/misc/infra-secrets/velum-backup.sql.gz | \
docker exec -i $(docker ps -qf name="velum-mariadb") \
mysql --password=$(cat /var/lib/misc/infra-secrets/mariadb-root-password) velum_production</command>
 </screen>
     </step>

     <step>
 <screen>&prompt.user;<command>reboot</command>
     </screen>
     </step>
    </procedure>
   </sect3>

   <sect3 xml:id="sec.admin.restore.admin.rebuild.configuration">
    <title>Restore Admin Node Configuration</title>
    <para>
     Section to be added.
    </para>
   </sect3>
 </sect2>

  <sect2 xml:id="sec.admin.restore.etcd">
   <title>Restore <literal>etcd</literal></title>
   <important>
    <title>Preserve Node FQDN</title>
    <para>
     The backup must be restored to a node that has the same FQDN
     as the previous <literal>etcd</literal> node where the backup
     was taken.
    </para>
   </important>

   <sect3 xml:id="sec.admin.restore.etcd.master">
    <title>Restore <literal>etcd</literal> master</title>
    <procedure>
     <step>
      <para>
       Transfer the backup archive onto the new admin node (in this
       example to <filename>/tmp</filename>).
      </para>
     </step>
     <step>
      <para>
       Extract tarball file into the <filename>/var/lib/etcd</filename>
       directory
      </para>
<screen>&prompt.user;<command>rm /var/lib/etcd/* -fr</command>

<command>cd /var/lib/etcd</command>

<command>tar -xvzf /tmp/etcd-`date +%F`.tar.gz</command>
      </screen>
     </step>

     <step>
      <para>
       Move <command>etcd</command> and <command>etcdctl</command>
       config files.
      </para>
<screen>&prompt.user;<command>mv etcd /etc/sysconfig/etcd</command>

<command>mv etcdctl /etc/sysconfig/etcdctl</command>
      </screen>
     </step>

     <step>
      <para>
       Restore the minion certificate/key.
      </para>
<screen>&prompt.user;<command>mv SUSE_CaaSP_CA.crt /etc/pki/trust/anchors/SUSE_CaaSP_CA.crt</command>

<command>mv minion.crt /etc/pki/minion.crt</command>

<command>mv minion.key /etc/pki/minion.key</command>
      </screen>
     </step>

     <step>
      <para>
        Initialize a new <literal>etcd</literal> cluster
      </para>
<screen>&prompt.user;<command>set -a</command>

<command>source /etc/sysconfig/etcd</command>

<command>/usr/sbin/etcd --name="${ETCD_NAME}" \
--data-dir="${ETCD_DATA_DIR}" \
--listen-client-urls="${ETCD_LISTEN_CLIENT_URLS}" \
--force-new-cluster</command>
      </screen>
     </step>

     <step>
      <para>
       Wait for the server to start up and look for the message
       <quote>ready to serve client requests</quote> in the output.
      </para>
<screen>
2017-08-22 11:32:03.296340 I | embed: ready to serve client requests <co xml:id="co.ready"/>
2017-08-22 11:32:03.296807 I | embed: serving client requests on [::]:2379
2017-08-22 11:32:03.298188 E | etcdmain: forgot to set Type=notify in systemd service file?
      </screen>
      <calloutlist>
       <callout arearefs="co.ready">
         <para>Server is ready to serve requests.</para>
       </callout>
      </calloutlist>
      <para>
       Once the server has reached this point, you can quit it
       using
       <keycombo>
        <keycap function="control"/>
        <keycap>C</keycap>
       </keycombo>.
      </para>
     </step>

     <step>
      <para>
       Ensure correct permissions on <filename>/var/lib/etcd</filename>
      </para>
<screen>&prompt.root;<command>chown -R etcd:etcd /var/lib/etcd</command>
     </screen>
    </step>
    <step>
     <para>
      Start the <command>etcd</command> service.
     </para>
<screen>
<command>systemctl start etcd</command>
    </screen>
    </step>
   </procedure>
  </sect3>

  <sect3 xml:id="sec.admin.restore.etcd.cluster">
   <title>Restore Cluster Nodes</title>
   <para>
   </para>

   <procedure>
    <step>
     <para>
      Prepare and add more nodes to the cluster.

      These files can be copied from the first restored node, but you
      must replace <literal>ETCD_NAME</literal> in all places with
      <literal>ETCD_NAME</literal> from the individual nodes'
      contents of <filename>/etc/machine-id</filename>.
    </para>
<screen>&prompt.user;<command>/etc/sysconfig/etcd</command>

<command>/etcd/sysconfig/etcdctl</command>
     </screen>
    </step>

   <step>
   <para>
   Add new node to the cluster.
   Get ETCD_NAME of new node, it can be read from /etc/macine-id file and run following command on restored first etcd node
   </para>

<screen>&prompt.user;<command>etcdctl member add <replaceable>&lt;NEW-MACHINE-ID&gt;</replaceable> https://<replaceable>&lt;NEW-MACHINE-ID&gt;.&lt;LOCAL-DOMAIN&gt;</replaceable>:2380</command>
</screen>

<para>
 For example:
</para>
<screen>&prompt.user;<command>etcdctl member add d5e33fd8fa954a89aa3ddd12937c08e5 \
https://d5e33fd8fa954a89aa3ddd12937c08e5.infra.caasp.local:2380</command>

Added member named d5e33fd8fa954a89aa3ddd12937c08e5 with ID 4e5c6003ee1b53d5 to cluster
</screen>

   </step>

   </procedure>
  </sect3>
 </sect2>
 </sect1>



 <sect1 xml:id="sec.admin.validate">
  <title>Validate Restore Procedure</title>

  <sect2 xml:id="sec.admin.validate.etcd">
   <title>Validate <literal>etcd</literal> service</title>
  <para>
   After restoration of the cluster configuration, you must
   verify that the configuration has taken and <literal>etcd</literal>
   will work as desired.
  </para>

  <procedure>
   <title>Validation</title>
  <step>
 <screen>&prompt.user;<command>source /etc/sysconfig/etcdctl</command>

<command>etcdctl cluster-health</command>

member e69de3a81c5588dd is healthy:
got healthy result from https://627c1595c5e845c992d34613de46cbc6.infra.caasp.local:2379
cluster is healthy
 </screen>
  </step>

  <step>
 <screen>&prompt.user;<command>etcdctl member list</command>

e69de3a81c5588dd: name=627c1595c5e845c992d34613de46cbc6
                  peerURLs=https://627c1595c5e845c992d34613de46cbc6.infra.caasp.local:2380
                  clientURLs=https://627c1595c5e845c992d34613de46cbc6.infra.caasp.local:2379
                  isLeader=true
   </screen>
  </step>

  </procedure>
  </sect2>
 </sect1>

 <sect1 xml:id="sec.admin.recovery">
  <title>Disaster Recovery</title>
  <para>
   Steps that can be taken after a catastrophic outage event.
  </para>

  <sect2 xml:id="sec.admin.recovery.blackout">
   <title>Collecting Cluster Information After Power Outage</title>

   <sect3 xml:id="sec.admin.recovery.blackout.general">
    <title>Collect General Information</title>
    <procedure>
     <step>
      <para>
       Check hardware and kernel status on all nodes using <command>dmesg</command>.
      </para>
     </step>

     <step>
      <para>
       Check CPU usage and load on all nodes using <command>top</command>.
       Look out for any excessive CPU/memory usage or high load.
      </para>
     </step>
    </procedure>
   </sect3>

   <sect3 xml:id="sec.admin.recovery.blackout.velum">
    <title>Check &dashboard; Information</title>
    <procedure>
     <step>
      <para>
       Login to &dashboard; dashboard and check the nodes' status. All nodes should
       be green. If some nodes are in failed status, please re-run the
       orchestration. If the <guimenu>RETRY</guimenu> option in &dashboard; is
       not available, you can run it manually from a shell on the admin node.
      </para>
<screen>&prompt.user;<command>docker exec -it $(docker ps | grep "salt-master" | awk '{print $1}') \
salt-run -l debug state.orchestrate orch.kubernetes</command>
 </screen>
     </step>
    </procedure>
   </sect3>

   <sect3 xml:id="sec.admin.recovery.blackout.etcd">
    <title>Check <literal>etcd</literal></title>
    <para>
     <xref linkend="sec.admin.validate.etcd"/>
    </para>
   </sect3>

   <sect3 xml:id="sec.admin.recovery.blackout.kubernetes">
    <title>Check Kubernetes Cluster Health</title>

    <procedure>
    <step>
     <para>
      Verify that all of the nodes you expect to see are present and that they
      are all in the <literal>Ready</literal> state.
     </para>
<screen>&prompt.user;<command>kubectl get nodes</command></screen>
    </step>
    <step>
     <para>
      Verify that &kube; master, <literal>Dex</literal> and <literal>KubeDNS</literal>
      services are running.
     </para>
<screen>&prompt.user;<command>kubectl cluster-info</command></screen>
    </step>

    <step>
     <para>
     If is required to get more status data you can dump detailed cluster
     information.
    </para>
<screen>&prompt.user;<command>kubectl cluster-info dump</command></screen>
    </step>

    <step>
     <para>
      For even more information you can use <command>journalctl</command> to
      dig into the log information on individual machines.
     </para>
     <itemizedlist mark="bullet">
      <listitem>
       <para>
        On master nodes:
       </para>
<screen><command>journalctl -u kube-apiserver</command>
<command>journalctl -u kube-scheduler</command>
<command>journalctl -u kube-controller-manager</command>
<command>journalctl -u kube-proxy</command>
<command>journalctl -u kubelet</command>
       </screen>
      </listitem>
      <listitem>
       <para>
        On worker nodes:
       </para>
<screen><command>journalctl -u kube-proxy</command>
<command>journalctl -u kubelet</command>
       </screen>
      </listitem>
     </itemizedlist>
    </step>
   </procedure>
  </sect3>
  </sect2>
 </sect1>

</chapter>
