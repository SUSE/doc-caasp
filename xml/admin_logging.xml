<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter
[
  <!ENTITY % entities SYSTEM "entity-decl.ent">
    %entities;
]>
<chapter version="5.1" xml:id="cha.admin.logging"
  xmlns="http://docbook.org/ns/docbook"
  xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink">
 <info>
  <title>Logging</title>
  <dm:docmanager xmlns:dm="urn:x-suse:ns:docmanager">
   <dm:bugtracker/>
   <dm:translation>yes</dm:translation>
  </dm:docmanager>
 </info>

 <sect1 xml:id="sec.admin.logging.intro">
  <title>About &productname; Logging</title>
  <para>
   Fill this in with details.

   * What logs are there on the admin node?
   * What log services are interesting on the admin node?

  </para>
 </sect1>

 <sect1 xml:id="sec.admin.logging.salt">
  <title>Salt Logging</title>
  <para>
   In this case, reading the <filename>events-summarized.txt</filename> file generated by the last command should be
   enough for detecting most (if not all) of the issues when an action has been triggered.
  </para>
  <para>
   - Retrieve salt orchestration logs
     - This is what contains all the information to debug a salt issue
    </para>
  <screen>
- docker exec -it $(docker ps | grep dashboard | awk '{print $1}') entrypoint.sh bundle exec rails runner 'puts(Minion.all.map(&amp;:to_yaml))' &gt; events.txt
- /var/lib/supportutils-plugin-suse-caasp/debug-salt --json_output=events.txt --summary_output=events-summarized.txt --text-status --no-color
        </screen>

       <sect2 xml:id="sec.admin.logging.salt.master">
        <title>Salt Master Log</title>
        <para>
        - Retrieve salt master logs
          - This doesn't usually provide much insight, but might contain some issues related to the salt-database connection.
              - docker exec -it $(docker ps | grep salt-master | awk '{print $1}') cat /var/log/salt/master
        </para>
             </sect2>

       <sect2 xml:id="sec.admin.logging.salt.minion">
        <title>Salt Minion Logs</title>
        <para>
        - Retrieve the salt-minion logs for all nodes
          - This will show all output for all salt-minions at once, executed from the admin node:
              </para>
              <screen>
               - docker exec -it $(docker ps | grep salt-master | awk '{print $1}') salt '*' cmd.run "cat /var/log/salt/minion"
              </screen>
        <para>
         Of course, it's possible to retrieve this information on any node by just reading the <filename>/var/log/salt/minion</filename> file.
        </para>
       </sect2>
      </sect1>

     <sect1 xml:id="sec.admin.logging.transactional-updates">
      <title>Transactional Update Log</title>
      <para>
      Sometimes, <command>transactional-update</command> has issues when creating new snapshots, when downloading the packages from the
      update channels. In this case:
           </para>

           <screen>
      - docker exec -it $(docker ps | grep salt-master | awk '{print $1}') salt -P 'roles:(admin|kube-master|kube-minion)' cmd.run "journalctl -u transactional-update"
     </screen>

    </sect1>

    <sect1 xml:id="sec.admin.logging.fluentd">
     <title>External log collection (fluentd)</title>
     <para>
      There's a notable exception at infra level, and is that we allow audit logs to be configured from within Velum.
      This allows the audit logs to be written on the Kubernetes master nodes at <filename>/var/log/kube-apiserver/audit.log</filename> and
      a data collector (continuous reading) like <literal>fluentd</literal> can be used to collect all the audit logs.

      At Kubernetes level, there are different solutions that can be implemented, for example: <literal>fluentd</literal>
      can be used to collect all applications log in a central instance (https://docs.fluentd.org/v0.12/articles/kubernetes-fluentd).
      Then, Elasticsearch and Kibana can be used (https://kubernetes.io/docs/tasks/debug-application-cluster/logging-elasticsearch-kibana/)
      to have an intuitive way to interact with the logs.

      https://docs.fluentd.org/v0.12/articles/kubernetes-fluentd
     </para>
    </sect1>

    <sect1 xml:id="sec.admin.logging.log-levels">
     <title>Log level explanations</title>
     <para>
      There are two main components and their respective sub-components that
      allow configuration of different loglevels: Salt and &kube;.
     </para>

      <itemizedlist>
       <listitem>
        <para>
         Salt
        </para>
         <itemizedlist>
          <listitem>
           <para>
            Salt master
           </para>
           </listitem>
           <listitem>
            <para>
             Salt minion on each machine
            </para>
           </listitem>
          </itemizedlist>
        </listitem>

        <listitem>
         <para>
          Kubernetes
         </para>
         <itemizedlist>
          <listitem>
           <para>
            Master nodes
           </para>
           <itemizedlist mark="circle">
            <listitem>
             <para>
              apiserver
             </para>
            </listitem>
            <listitem>
             <para>
              controller-manager
             </para>
            </listitem>
            <listitem>
             <para>
              scheduler
             </para>
            </listitem>
           </itemizedlist>
          </listitem>

          <listitem>
           <para>
            All nodes
           </para>
           <itemizedlist mark="circle">
            <listitem>
             <para>
              kubelet
             </para>
            </listitem>
            <listitem>
             <para>
              kube proxy
             </para>
            </listitem>
            </itemizedlist>
          </listitem>
         </itemizedlist>
         </listitem>
          </itemizedlist>

          <sect2 xml:id="sec.admin.logging.log-levels.salt">
           <title>Salt Log Levels</title>

        <para>
         Salt provides different loglevels that apply both to the master and the
         minions.
        </para>

      <itemizedlist>
       <listitem>
        <para>
         all
        </para>
       </listitem>
       <listitem>
        <para>
         garbage
        </para>
       </listitem>
       <listitem>
        <para>
         trace
        </para>
       </listitem>
       <listitem>
        <para>
         debug
        </para>
       </listitem>
       <listitem>
        <para>
         profile
        </para>
       </listitem>
       <listitem>
        <para>
         info
        </para>
       </listitem>
       <listitem>
        <para>
         warning
        </para>
       </listitem>
       <listitem>
        <para>
         error
        </para>
       </listitem>
       <listitem>
        <para>
         critical
        </para>
       </listitem>
       <listitem>
        <para>
         quiet
        </para>
       </listitem>
      </itemizedlist>

      <para>
      For detailed explanations of the usage of these log levels please see:
      <link xlink:href="https://docs.saltstack.com/en/latest/ref/configuration/logging/">Salt Log Levels (Upstream)</link>
      </para>

      <para>
      When debugging issues, <literal>debug</literal> is usually enough and is what the CaaS Platform engineers use most of
      the time to detect problems.
     </para>

     <para>
      In the case of the salt master, this configuration can be modified in the admin node, at <filename>/etc/caasp/salt-master-custom.conf</filename>.
      For example, inside this file you can place: <literal>log_level: debug</literal>. Note that after any change on this file you need to
      restart the <literal>salt-master</literal> container, like: <command>docker rm -f $(docker ps | grep salt-master | awk '{print $1}')</command>. After
      deleting this container, the <literal>kubelet</literal> will bring a new <literal>salt-master</literal> container automatically with the new configuration
      applied. So you can check the logs with the <literal>debug</literal> loglevel now: <command>docker logs -f $(docker ps | grep salt-master | awk '{print $1}')</command>.
     </para>

     <para>
      In the case of the salt minions, there's no "official" way to tweak this configuration, however,
      a new file can be added on the minion that you want to print debugging salt information in:
      <filename>/etc/salt/minion.d/100-debug.conf</filename>. After doing a <command>systemctl restart salt-minion</command> you can check
      that it now prints debugging information with <command>journalctl -fu salt-minion</command>.
     </para>
    </sect2>

    <sect2 xml:id="sec.admin.logging.log-levels.kubernetes">
     <title>&kube; Log Levels</title>
     <para>
      As for Kubernetes, our default <literal>loglevel</literal> is 2 (https://kubernetes.io/docs/reference/kubectl/cheatsheet/#kubectl-output-verbosity-and-debugging).
      However, this setting can be tweaked by using pillar overrides. If modified, this setting will be applied to all
      Kubernetes components; there's no way to set different loglevel per components, and throughout the whole cluster;
      there's no way to specify different loglevels per machine. It's possible to do so, but if the customer does that,
      the configuration where they set this loglevel could be overriden by salt at anytime, so it's not advised to do
      any kind of local change to the machines aside from what we explicitly document.
     </para>
    </sect2>

 </sect1>
</chapter>
